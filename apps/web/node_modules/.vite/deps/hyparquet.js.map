{
  "version": 3,
  "sources": ["../../hyparquet/src/constants.js", "../../hyparquet/src/wkb.js", "../../hyparquet/src/convert.js", "../../hyparquet/src/schema.js", "../../hyparquet/src/thrift.js", "../../hyparquet/src/geoparquet.js", "../../hyparquet/src/metadata.js", "../../hyparquet/src/indexes.js", "../../hyparquet/src/utils.js", "../../hyparquet/src/filter.js", "../../hyparquet/src/plan.js", "../../hyparquet/src/assemble.js", "../../hyparquet/src/delta.js", "../../hyparquet/src/encoding.js", "../../hyparquet/src/plain.js", "../../hyparquet/src/snappy.js", "../../hyparquet/src/datapage.js", "../../hyparquet/src/column.js", "../../hyparquet/src/rowgroup.js", "../../hyparquet/src/read.js", "../../hyparquet/src/query.js"],
  "sourcesContent": ["\n/** @type {import('../src/types.d.ts').ParquetType[]} */\nexport const ParquetTypes = [\n  'BOOLEAN',\n  'INT32',\n  'INT64',\n  'INT96', // deprecated\n  'FLOAT',\n  'DOUBLE',\n  'BYTE_ARRAY',\n  'FIXED_LEN_BYTE_ARRAY',\n]\n\n/** @type {import('../src/types.d.ts').Encoding[]} */\nexport const Encodings = [\n  'PLAIN',\n  'GROUP_VAR_INT', // deprecated\n  'PLAIN_DICTIONARY',\n  'RLE',\n  'BIT_PACKED', // deprecated\n  'DELTA_BINARY_PACKED',\n  'DELTA_LENGTH_BYTE_ARRAY',\n  'DELTA_BYTE_ARRAY',\n  'RLE_DICTIONARY',\n  'BYTE_STREAM_SPLIT',\n]\n\n/** @type {import('../src/types.d.ts').FieldRepetitionType[]} */\nexport const FieldRepetitionTypes = [\n  'REQUIRED',\n  'OPTIONAL',\n  'REPEATED',\n]\n\n/** @type {import('../src/types.d.ts').ConvertedType[]} */\nexport const ConvertedTypes = [\n  'UTF8',\n  'MAP',\n  'MAP_KEY_VALUE',\n  'LIST',\n  'ENUM',\n  'DECIMAL',\n  'DATE',\n  'TIME_MILLIS',\n  'TIME_MICROS',\n  'TIMESTAMP_MILLIS',\n  'TIMESTAMP_MICROS',\n  'UINT_8',\n  'UINT_16',\n  'UINT_32',\n  'UINT_64',\n  'INT_8',\n  'INT_16',\n  'INT_32',\n  'INT_64',\n  'JSON',\n  'BSON',\n  'INTERVAL',\n]\n\n/** @type {import('../src/types.d.ts').CompressionCodec[]} */\nexport const CompressionCodecs = [\n  'UNCOMPRESSED',\n  'SNAPPY',\n  'GZIP',\n  'LZO',\n  'BROTLI',\n  'LZ4',\n  'ZSTD',\n  'LZ4_RAW',\n]\n\n/** @type {import('../src/types.d.ts').PageType[]} */\nexport const PageTypes = [\n  'DATA_PAGE',\n  'INDEX_PAGE',\n  'DICTIONARY_PAGE',\n  'DATA_PAGE_V2',\n]\n\n/** @type {import('../src/types.d.ts').BoundaryOrder[]} */\nexport const BoundaryOrders = [\n  'UNORDERED',\n  'ASCENDING',\n  'DESCENDING',\n]\n\n/** @type {import('../src/types.d.ts').EdgeInterpolationAlgorithm[]} */\nexport const EdgeInterpolationAlgorithms = [\n  'SPHERICAL',\n  'VINCENTY',\n  'THOMAS',\n  'ANDOYER',\n  'KARNEY',\n]\n", "/**\n * WKB (Well-Known Binary) decoder for geometry objects.\n *\n * @import {DataReader, Geometry} from '../src/types.js'\n * @param {DataReader} reader\n * @returns {Geometry} geometry object\n */\nexport function wkbToGeojson(reader) {\n  const flags = getFlags(reader)\n\n  if (flags.type === 1) { // Point\n    return { type: 'Point', coordinates: readPosition(reader, flags) }\n  } else if (flags.type === 2) { // LineString\n    return { type: 'LineString', coordinates: readLine(reader, flags) }\n  } else if (flags.type === 3) { // Polygon\n    return { type: 'Polygon', coordinates: readPolygon(reader, flags) }\n  } else if (flags.type === 4) { // MultiPoint\n    const points = []\n    for (let i = 0; i < flags.count; i++) {\n      points.push(readPosition(reader, getFlags(reader)))\n    }\n    return { type: 'MultiPoint', coordinates: points }\n  } else if (flags.type === 5) { // MultiLineString\n    const lines = []\n    for (let i = 0; i < flags.count; i++) {\n      lines.push(readLine(reader, getFlags(reader)))\n    }\n    return { type: 'MultiLineString', coordinates: lines }\n  } else if (flags.type === 6) { // MultiPolygon\n    const polygons = []\n    for (let i = 0; i < flags.count; i++) {\n      polygons.push(readPolygon(reader, getFlags(reader)))\n    }\n    return { type: 'MultiPolygon', coordinates: polygons }\n  } else if (flags.type === 7) { // GeometryCollection\n    const geometries = []\n    for (let i = 0; i < flags.count; i++) {\n      geometries.push(wkbToGeojson(reader))\n    }\n    return { type: 'GeometryCollection', geometries }\n  } else {\n    throw new Error(`Unsupported geometry type: ${flags.type}`)\n  }\n}\n\n/**\n * @typedef {object} WkbFlags\n * @property {boolean} littleEndian\n * @property {number} type\n * @property {number} dim\n * @property {number} count\n */\n\n/**\n * Extract ISO WKB flags and base geometry type.\n *\n * @param {DataReader} reader\n * @returns {WkbFlags}\n */\nfunction getFlags(reader) {\n  const { view } = reader\n  const littleEndian = view.getUint8(reader.offset++) === 1\n  const rawType = view.getUint32(reader.offset, littleEndian)\n  reader.offset += 4\n\n  const type = rawType % 1000\n  const flags = Math.floor(rawType / 1000)\n\n  let count = 0\n  if (type > 1 && type <= 7) {\n    count = view.getUint32(reader.offset, littleEndian)\n    reader.offset += 4\n  }\n\n  // XY, XYZ, XYM, XYZM\n  let dim = 2\n  if (flags) dim++\n  if (flags === 3) dim++\n\n  return { littleEndian, type, dim, count }\n}\n\n/**\n * @param {DataReader} reader\n * @param {WkbFlags} flags\n * @returns {number[]}\n */\nfunction readPosition(reader, flags) {\n  const points = []\n  for (let i = 0; i < flags.dim; i++) {\n    const coord = reader.view.getFloat64(reader.offset, flags.littleEndian)\n    reader.offset += 8\n    points.push(coord)\n  }\n  return points\n}\n\n/**\n * @param {DataReader} reader\n * @param {WkbFlags} flags\n * @returns {number[][]}\n */\nfunction readLine(reader, flags) {\n  const points = []\n  for (let i = 0; i < flags.count; i++) {\n    points.push(readPosition(reader, flags))\n  }\n  return points\n}\n\n/**\n * @param {DataReader} reader\n * @param {WkbFlags} flags\n * @returns {number[][][]}\n */\nfunction readPolygon(reader, flags) {\n  const { view } = reader\n  const rings = []\n  for (let r = 0; r < flags.count; r++) {\n    const count = view.getUint32(reader.offset, flags.littleEndian)\n    reader.offset += 4\n    rings.push(readLine(reader, { ...flags, count }))\n  }\n  return rings\n}\n", "import { wkbToGeojson } from './wkb.js'\n\n/**\n * @import {ColumnDecoder, DecodedArray, Encoding, ParquetParsers} from '../src/types.js'\n */\n\nconst decoder = new TextDecoder()\n\n/**\n * Default type parsers when no custom ones are given\n * @type ParquetParsers\n */\nexport const DEFAULT_PARSERS = {\n  timestampFromMilliseconds(millis) {\n    return new Date(Number(millis))\n  },\n  timestampFromMicroseconds(micros) {\n    return new Date(Number(micros / 1000n))\n  },\n  timestampFromNanoseconds(nanos) {\n    return new Date(Number(nanos / 1000000n))\n  },\n  dateFromDays(days) {\n    return new Date(days * 86400000)\n  },\n  stringFromBytes(bytes) {\n    return bytes && decoder.decode(bytes)\n  },\n  geometryFromBytes(bytes) {\n    return bytes && wkbToGeojson({ view: new DataView(bytes.buffer, bytes.byteOffset, bytes.byteLength), offset: 0 })\n  },\n  geographyFromBytes(bytes) {\n    return bytes && wkbToGeojson({ view: new DataView(bytes.buffer, bytes.byteOffset, bytes.byteLength), offset: 0 })\n  },\n}\n\n/**\n * Convert known types from primitive to rich, and dereference dictionary.\n *\n * @param {DecodedArray} data series of primitive types\n * @param {DecodedArray | undefined} dictionary\n * @param {Encoding} encoding\n * @param {ColumnDecoder} columnDecoder\n * @returns {DecodedArray} series of rich types\n */\nexport function convertWithDictionary(data, dictionary, encoding, columnDecoder) {\n  if (dictionary && encoding.endsWith('_DICTIONARY')) {\n    let output = data\n    if (data instanceof Uint8Array && !(dictionary instanceof Uint8Array)) {\n      // @ts-expect-error upgrade data to match dictionary type with fancy constructor\n      output = new dictionary.constructor(data.length)\n    }\n    for (let i = 0; i < data.length; i++) {\n      output[i] = dictionary[data[i]]\n    }\n    return output\n  } else {\n    return convert(data, columnDecoder)\n  }\n}\n\n/**\n * Convert known types from primitive to rich.\n *\n * @param {DecodedArray} data series of primitive types\n * @param {Pick<ColumnDecoder, \"element\" | \"utf8\" | \"parsers\">} columnDecoder\n * @returns {DecodedArray} series of rich types\n */\nexport function convert(data, columnDecoder) {\n  const { element, parsers, utf8 = true } = columnDecoder\n  const { type, converted_type: ctype, logical_type: ltype } = element\n  if (ctype === 'DECIMAL') {\n    const scale = element.scale || 0\n    const factor = 10 ** -scale\n    const arr = new Array(data.length)\n    for (let i = 0; i < arr.length; i++) {\n      if (data[i] instanceof Uint8Array) {\n        arr[i] = parseDecimal(data[i]) * factor\n      } else {\n        arr[i] = Number(data[i]) * factor\n      }\n    }\n    return arr\n  }\n  if (!ctype && type === 'INT96') {\n    return Array.from(data).map(v => parsers.timestampFromNanoseconds(parseInt96Nanos(v)))\n  }\n  if (ctype === 'DATE') {\n    return Array.from(data).map(v => parsers.dateFromDays(v))\n  }\n  if (ctype === 'TIMESTAMP_MILLIS') {\n    return Array.from(data).map(v => parsers.timestampFromMilliseconds(v))\n  }\n  if (ctype === 'TIMESTAMP_MICROS') {\n    return Array.from(data).map(v => parsers.timestampFromMicroseconds(v))\n  }\n  if (ctype === 'JSON') {\n    return data.map(v => JSON.parse(decoder.decode(v)))\n  }\n  if (ctype === 'BSON') {\n    throw new Error('parquet bson not supported')\n  }\n  if (ctype === 'INTERVAL') {\n    throw new Error('parquet interval not supported')\n  }\n  if (ltype?.type === 'GEOMETRY') {\n    return data.map(v => parsers.geometryFromBytes(v))\n  }\n  if (ltype?.type === 'GEOGRAPHY') {\n    return data.map(v => parsers.geographyFromBytes(v))\n  }\n  if (ctype === 'UTF8' || ltype?.type === 'STRING' || utf8 && type === 'BYTE_ARRAY') {\n    return data.map(v => parsers.stringFromBytes(v))\n  }\n  if (ctype === 'UINT_64' || ltype?.type === 'INTEGER' && ltype.bitWidth === 64 && !ltype.isSigned) {\n    if (data instanceof BigInt64Array) {\n      return new BigUint64Array(data.buffer, data.byteOffset, data.length)\n    }\n    const arr = new BigUint64Array(data.length)\n    for (let i = 0; i < arr.length; i++) arr[i] = BigInt(data[i])\n    return arr\n  }\n  if (ctype === 'UINT_32' || ltype?.type === 'INTEGER' && ltype.bitWidth === 32 && !ltype.isSigned) {\n    if (data instanceof Int32Array) {\n      return new Uint32Array(data.buffer, data.byteOffset, data.length)\n    }\n    const arr = new Uint32Array(data.length)\n    for (let i = 0; i < arr.length; i++) arr[i] = data[i]\n    return arr\n  }\n  if (ltype?.type === 'FLOAT16') {\n    return Array.from(data).map(parseFloat16)\n  }\n  if (ltype?.type === 'TIMESTAMP') {\n    const { unit } = ltype\n    /** @type {ParquetParsers[keyof ParquetParsers]} */\n    let parser = parsers.timestampFromMilliseconds\n    if (unit === 'MICROS') parser = parsers.timestampFromMicroseconds\n    if (unit === 'NANOS') parser = parsers.timestampFromNanoseconds\n    const arr = new Array(data.length)\n    for (let i = 0; i < arr.length; i++) {\n      arr[i] = parser(data[i])\n    }\n    return arr\n  }\n  return data\n}\n\n/**\n * @param {Uint8Array} bytes\n * @returns {number}\n */\nexport function parseDecimal(bytes) {\n  if (!bytes.length) return 0\n\n  let value = 0n\n  for (const byte of bytes) {\n    value = value * 256n + BigInt(byte)\n  }\n\n  // handle signed\n  const bits = bytes.length * 8\n  if (value >= 2n ** BigInt(bits - 1)) {\n    value -= 2n ** BigInt(bits)\n  }\n\n  return Number(value)\n}\n\n/**\n * Converts INT96 date format (hi 32bit days, lo 64bit nanos) to nanos since epoch\n * @param {bigint} value\n * @returns {bigint}\n */\nfunction parseInt96Nanos(value) {\n  const days = (value >> 64n) - 2440588n\n  const nano = value & 0xffffffffffffffffn\n  return days * 86400000000000n + nano\n}\n\n/**\n * @param {Uint8Array | undefined} bytes\n * @returns {number | undefined}\n */\nexport function parseFloat16(bytes) {\n  if (!bytes) return undefined\n  const int16 = bytes[1] << 8 | bytes[0]\n  const sign = int16 >> 15 ? -1 : 1\n  const exp = int16 >> 10 & 0x1f\n  const frac = int16 & 0x3ff\n  if (exp === 0) return sign * 2 ** -14 * (frac / 1024) // subnormals\n  if (exp === 0x1f) return frac ? NaN : sign * Infinity\n  return sign * 2 ** (exp - 15) * (1 + frac / 1024)\n}\n", "/**\n * Build a tree from the schema elements.\n *\n * @import {SchemaElement, SchemaTree} from '../src/types.d.ts'\n * @param {SchemaElement[]} schema\n * @param {number} rootIndex index of the root element\n * @param {string[]} path path to the element\n * @returns {SchemaTree} tree of schema elements\n */\nfunction schemaTree(schema, rootIndex, path) {\n  const element = schema[rootIndex]\n  const children = []\n  let count = 1\n\n  // Read the specified number of children\n  if (element.num_children) {\n    while (children.length < element.num_children) {\n      const childElement = schema[rootIndex + count]\n      const child = schemaTree(schema, rootIndex + count, [...path, childElement.name])\n      count += child.count\n      children.push(child)\n    }\n  }\n\n  return { count, element, children, path }\n}\n\n/**\n * Get schema elements from the root to the given element name.\n *\n * @param {SchemaElement[]} schema\n * @param {string[]} name path to the element\n * @returns {SchemaTree[]} list of schema elements\n */\nexport function getSchemaPath(schema, name) {\n  let tree = schemaTree(schema, 0, [])\n  const path = [tree]\n  for (const part of name) {\n    const child = tree.children.find(child => child.element.name === part)\n    if (!child) throw new Error(`parquet schema element not found: ${name}`)\n    path.push(child)\n    tree = child\n  }\n  return path\n}\n\n/**\n * Get all physical (leaf) column names.\n *\n * @param {SchemaTree} schemaTree\n * @returns {string[]} list of physical column names\n */\nexport function getPhysicalColumns(schemaTree) {\n  /** @type {string[]} */\n  const columns = []\n  /** @param {SchemaTree} node */\n  function traverse(node) {\n    if (node.children.length) {\n      for (const child of node.children) {\n        traverse(child)\n      }\n    } else {\n      columns.push(node.path.join('.'))\n    }\n  }\n  traverse(schemaTree)\n  return columns\n}\n\n/**\n * Get the max repetition level for a given schema path.\n *\n * @param {SchemaTree[]} schemaPath\n * @returns {number} max repetition level\n */\nexport function getMaxRepetitionLevel(schemaPath) {\n  let maxLevel = 0\n  for (const { element } of schemaPath) {\n    if (element.repetition_type === 'REPEATED') {\n      maxLevel++\n    }\n  }\n  return maxLevel\n}\n\n/**\n * Get the max definition level for a given schema path.\n *\n * @param {SchemaTree[]} schemaPath\n * @returns {number} max definition level\n */\nexport function getMaxDefinitionLevel(schemaPath) {\n  let maxLevel = 0\n  for (const { element } of schemaPath.slice(1)) {\n    if (element.repetition_type !== 'REQUIRED') {\n      maxLevel++\n    }\n  }\n  return maxLevel\n}\n\n/**\n * Check if a column is list-like.\n *\n * @param {SchemaTree} schema\n * @returns {boolean} true if list-like\n */\nexport function isListLike(schema) {\n  if (!schema) return false\n  if (schema.element.converted_type !== 'LIST') return false\n  if (schema.children.length > 1) return false\n\n  const firstChild = schema.children[0]\n  if (firstChild.children.length > 1) return false\n  if (firstChild.element.repetition_type !== 'REPEATED') return false\n\n  return true\n}\n\n/**\n * Check if a column is map-like.\n *\n * @param {SchemaTree} schema\n * @returns {boolean} true if map-like\n */\nexport function isMapLike(schema) {\n  if (!schema) return false\n  if (schema.element.converted_type !== 'MAP') return false\n  if (schema.children.length > 1) return false\n\n  const firstChild = schema.children[0]\n  if (firstChild.children.length !== 2) return false\n  if (firstChild.element.repetition_type !== 'REPEATED') return false\n\n  const keyChild = firstChild.children.find(child => child.element.name === 'key')\n  if (keyChild?.element.repetition_type === 'REPEATED') return false\n\n  const valueChild = firstChild.children.find(child => child.element.name === 'value')\n  if (valueChild?.element.repetition_type === 'REPEATED') return false\n\n  return true\n}\n\n/**\n * Returns true if a column is non-nested.\n *\n * @param {SchemaTree[]} schemaPath\n * @returns {boolean}\n */\nexport function isFlatColumn(schemaPath) {\n  if (schemaPath.length !== 2) return false\n  const [, column] = schemaPath\n  if (column.element.repetition_type === 'REPEATED') return false\n  if (column.children.length) return false\n  return true\n}\n", "// TCompactProtocol types\nexport const CompactType = {\n  STOP: 0,\n  TRUE: 1,\n  FALSE: 2,\n  BYTE: 3,\n  I16: 4,\n  I32: 5,\n  I64: 6,\n  DOUBLE: 7,\n  BINARY: 8,\n  LIST: 9,\n  SET: 10,\n  MAP: 11,\n  STRUCT: 12,\n  UUID: 13,\n}\n\n/**\n * Parse TCompactProtocol\n *\n * @param {DataReader} reader\n * @returns {{ [key: `field_${number}`]: any }}\n */\nexport function deserializeTCompactProtocol(reader) {\n  let lastFid = 0\n  /** @type {ThriftObject} */\n  const value = {}\n\n  while (reader.offset < reader.view.byteLength) {\n    // Parse each field based on its type and add to the result object\n    const [type, fid, newLastFid] = readFieldBegin(reader, lastFid)\n    lastFid = newLastFid\n\n    if (type === CompactType.STOP) {\n      break\n    }\n\n    // Handle the field based on its type\n    value[`field_${fid}`] = readElement(reader, type)\n  }\n\n  return value\n}\n\n/**\n * Read a single element based on its type\n *\n * @import {DataReader, ThriftObject, ThriftType} from '../src/types.d.ts'\n * @param {DataReader} reader\n * @param {number} type\n * @returns {ThriftType}\n */\nfunction readElement(reader, type) {\n  switch (type) {\n  case CompactType.TRUE:\n    return true\n  case CompactType.FALSE:\n    return false\n  case CompactType.BYTE:\n    // read byte directly\n    return reader.view.getInt8(reader.offset++)\n  case CompactType.I16:\n  case CompactType.I32:\n    return readZigZag(reader)\n  case CompactType.I64:\n    return readZigZagBigInt(reader)\n  case CompactType.DOUBLE: {\n    const value = reader.view.getFloat64(reader.offset, true)\n    reader.offset += 8\n    return value\n  }\n  case CompactType.BINARY: {\n    const stringLength = readVarInt(reader)\n    const strBytes = new Uint8Array(reader.view.buffer, reader.view.byteOffset + reader.offset, stringLength)\n    reader.offset += stringLength\n    return strBytes\n  }\n  case CompactType.LIST: {\n    const byte = reader.view.getUint8(reader.offset++)\n    const elemType = byte & 0x0f\n    let listSize = byte >> 4\n    if (listSize === 15) {\n      listSize = readVarInt(reader)\n    }\n    const boolType = elemType === CompactType.TRUE || elemType === CompactType.FALSE\n    const values = new Array(listSize)\n    for (let i = 0; i < listSize; i++) {\n      values[i] = boolType ? readElement(reader, CompactType.BYTE) === 1 : readElement(reader, elemType)\n    }\n    return values\n  }\n  case CompactType.STRUCT: {\n    /** @type {ThriftObject} */\n    const structValues = {}\n    let lastFid = 0\n    while (true) {\n      const [fieldType, fid, newLastFid] = readFieldBegin(reader, lastFid)\n      lastFid = newLastFid\n      if (fieldType === CompactType.STOP) {\n        break\n      }\n      structValues[`field_${fid}`] = readElement(reader, fieldType)\n    }\n    return structValues\n  }\n  // TODO: MAP, SET, UUID\n  default:\n    throw new Error(`thrift unhandled type: ${type}`)\n  }\n}\n\n/**\n * Var int aka Unsigned LEB128.\n * Reads groups of 7 low bits until high bit is 0.\n *\n * @param {DataReader} reader\n * @returns {number}\n */\nexport function readVarInt(reader) {\n  let result = 0\n  let shift = 0\n  while (true) {\n    const byte = reader.view.getUint8(reader.offset++)\n    result |= (byte & 0x7f) << shift\n    if (!(byte & 0x80)) {\n      return result\n    }\n    shift += 7\n  }\n}\n\n/**\n * Read a varint as a bigint.\n *\n * @param {DataReader} reader\n * @returns {bigint}\n */\nfunction readVarBigInt(reader) {\n  let result = 0n\n  let shift = 0n\n  while (true) {\n    const byte = reader.view.getUint8(reader.offset++)\n    result |= BigInt(byte & 0x7f) << shift\n    if (!(byte & 0x80)) {\n      return result\n    }\n    shift += 7n\n  }\n}\n\n/**\n * Values of type int32 and int64 are transformed to a zigzag int.\n * A zigzag int folds positive and negative numbers into the positive number space.\n *\n * @param {DataReader} reader\n * @returns {number}\n */\nexport function readZigZag(reader) {\n  const zigzag = readVarInt(reader)\n  // convert zigzag to int\n  return zigzag >>> 1 ^ -(zigzag & 1)\n}\n\n/**\n * A zigzag int folds positive and negative numbers into the positive number space.\n * This version returns a BigInt.\n *\n * @param {DataReader} reader\n * @returns {bigint}\n */\nexport function readZigZagBigInt(reader) {\n  const zigzag = readVarBigInt(reader)\n  // convert zigzag to int\n  return zigzag >> 1n ^ -(zigzag & 1n)\n}\n\n/**\n * Read field type and field id\n *\n * @param {DataReader} reader\n * @param {number} lastFid\n * @returns {[number, number, number]} [type, fid, newLastFid]\n */\nfunction readFieldBegin(reader, lastFid) {\n  const byte = reader.view.getUint8(reader.offset++)\n  const type = byte & 0x0f\n  if (type === CompactType.STOP) {\n    // STOP also ends a struct\n    return [0, 0, lastFid]\n  }\n  const delta = byte >> 4\n  const fid = delta ? lastFid + delta : readZigZag(reader)\n  return [type, fid, fid]\n}\n", "/**\n * @import {KeyValue, LogicalType, SchemaElement} from '../src/types.d.ts'\n * @param {SchemaElement[]} schema\n * @param {KeyValue[] | undefined} key_value_metadata\n * @returns {void}\n */\nexport function markGeoColumns(schema, key_value_metadata) {\n  // Prepare the list of GeoParquet columns\n  /** @type {Map<string, LogicalType>} */\n  const columns = new Map()\n  const geo = key_value_metadata?.find(({ key }) => key === 'geo')?.value\n  const decodedColumns = (geo && JSON.parse(geo)?.columns) ?? {}\n  for (const [name, column] of Object.entries(decodedColumns)) {\n    if (column.encoding !== 'WKB') {\n      continue\n    }\n    const type = column.edges === 'spherical' ? 'GEOGRAPHY' : 'GEOMETRY'\n    const id = column.crs?.id ?? column.crs?.ids?.[0]\n    const crs = id ? `${id.authority}:${id.code.toString()}` : undefined\n    // Note: we can't infer GEOGRAPHY's algorithm from GeoParquet\n    columns.set(name, { type, crs })\n  }\n\n  // Mark schema elements with logical type\n  // Only look at root-level columns of type BYTE_ARRAY without existing logical_type\n  for (let i = 1; i < schema.length; i++) { // skip root\n    const element = schema[i]\n    const { logical_type, name, num_children, repetition_type, type } = element\n    if (num_children) {\n      i += num_children\n      continue // skip the element and its children\n    }\n    if (type === 'BYTE_ARRAY' && logical_type === undefined && repetition_type !== 'REPEATED') {\n      element.logical_type = columns.get(name)\n    }\n  }\n}\n", "import { CompressionCodecs, ConvertedTypes, EdgeInterpolationAlgorithms, Encodings, FieldRepetitionTypes, PageTypes, ParquetTypes } from './constants.js'\nimport { DEFAULT_PARSERS, parseDecimal, parseFloat16 } from './convert.js'\nimport { getSchemaPath } from './schema.js'\nimport { deserializeTCompactProtocol } from './thrift.js'\nimport { markGeoColumns } from './geoparquet.js'\n\nexport const defaultInitialFetchSize = 1 << 19 // 512kb\n\nconst decoder = new TextDecoder()\nfunction decode(/** @type {Uint8Array} */ value) {\n  return value && decoder.decode(value)\n}\n\n/**\n * Read parquet metadata from an async buffer.\n *\n * An AsyncBuffer is like an ArrayBuffer, but the slices are loaded\n * asynchronously, possibly over the network.\n *\n * You must provide the byteLength of the buffer, typically from a HEAD request.\n *\n * In theory, you could use suffix-range requests to fetch the end of the file,\n * and save a round trip. But in practice, this doesn't work because chrome\n * deems suffix-range requests as a not-safe-listed header, and will require\n * a pre-flight. So the byteLength is required.\n *\n * To make this efficient, we initially request the last 512kb of the file,\n * which is likely to contain the metadata. If the metadata length exceeds the\n * initial fetch, 512kb, we request the rest of the metadata from the AsyncBuffer.\n *\n * This ensures that we either make one 512kb initial request for the metadata,\n * or a second request for up to the metadata size.\n *\n * @param {AsyncBuffer} asyncBuffer parquet file contents\n * @param {MetadataOptions & { initialFetchSize?: number }} options initial fetch size in bytes (default 512kb)\n * @returns {Promise<FileMetaData>} parquet metadata object\n */\nexport async function parquetMetadataAsync(asyncBuffer, { parsers, initialFetchSize = defaultInitialFetchSize, geoparquet = true } = {}) {\n  if (!asyncBuffer || !(asyncBuffer.byteLength >= 0)) throw new Error('parquet expected AsyncBuffer')\n\n  // fetch last bytes (footer) of the file\n  const footerOffset = Math.max(0, asyncBuffer.byteLength - initialFetchSize)\n  const footerBuffer = await asyncBuffer.slice(footerOffset, asyncBuffer.byteLength)\n\n  // Check for parquet magic number \"PAR1\"\n  const footerView = new DataView(footerBuffer)\n  if (footerView.getUint32(footerBuffer.byteLength - 4, true) !== 0x31524150) {\n    throw new Error('parquet file invalid (footer != PAR1)')\n  }\n\n  // Parquet files store metadata at the end of the file\n  // Metadata length is 4 bytes before the last PAR1\n  const metadataLength = footerView.getUint32(footerBuffer.byteLength - 8, true)\n  if (metadataLength > asyncBuffer.byteLength - 8) {\n    throw new Error(`parquet metadata length ${metadataLength} exceeds available buffer ${asyncBuffer.byteLength - 8}`)\n  }\n\n  // check if metadata size fits inside the initial fetch\n  if (metadataLength + 8 > initialFetchSize) {\n    // fetch the rest of the metadata\n    const metadataOffset = asyncBuffer.byteLength - metadataLength - 8\n    const metadataBuffer = await asyncBuffer.slice(metadataOffset, footerOffset)\n    // combine initial fetch with the new slice\n    const combinedBuffer = new ArrayBuffer(metadataLength + 8)\n    const combinedView = new Uint8Array(combinedBuffer)\n    combinedView.set(new Uint8Array(metadataBuffer))\n    combinedView.set(new Uint8Array(footerBuffer), footerOffset - metadataOffset)\n    return parquetMetadata(combinedBuffer, { parsers, geoparquet })\n  } else {\n    // parse metadata from the footer\n    return parquetMetadata(footerBuffer, { parsers, geoparquet })\n  }\n}\n\n/**\n * Read parquet metadata from a buffer synchronously.\n *\n * @import {KeyValue} from '../src/types.d.ts'\n * @param {ArrayBuffer} arrayBuffer parquet file footer\n * @param {MetadataOptions} options metadata parsing options\n * @returns {FileMetaData} parquet metadata object\n */\nexport function parquetMetadata(arrayBuffer, { parsers, geoparquet = true } = {}) {\n  if (!(arrayBuffer instanceof ArrayBuffer)) throw new Error('parquet expected ArrayBuffer')\n  const view = new DataView(arrayBuffer)\n\n  // Use default parsers if not given\n  parsers = { ...DEFAULT_PARSERS, ...parsers }\n\n  // Validate footer magic number \"PAR1\"\n  if (view.byteLength < 8) {\n    throw new Error('parquet file is too short')\n  }\n  if (view.getUint32(view.byteLength - 4, true) !== 0x31524150) {\n    throw new Error('parquet file invalid (footer != PAR1)')\n  }\n\n  // Parquet files store metadata at the end of the file\n  // Metadata length is 4 bytes before the last PAR1\n  const metadataLengthOffset = view.byteLength - 8\n  const metadataLength = view.getUint32(metadataLengthOffset, true)\n  if (metadataLength > view.byteLength - 8) {\n    // {metadata}, metadata_length, PAR1\n    throw new Error(`parquet metadata length ${metadataLength} exceeds available buffer ${view.byteLength - 8}`)\n  }\n\n  const metadataOffset = metadataLengthOffset - metadataLength\n  const reader = { view, offset: metadataOffset }\n  const metadata = deserializeTCompactProtocol(reader)\n\n  // Parse metadata from thrift data\n  const version = metadata.field_1\n  /** @type {SchemaElement[]} */\n  const schema = metadata.field_2.map((/** @type {any} */ field) => ({\n    type: ParquetTypes[field.field_1],\n    type_length: field.field_2,\n    repetition_type: FieldRepetitionTypes[field.field_3],\n    name: decode(field.field_4),\n    num_children: field.field_5,\n    converted_type: ConvertedTypes[field.field_6],\n    scale: field.field_7,\n    precision: field.field_8,\n    field_id: field.field_9,\n    logical_type: logicalType(field.field_10),\n  }))\n  // schema element per column index\n  const columnSchema = schema.filter(e => e.type)\n  const num_rows = metadata.field_3\n  const row_groups = metadata.field_4.map((/** @type {any} */ rowGroup) => ({\n    columns: rowGroup.field_1.map((/** @type {any} */ column, /** @type {number} */ columnIndex) => ({\n      file_path: decode(column.field_1),\n      file_offset: column.field_2,\n      meta_data: column.field_3 && {\n        type: ParquetTypes[column.field_3.field_1],\n        encodings: column.field_3.field_2?.map((/** @type {number} */ e) => Encodings[e]),\n        path_in_schema: column.field_3.field_3.map(decode),\n        codec: CompressionCodecs[column.field_3.field_4],\n        num_values: column.field_3.field_5,\n        total_uncompressed_size: column.field_3.field_6,\n        total_compressed_size: column.field_3.field_7,\n        key_value_metadata: column.field_3.field_8?.map((/** @type {any} */ kv) => ({\n          key: decode(kv.field_1),\n          value: decode(kv.field_2),\n        })),\n        data_page_offset: column.field_3.field_9,\n        index_page_offset: column.field_3.field_10,\n        dictionary_page_offset: column.field_3.field_11,\n        statistics: convertStats(column.field_3.field_12, columnSchema[columnIndex], parsers),\n        encoding_stats: column.field_3.field_13?.map((/** @type {any} */ encodingStat) => ({\n          page_type: PageTypes[encodingStat.field_1],\n          encoding: Encodings[encodingStat.field_2],\n          count: encodingStat.field_3,\n        })),\n        bloom_filter_offset: column.field_3.field_14,\n        bloom_filter_length: column.field_3.field_15,\n        size_statistics: column.field_3.field_16 && {\n          unencoded_byte_array_data_bytes: column.field_3.field_16.field_1,\n          repetition_level_histogram: column.field_3.field_16.field_2,\n          definition_level_histogram: column.field_3.field_16.field_3,\n        },\n        geospatial_statistics: column.field_3.field_17 && {\n          bbox: column.field_3.field_17.field_1 && {\n            xmin: column.field_3.field_17.field_1.field_1,\n            xmax: column.field_3.field_17.field_1.field_2,\n            ymin: column.field_3.field_17.field_1.field_3,\n            ymax: column.field_3.field_17.field_1.field_4,\n            zmin: column.field_3.field_17.field_1.field_5,\n            zmax: column.field_3.field_17.field_1.field_6,\n            mmin: column.field_3.field_17.field_1.field_7,\n            mmax: column.field_3.field_17.field_1.field_8,\n          },\n          geospatial_types: column.field_3.field_17.field_2,\n        },\n      },\n      offset_index_offset: column.field_4,\n      offset_index_length: column.field_5,\n      column_index_offset: column.field_6,\n      column_index_length: column.field_7,\n      crypto_metadata: column.field_8,\n      encrypted_column_metadata: column.field_9,\n    })),\n    total_byte_size: rowGroup.field_2,\n    num_rows: rowGroup.field_3,\n    sorting_columns: rowGroup.field_4?.map((/** @type {any} */ sortingColumn) => ({\n      column_idx: sortingColumn.field_1,\n      descending: sortingColumn.field_2,\n      nulls_first: sortingColumn.field_3,\n    })),\n    file_offset: rowGroup.field_5,\n    total_compressed_size: rowGroup.field_6,\n    ordinal: rowGroup.field_7,\n  }))\n  /** @type {KeyValue[] | undefined} */\n  const key_value_metadata = metadata.field_5?.map((/** @type {any} */ kv) => ({\n    key: decode(kv.field_1),\n    value: decode(kv.field_2),\n  }))\n  const created_by = decode(metadata.field_6)\n\n  if (geoparquet) {\n    markGeoColumns(schema, key_value_metadata)\n  }\n\n  return {\n    version,\n    schema,\n    num_rows,\n    row_groups,\n    key_value_metadata,\n    created_by,\n    metadata_length: metadataLength,\n  }\n}\n\n/**\n * Return a tree of schema elements from parquet metadata.\n *\n * @param {{schema: SchemaElement[]}} metadata parquet metadata object\n * @returns {SchemaTree} tree of schema elements\n */\nexport function parquetSchema({ schema }) {\n  return getSchemaPath(schema, [])[0]\n}\n\n/**\n * @param {any} logicalType\n * @returns {LogicalType | undefined}\n */\nfunction logicalType(logicalType) {\n  if (logicalType?.field_1) return { type: 'STRING' }\n  if (logicalType?.field_2) return { type: 'MAP' }\n  if (logicalType?.field_3) return { type: 'LIST' }\n  if (logicalType?.field_4) return { type: 'ENUM' }\n  if (logicalType?.field_5) return {\n    type: 'DECIMAL',\n    scale: logicalType.field_5.field_1,\n    precision: logicalType.field_5.field_2,\n  }\n  if (logicalType?.field_6) return { type: 'DATE' }\n  if (logicalType?.field_7) return {\n    type: 'TIME',\n    isAdjustedToUTC: logicalType.field_7.field_1,\n    unit: timeUnit(logicalType.field_7.field_2),\n  }\n  if (logicalType?.field_8) return {\n    type: 'TIMESTAMP',\n    isAdjustedToUTC: logicalType.field_8.field_1,\n    unit: timeUnit(logicalType.field_8.field_2),\n  }\n  if (logicalType?.field_10) return {\n    type: 'INTEGER',\n    bitWidth: logicalType.field_10.field_1,\n    isSigned: logicalType.field_10.field_2,\n  }\n  if (logicalType?.field_11) return { type: 'NULL' }\n  if (logicalType?.field_12) return { type: 'JSON' }\n  if (logicalType?.field_13) return { type: 'BSON' }\n  if (logicalType?.field_14) return { type: 'UUID' }\n  if (logicalType?.field_15) return { type: 'FLOAT16' }\n  if (logicalType?.field_16) return {\n    type: 'VARIANT',\n    specification_version: logicalType.field_16.field_1,\n  }\n  if (logicalType?.field_17) return {\n    type: 'GEOMETRY',\n    crs: decode(logicalType.field_17.field_1),\n  }\n  if (logicalType?.field_18) return {\n    type: 'GEOGRAPHY',\n    crs: decode(logicalType.field_18.field_1),\n    algorithm: EdgeInterpolationAlgorithms[logicalType.field_18.field_2],\n  }\n  return logicalType\n}\n\n/**\n * @param {any} unit\n * @returns {TimeUnit}\n */\nfunction timeUnit(unit) {\n  if (unit.field_1) return 'MILLIS'\n  if (unit.field_2) return 'MICROS'\n  if (unit.field_3) return 'NANOS'\n  throw new Error('parquet time unit required')\n}\n\n/**\n * Convert column statistics based on column type.\n *\n * @import {AsyncBuffer, FileMetaData, LogicalType, MetadataOptions, MinMaxType, ParquetParsers, SchemaElement, SchemaTree, Statistics, TimeUnit} from '../src/types.d.ts'\n * @param {any} stats\n * @param {SchemaElement} schema\n * @param {ParquetParsers} parsers\n * @returns {Statistics}\n */\nfunction convertStats(stats, schema, parsers) {\n  return stats && {\n    max: convertMetadata(stats.field_1, schema, parsers),\n    min: convertMetadata(stats.field_2, schema, parsers),\n    null_count: stats.field_3,\n    distinct_count: stats.field_4,\n    max_value: convertMetadata(stats.field_5, schema, parsers),\n    min_value: convertMetadata(stats.field_6, schema, parsers),\n    is_max_value_exact: stats.field_7,\n    is_min_value_exact: stats.field_8,\n  }\n}\n\n/**\n * @param {Uint8Array | undefined} value\n * @param {SchemaElement} schema\n * @param {ParquetParsers} parsers\n * @returns {MinMaxType | undefined}\n */\nexport function convertMetadata(value, schema, parsers) {\n  const { type, converted_type, logical_type } = schema\n  if (value === undefined) return value\n  if (type === 'BOOLEAN') return value[0] === 1\n  if (type === 'BYTE_ARRAY') return parsers.stringFromBytes(value)\n  const view = new DataView(value.buffer, value.byteOffset, value.byteLength)\n  if (type === 'FLOAT' && view.byteLength === 4) return view.getFloat32(0, true)\n  if (type === 'DOUBLE' && view.byteLength === 8) return view.getFloat64(0, true)\n  if (type === 'INT32' && converted_type === 'DATE') return parsers.dateFromDays(view.getInt32(0, true))\n  if (type === 'INT64' && converted_type === 'TIMESTAMP_MILLIS') return parsers.timestampFromMilliseconds(view.getBigInt64(0, true))\n  if (type === 'INT64' && converted_type === 'TIMESTAMP_MICROS') return parsers.timestampFromMicroseconds(view.getBigInt64(0, true))\n  if (type === 'INT64' && logical_type?.type === 'TIMESTAMP' && logical_type?.unit === 'NANOS') return parsers.timestampFromNanoseconds(view.getBigInt64(0, true))\n  if (type === 'INT64' && logical_type?.type === 'TIMESTAMP' && logical_type?.unit === 'MICROS') return parsers.timestampFromMicroseconds(view.getBigInt64(0, true))\n  if (type === 'INT64' && logical_type?.type === 'TIMESTAMP') return parsers.timestampFromMilliseconds(view.getBigInt64(0, true))\n  if (type === 'INT32' && view.byteLength === 4) return view.getInt32(0, true)\n  if (type === 'INT64' && view.byteLength === 8) return view.getBigInt64(0, true)\n  if (converted_type === 'DECIMAL') return parseDecimal(value) * 10 ** -(schema.scale || 0)\n  if (logical_type?.type === 'FLOAT16') return parseFloat16(value)\n  if (type === 'FIXED_LEN_BYTE_ARRAY') return value\n  // assert(false)\n  return value\n}\n", "import { BoundaryOrders } from './constants.js'\nimport { DEFAULT_PARSERS } from './convert.js'\nimport { convertMetadata } from './metadata.js'\nimport { deserializeTCompactProtocol } from './thrift.js'\n\n/**\n * @param {DataReader} reader\n * @param {SchemaElement} schema\n * @param {ParquetParsers | undefined} parsers\n * @returns {ColumnIndex}\n */\nexport function readColumnIndex(reader, schema, parsers = undefined) {\n  parsers = { ...DEFAULT_PARSERS, ...parsers }\n\n  const thrift = deserializeTCompactProtocol(reader)\n  return {\n    null_pages: thrift.field_1,\n    min_values: thrift.field_2.map((/** @type {any} */ m) => convertMetadata(m, schema, parsers)),\n    max_values: thrift.field_3.map((/** @type {any} */ m) => convertMetadata(m, schema, parsers)),\n    boundary_order: BoundaryOrders[thrift.field_4],\n    null_counts: thrift.field_5,\n    repetition_level_histograms: thrift.field_6,\n    definition_level_histograms: thrift.field_7,\n  }\n}\n\n/**\n * @param {DataReader} reader\n * @returns {OffsetIndex}\n */\nexport function readOffsetIndex(reader) {\n  const thrift = deserializeTCompactProtocol(reader)\n  return {\n    page_locations: thrift.field_1.map(pageLocation),\n    unencoded_byte_array_data_bytes: thrift.field_2,\n  }\n}\n\n/**\n * @import {ColumnIndex, DataReader, OffsetIndex, PageLocation, ParquetParsers, SchemaElement} from '../src/types.d.ts'\n * @param {any} loc\n * @returns {PageLocation}\n */\nfunction pageLocation(loc) {\n  return {\n    offset: loc.field_1,\n    compressed_page_size: loc.field_2,\n    first_row_index: loc.field_3,\n  }\n}\n", "import { defaultInitialFetchSize } from './metadata.js'\n\n/**\n * Replace bigint, date, etc with legal JSON types.\n *\n * @param {any} obj object to convert\n * @returns {unknown} converted object\n */\nexport function toJson(obj) {\n  if (obj === undefined) return null\n  if (typeof obj === 'bigint') return Number(obj)\n  if (Array.isArray(obj)) return obj.map(toJson)\n  if (obj instanceof Uint8Array) return Array.from(obj)\n  if (obj instanceof Date) return obj.toISOString()\n  if (obj instanceof Object) {\n    /** @type {Record<string, unknown>} */\n    const newObj = {}\n    for (const key of Object.keys(obj)) {\n      if (obj[key] === undefined) continue\n      newObj[key] = toJson(obj[key])\n    }\n    return newObj\n  }\n  return obj\n}\n\n/**\n * Concatenate two arrays fast.\n *\n * @param {any[]} aaa\n * @param {DecodedArray} bbb\n */\nexport function concat(aaa, bbb) {\n  const chunk = 10000\n  for (let i = 0; i < bbb.length; i += chunk) {\n    aaa.push(...bbb.slice(i, i + chunk))\n  }\n}\n\n/**\n * Deep equality.\n *\n * @param {any} a\n * @param {any} b\n * @param {boolean} [strict]\n * @returns {boolean}\n */\nexport function equals(a, b, strict = true) {\n  // eslint-disable-next-line eqeqeq\n  if (strict ? a === b : a == b) return true\n  if (a instanceof Uint8Array && b instanceof Uint8Array) return equals(Array.from(a), Array.from(b), strict)\n  if (!a || !b || typeof a !== typeof b) return false\n  if (Array.isArray(a) && Array.isArray(b)) {\n    if (a.length !== b.length) return false\n    for (let i = 0; i < a.length; i++) {\n      if (!equals(a[i], b[i], strict)) return false\n    }\n    return true\n  }\n  if (typeof a !== 'object') return false\n  const aKeys = Object.keys(a)\n  if (aKeys.length !== Object.keys(b).length) return false\n  for (const k of aKeys) {\n    if (!equals(a[k], b[k], strict)) return false\n  }\n  return true\n}\n\n/**\n * Get the byte length using fetch with a ranged GET request.\n * Aborts the request if server returns 200 instead of 206.\n *\n * @param {string} url\n * @param {RequestInit} [requestInit] fetch options\n * @param {typeof globalThis.fetch} [fetchFn] fetch function to use\n * @returns {Promise<number>}\n */\nasync function byteLengthFromUrlUsingFetch(url, requestInit = {}, fetchFn = globalThis.fetch) {\n  const controller = new AbortController()\n  const headers = new Headers(requestInit.headers)\n  headers.set('Range', 'bytes=0-0')\n\n  const res = await fetchFn(url, {\n    ...requestInit,\n    headers,\n    signal: controller.signal,\n  })\n\n  if (!res.ok) throw new Error(`fetch with range failed ${res.status}`)\n\n  // Server supports Range requests (206 Partial Content)\n  if (res.status === 206) {\n    const contentRange = res.headers.get('Content-Range')\n    if (!contentRange) throw new Error('missing content-range header')\n\n    // Parse \"bytes 0-0/9446073\" to get total length\n    const match = contentRange.match(/bytes \\d+-\\d+\\/(\\d+)/)\n    if (!match) throw new Error(`invalid content-range header: ${contentRange}`)\n\n    return parseInt(match[1])\n  }\n\n  // Server ignored Range and returned 200 - get Content-Length and abort request\n  if (res.status === 200) {\n    const contentLength = res.headers.get('Content-Length')\n\n    // Abort the request to stop any ongoing download\n    controller.abort()\n\n    if (contentLength) return parseInt(contentLength)\n  }\n\n  throw new Error('server does not support range requests and missing content-length')\n}\n\n/**\n * Get the byte length of a URL using a HEAD request.\n * If HEAD fails with 403 (e.g., with signed S3 URLs), falls back to a ranged GET request.\n * If HEAD succeeds but Content-Length is missing, falls back to GET with range.\n * If requestInit is provided, it will be passed to fetch.\n *\n * @param {string} url\n * @param {RequestInit} [requestInit] fetch options\n * @param {typeof globalThis.fetch} [customFetch] fetch function to use\n * @returns {Promise<number>}\n */\nexport async function byteLengthFromUrl(url, requestInit, customFetch) {\n  const fetch = customFetch ?? globalThis.fetch\n  const res = await fetch(url, { ...requestInit, method: 'HEAD' })\n\n  // If HEAD request is forbidden (common with signed S3 URLs), try GET with range\n  if (res.status === 403) {\n    return byteLengthFromUrlUsingFetch(url, requestInit, fetch)\n  }\n\n  if (!res.ok) throw new Error(`fetch head failed ${res.status}`)\n  const length = res.headers.get('Content-Length')\n  // If Content-Length is missing from HEAD, fallback to GET with range\n  if (!length) {\n    return byteLengthFromUrlUsingFetch(url, requestInit, fetch)\n  }\n  return parseInt(length)\n}\n\n/**\n * Construct an AsyncBuffer for a URL.\n * If byteLength is not provided, will make a HEAD request to get the file size.\n * If fetch is provided, it will be used instead of the global fetch.\n * If requestInit is provided, it will be passed to fetch.\n *\n * @param {object} options\n * @param {string} options.url\n * @param {number} [options.byteLength]\n * @param {typeof globalThis.fetch} [options.fetch] fetch function to use\n * @param {RequestInit} [options.requestInit]\n * @returns {Promise<AsyncBuffer>}\n */\nexport async function asyncBufferFromUrl({ url, byteLength, requestInit, fetch: customFetch }) {\n  if (!url) throw new Error('missing url')\n  const fetch = customFetch ?? globalThis.fetch\n  // byte length from HEAD request\n  byteLength ??= await byteLengthFromUrl(url, requestInit, fetch)\n\n  /**\n   * A promise for the whole buffer, if range requests are not supported.\n   * @type {Promise<ArrayBuffer>|undefined}\n   */\n  let buffer = undefined\n  const init = requestInit || {}\n\n  return {\n    byteLength,\n    async slice(start, end) {\n      if (buffer) {\n        return buffer.then(buffer => buffer.slice(start, end))\n      }\n\n      const headers = new Headers(init.headers)\n      const endStr = end === undefined ? '' : end - 1\n      headers.set('Range', `bytes=${start}-${endStr}`)\n\n      const res = await fetch(url, { ...init, headers })\n      if (!res.ok || !res.body) throw new Error(`fetch failed ${res.status}`)\n\n      if (res.status === 200) {\n        // Endpoint does not support range requests and returned the whole object\n        buffer = res.arrayBuffer()\n        return buffer.then(buffer => buffer.slice(start, end))\n      } else if (res.status === 206) {\n        // The endpoint supports range requests and sent us the requested range\n        return res.arrayBuffer()\n      } else {\n        throw new Error(`fetch received unexpected status code ${res.status}`)\n      }\n    },\n  }\n}\n\n/**\n * Returns a cached layer on top of an AsyncBuffer. For caching slices of a file\n * that are read multiple times, possibly over a network.\n *\n * @param {AsyncBuffer} file file-like object to cache\n * @param {{ minSize?: number }} [options]\n * @returns {AsyncBuffer} cached file-like object\n */\nexport function cachedAsyncBuffer({ byteLength, slice }, { minSize = defaultInitialFetchSize } = {}) {\n  if (byteLength < minSize) {\n    // Cache whole file if it's small\n    const buffer = slice(0, byteLength)\n    return {\n      byteLength,\n      async slice(start, end) {\n        return (await buffer).slice(start, end)\n      },\n    }\n  }\n  const cache = new Map()\n  return {\n    byteLength,\n    /**\n     * @param {number} start\n     * @param {number} [end]\n     * @returns {Awaitable<ArrayBuffer>}\n     */\n    slice(start, end) {\n      const key = cacheKey(start, end, byteLength)\n      const cached = cache.get(key)\n      if (cached) return cached\n      // cache miss, read from file\n      const promise = slice(start, end)\n      cache.set(key, promise)\n      return promise\n    },\n  }\n}\n\n\n/**\n * Returns canonical cache key for a byte range 'start,end'.\n * Normalize int-range and suffix-range requests to the same key.\n *\n * @import {AsyncBuffer, Awaitable, DecodedArray} from '../src/types.d.ts'\n * @param {number} start start byte of range\n * @param {number} [end] end byte of range, or undefined for suffix range\n * @param {number} [size] size of file, or undefined for suffix range\n * @returns {string}\n */\nfunction cacheKey(start, end, size) {\n  if (start < 0) {\n    if (end !== undefined) throw new Error(`invalid suffix range [${start}, ${end}]`)\n    if (size === undefined) return `${start},`\n    return `${size + start},${size}`\n  } else if (end !== undefined) {\n    if (start > end) throw new Error(`invalid empty range [${start}, ${end}]`)\n    return `${start},${end}`\n  } else if (size === undefined) {\n    return `${start},`\n  } else {\n    return `${start},${size}`\n  }\n}\n\n/**\n * Flatten a list of lists into a single list.\n *\n * @param {DecodedArray[]} [chunks]\n * @returns {DecodedArray}\n */\nexport function flatten(chunks) {\n  if (!chunks) return []\n  if (chunks.length === 1) return chunks[0]\n  /** @type {any[]} */\n  const output = []\n  for (const chunk of chunks) {\n    concat(output, chunk)\n  }\n  return output\n}\n", "import { equals } from './utils.js'\n\n/**\n * @import {ParquetQueryFilter, RowGroup} from '../src/types.js'\n */\n\n/**\n * Returns an array of column names needed to evaluate the filter.\n *\n * @param {ParquetQueryFilter} [filter]\n * @returns {string[]}\n */\nexport function columnsNeededForFilter(filter) {\n  if (!filter) return []\n  /** @type {string[]} */\n  const columns = []\n  if ('$and' in filter && Array.isArray(filter.$and)) {\n    columns.push(...filter.$and.flatMap(columnsNeededForFilter))\n  } else if ('$or' in filter && Array.isArray(filter.$or)) {\n    columns.push(...filter.$or.flatMap(columnsNeededForFilter))\n  } else if ('$nor' in filter && Array.isArray(filter.$nor)) {\n    columns.push(...filter.$nor.flatMap(columnsNeededForFilter))\n  } else {\n    columns.push(...Object.keys(filter))\n  }\n  return columns\n}\n\n/**\n * Match a record against a query filter\n *\n * @param {Record<string, any>} record\n * @param {ParquetQueryFilter} filter\n * @param {boolean} [strict]\n * @returns {boolean}\n */\nexport function matchFilter(record, filter, strict = true) {\n  if ('$and' in filter && Array.isArray(filter.$and)) {\n    return filter.$and.every(subQuery => matchFilter(record, subQuery, strict))\n  }\n  if ('$or' in filter && Array.isArray(filter.$or)) {\n    return filter.$or.some(subQuery => matchFilter(record, subQuery, strict))\n  }\n  if ('$nor' in filter && Array.isArray(filter.$nor)) {\n    return !filter.$nor.some(subQuery => matchFilter(record, subQuery, strict))\n  }\n\n  return Object.entries(filter).every(([field, condition]) => {\n    const value = record[field]\n\n    // implicit $eq for non-object conditions\n    if (typeof condition !== 'object' || condition === null || Array.isArray(condition)) {\n      return equals(value, condition, strict)\n    }\n\n    return Object.entries(condition || {}).every(([operator, target]) => {\n      if (operator === '$gt') return value > target\n      if (operator === '$gte') return value >= target\n      if (operator === '$lt') return value < target\n      if (operator === '$lte') return value <= target\n      if (operator === '$eq') return equals(value, target, strict)\n      if (operator === '$ne') return !equals(value, target, strict)\n      if (operator === '$in') return Array.isArray(target) && target.includes(value)\n      if (operator === '$nin') return Array.isArray(target) && !target.includes(value)\n      if (operator === '$not') return !matchFilter({ [field]: value }, { [field]: target }, strict)\n      return true\n    })\n  })\n}\n\n/**\n * Check if a row group can be skipped based on filter and column statistics.\n *\n * @param {object} options\n * @param {RowGroup} options.rowGroup\n * @param {string[]} options.physicalColumns\n * @param {ParquetQueryFilter | undefined} options.filter\n * @param {boolean} [options.strict]\n * @returns {boolean} true if the row group can be skipped\n */\nexport function canSkipRowGroup({ rowGroup, physicalColumns, filter, strict = true }) {\n  if (!filter) return false\n\n  // Handle logical operators\n  if ('$and' in filter && Array.isArray(filter.$and)) {\n    // For AND, we can skip if ANY condition allows skipping\n    return filter.$and.some(subFilter => canSkipRowGroup({ rowGroup, physicalColumns, filter: subFilter, strict }))\n  }\n  if ('$or' in filter && Array.isArray(filter.$or)) {\n    // For OR, we can skip only if ALL conditions allow skipping\n    return filter.$or.every(subFilter => canSkipRowGroup({ rowGroup, physicalColumns, filter: subFilter, strict }))\n  }\n  if ('$nor' in filter && Array.isArray(filter.$nor)) {\n    // For NOR, we can skip if none of the conditions allow skipping\n    // This is complex, so we'll be conservative and not skip\n    return false\n  }\n\n  // Check column filters\n  for (const [field, condition] of Object.entries(filter)) {\n    // Find the column chunk for this field\n    const columnIndex = physicalColumns.indexOf(field)\n    if (columnIndex === -1) continue\n\n    const stats = rowGroup.columns[columnIndex].meta_data?.statistics\n    if (!stats) continue // No statistics available, can't skip\n\n    const { min, max, min_value, max_value } = stats\n    const minVal = min_value !== undefined ? min_value : min\n    const maxVal = max_value !== undefined ? max_value : max\n\n    if (minVal === undefined || maxVal === undefined) continue\n\n    // Handle operators\n    for (const [operator, target] of Object.entries(condition || {})) {\n      if (operator === '$gt' && maxVal <= target) return true\n      if (operator === '$gte' && maxVal < target) return true\n      if (operator === '$lt' && minVal >= target) return true\n      if (operator === '$lte' && minVal > target) return true\n      if (operator === '$eq' && (target < minVal || target > maxVal)) return true\n      if (operator === '$ne' && equals(minVal, maxVal, strict) && equals(minVal, target, strict)) return true\n      if (operator === '$in' && Array.isArray(target) && target.every(v => v < minVal || v > maxVal)) return true\n      if (operator === '$nin' && Array.isArray(target) && equals(minVal, maxVal, strict) && target.includes(minVal)) return true\n    }\n  }\n\n  return false\n}\n", "import { canSkipRowGroup } from './filter.js'\nimport { parquetSchema } from './metadata.js'\nimport { getPhysicalColumns } from './schema.js'\n\n// Combine column chunks if less than 2mb\nconst runLimit = 1 << 21 // 2mb\n\n/**\n * @import {AsyncBuffer, ByteRange, ChunkPlan, GroupPlan, ParquetReadOptions, QueryPlan} from '../src/types.js'\n */\n/**\n * Plan which byte ranges to read to satisfy a read request.\n * Metadata must be non-null.\n *\n * @param {ParquetReadOptions} options\n * @returns {QueryPlan}\n */\nexport function parquetPlan({ metadata, rowStart = 0, rowEnd = Infinity, columns, filter, filterStrict = true, useOffsetIndex = false }) {\n  if (!metadata) throw new Error('parquetPlan requires metadata')\n  /** @type {GroupPlan[]} */\n  const groups = []\n  /** @type {ByteRange[]} */\n  const fetches = []\n  /** @type {ByteRange[]} */\n  const indexes = []\n  const physicalColumns = getPhysicalColumns(parquetSchema(metadata))\n\n  // find which row groups to read\n  let groupStart = 0 // first row index of the current group\n  for (const rowGroup of metadata.row_groups) {\n    const groupRows = Number(rowGroup.num_rows)\n    const groupEnd = groupStart + groupRows\n    // if row group overlaps with row range, add it to the plan\n    if (groupRows > 0 && groupEnd > rowStart && groupStart < rowEnd && !canSkipRowGroup({ rowGroup, physicalColumns, filter, strict: filterStrict })) {\n      /** @type {ChunkPlan[]} */\n      const chunks = []\n      let groupStartByte = Infinity\n      let groupEndByte = -Infinity\n      // loop through each column chunk\n      for (const chunk of rowGroup.columns) {\n        const meta = chunk.meta_data\n        if (chunk.file_path) throw new Error('parquet file_path not supported')\n        if (!meta) throw new Error('parquet column metadata is undefined')\n        // add included column chunks to the plan\n        if (!columns || columns.includes(meta.path_in_schema[0])) {\n          // full column chunk\n          const columnOffset = meta.dictionary_page_offset || meta.data_page_offset\n          const startByte = Number(columnOffset)\n          const endByte = Number(columnOffset + meta.total_compressed_size)\n          // update group byte range\n          if (startByte < groupStartByte) groupStartByte = startByte\n          if (endByte > groupEndByte) groupEndByte = endByte\n\n          if (useOffsetIndex && chunk.offset_index_offset && chunk.offset_index_length) {\n            const offsetIndexStart = Number(chunk.offset_index_offset)\n            chunks.push({\n              columnMetadata: meta,\n              offsetIndex: {\n                startByte: offsetIndexStart,\n                endByte: offsetIndexStart + chunk.offset_index_length,\n              },\n              bounds: { startByte, endByte },\n            })\n          } else {\n            chunks.push({\n              columnMetadata: meta,\n              range: { startByte, endByte },\n            })\n          }\n\n        }\n      }\n      const selectStart = Math.max(rowStart - groupStart, 0)\n      const selectEnd = Math.min(rowEnd - groupStart, groupRows)\n      groups.push({ chunks, rowGroup, groupStart, groupRows, selectStart, selectEnd })\n\n      // combine runs of column chunks\n      /** @type {ByteRange | undefined} */\n      let run\n      for (const chunk of chunks) {\n        if ('offsetIndex' in chunk) {\n          indexes.push(chunk.offsetIndex)\n        } else {\n          const { range } = chunk\n          if (columns) {\n            fetches.push(range)\n          } else if (run && range.endByte - run.startByte <= runLimit) {\n            // extend range\n            run.endByte = range.endByte\n          } else {\n            // new range\n            if (run) fetches.push(run)\n            run = { ...range }\n          }\n        }\n      }\n      if (run) fetches.push(run)\n    }\n\n    groupStart = groupEnd\n  }\n  if (!isFinite(rowEnd)) rowEnd = groupStart\n  fetches.push(...indexes)\n\n  return { metadata, rowStart, rowEnd, columns, fetches, groups }\n}\n\n/**\n * Prefetch byte ranges from an AsyncBuffer.\n *\n * @param {AsyncBuffer} file\n * @param {QueryPlan} plan\n * @returns {AsyncBuffer}\n */\nexport function prefetchAsyncBuffer(file, { fetches }) {\n  // fetch byte ranges from the file\n  const promises = fetches.map(({ startByte, endByte }) => file.slice(startByte, endByte))\n  return {\n    byteLength: file.byteLength,\n    slice(start, end = file.byteLength) {\n      // find matching slice\n      const index = fetches.findIndex(({ startByte, endByte }) => startByte <= start && end <= endByte)\n      if (index < 0) {\n        // fallback to direct read\n        return file.slice(start, end)\n      }\n      if (fetches[index].startByte !== start || fetches[index].endByte !== end) {\n        // slice a subrange of the prefetch\n        const startOffset = start - fetches[index].startByte\n        const endOffset = end - fetches[index].startByte\n        if (promises[index] instanceof Promise) {\n          return promises[index].then(buffer => buffer.slice(startOffset, endOffset))\n        } else {\n          return promises[index].slice(startOffset, endOffset)\n        }\n      } else {\n        return promises[index]\n      }\n    },\n  }\n}\n", "import { getMaxDefinitionLevel, isListLike, isMapLike } from './schema.js'\n\n/**\n * Reconstructs a complex nested structure from flat arrays of values and\n * definition and repetition levels, according to Dremel encoding.\n *\n * @param {any[]} output\n * @param {number[] | undefined} definitionLevels\n * @param {number[]} repetitionLevels\n * @param {DecodedArray} values\n * @param {SchemaTree[]} schemaPath\n * @returns {DecodedArray}\n */\nexport function assembleLists(output, definitionLevels, repetitionLevels, values, schemaPath) {\n  const n = definitionLevels?.length || repetitionLevels.length\n  if (!n) return values\n  const maxDefinitionLevel = getMaxDefinitionLevel(schemaPath)\n  const repetitionPath = schemaPath.map(({ element }) => element.repetition_type)\n  let valueIndex = 0\n\n  // Track state of nested structures\n  const containerStack = [output]\n  let currentContainer = output\n  let currentDepth = 0 // schema depth\n  let currentDefLevel = 0 // list depth\n  let currentRepLevel = 0\n\n  if (repetitionLevels[0]) {\n    // continue previous row\n    while (currentDepth < repetitionPath.length - 2 && currentRepLevel < repetitionLevels[0]) {\n      currentDepth++\n      if (repetitionPath[currentDepth] !== 'REQUIRED') {\n        // go into last list\n        currentContainer = currentContainer.at(-1)\n        containerStack.push(currentContainer)\n        currentDefLevel++\n      }\n      if (repetitionPath[currentDepth] === 'REPEATED') currentRepLevel++\n    }\n  }\n\n  for (let i = 0; i < n; i++) {\n    // assert(currentDefLevel === containerStack.length - 1)\n    const def = definitionLevels?.length ? definitionLevels[i] : maxDefinitionLevel\n    const rep = repetitionLevels[i]\n\n    // Pop up to start of rep level\n    while (currentDepth && (rep < currentRepLevel || repetitionPath[currentDepth] !== 'REPEATED')) {\n      if (repetitionPath[currentDepth] !== 'REQUIRED') {\n        containerStack.pop()\n        currentDefLevel--\n      }\n      if (repetitionPath[currentDepth] === 'REPEATED') currentRepLevel--\n      currentDepth--\n    }\n    // @ts-expect-error won't be empty\n    currentContainer = containerStack.at(-1)\n\n    // Go deeper to end of definition level\n    while (\n      (currentDepth < repetitionPath.length - 2 || repetitionPath[currentDepth + 1] === 'REPEATED') &&\n      (currentDefLevel < def || repetitionPath[currentDepth + 1] === 'REQUIRED')\n    ) {\n      currentDepth++\n      if (repetitionPath[currentDepth] !== 'REQUIRED') {\n        /** @type {any[]} */\n        const newList = []\n        currentContainer.push(newList)\n        currentContainer = newList\n        containerStack.push(newList)\n        currentDefLevel++\n      }\n      if (repetitionPath[currentDepth] === 'REPEATED') currentRepLevel++\n    }\n\n    // Add value or null based on definition level\n    if (def === maxDefinitionLevel) {\n      // assert(currentDepth === maxDefinitionLevel || currentDepth === repetitionPath.length - 2)\n      currentContainer.push(values[valueIndex++])\n    } else if (currentDepth === repetitionPath.length - 2) {\n      currentContainer.push(null)\n    } else {\n      currentContainer.push([])\n    }\n  }\n\n  // Handle edge cases for empty inputs or single-level data\n  if (!output.length) {\n    // return max definition level of nested lists\n    for (let i = 0; i < maxDefinitionLevel; i++) {\n      /** @type {any[]} */\n      const newList = []\n      currentContainer.push(newList)\n      currentContainer = newList\n    }\n  }\n\n  return output\n}\n\n/**\n * Assemble a nested structure from subcolumn data.\n * https://github.com/apache/parquet-format/blob/apache-parquet-format-2.10.0/LogicalTypes.md#nested-types\n *\n * @param {Map<string, DecodedArray>} subcolumnData\n * @param {SchemaTree} schema top-level schema element\n * @param {number} [depth] depth of nested structure\n */\nexport function assembleNested(subcolumnData, schema, depth = 0) {\n  const path = schema.path.join('.')\n  const optional = schema.element.repetition_type === 'OPTIONAL'\n  const nextDepth = optional ? depth + 1 : depth\n\n  if (isListLike(schema)) {\n    let sublist = schema.children[0]\n    let subDepth = nextDepth\n    if (sublist.children.length === 1) {\n      sublist = sublist.children[0]\n      subDepth++\n    }\n    assembleNested(subcolumnData, sublist, subDepth)\n\n    const subcolumn = sublist.path.join('.')\n    const values = subcolumnData.get(subcolumn)\n    if (!values) throw new Error('parquet list column missing values')\n    if (optional) flattenAtDepth(values, depth)\n    subcolumnData.set(path, values)\n    subcolumnData.delete(subcolumn)\n    return\n  }\n\n  if (isMapLike(schema)) {\n    const mapName = schema.children[0].element.name\n\n    // Assemble keys and values\n    assembleNested(subcolumnData, schema.children[0].children[0], nextDepth + 1)\n    assembleNested(subcolumnData, schema.children[0].children[1], nextDepth + 1)\n\n    const keys = subcolumnData.get(`${path}.${mapName}.key`)\n    const values = subcolumnData.get(`${path}.${mapName}.value`)\n\n    if (!keys) throw new Error('parquet map column missing keys')\n    if (!values) throw new Error('parquet map column missing values')\n    if (keys.length !== values.length) {\n      throw new Error('parquet map column key/value length mismatch')\n    }\n\n    const out = assembleMaps(keys, values, nextDepth)\n    if (optional) flattenAtDepth(out, depth)\n\n    subcolumnData.delete(`${path}.${mapName}.key`)\n    subcolumnData.delete(`${path}.${mapName}.value`)\n    subcolumnData.set(path, out)\n    return\n  }\n\n  // Struct-like column\n  if (schema.children.length) {\n    // construct a meta struct and then invert\n    const invertDepth = schema.element.repetition_type === 'REQUIRED' ? depth : depth + 1\n    /** @type {Record<string, any>} */\n    const struct = {}\n    for (const child of schema.children) {\n      assembleNested(subcolumnData, child, invertDepth)\n      const childData = subcolumnData.get(child.path.join('.'))\n      if (!childData) throw new Error('parquet struct missing child data')\n      struct[child.element.name] = childData\n    }\n    // remove children\n    for (const child of schema.children) {\n      subcolumnData.delete(child.path.join('.'))\n    }\n    // invert struct by depth\n    const inverted = invertStruct(struct, invertDepth)\n    if (optional) flattenAtDepth(inverted, depth)\n    subcolumnData.set(path, inverted)\n  }\n}\n\n/**\n * @import {DecodedArray, SchemaTree} from '../src/types.d.ts'\n * @param {DecodedArray} arr\n * @param {number} depth\n */\nfunction flattenAtDepth(arr, depth) {\n  for (let i = 0; i < arr.length; i++) {\n    if (depth) {\n      flattenAtDepth(arr[i], depth - 1)\n    } else {\n      arr[i] = arr[i][0]\n    }\n  }\n}\n\n/**\n * @param {DecodedArray} keys\n * @param {DecodedArray} values\n * @param {number} depth\n * @returns {any[]}\n */\nfunction assembleMaps(keys, values, depth) {\n  const out = []\n  for (let i = 0; i < keys.length; i++) {\n    if (depth) {\n      out.push(assembleMaps(keys[i], values[i], depth - 1)) // go deeper\n    } else {\n      if (keys[i]) {\n        /** @type {Record<string, any>} */\n        const obj = {}\n        for (let j = 0; j < keys[i].length; j++) {\n          const value = values[i][j]\n          obj[keys[i][j]] = value === undefined ? null : value\n        }\n        out.push(obj)\n      } else {\n        out.push(undefined)\n      }\n    }\n  }\n  return out\n}\n\n/**\n * Invert a struct-like object by depth.\n *\n * @param {Record<string, any[]>} struct\n * @param {number} depth\n * @returns {any[]}\n */\nfunction invertStruct(struct, depth) {\n  const keys = Object.keys(struct)\n  const length = struct[keys[0]]?.length\n  const out = []\n  for (let i = 0; i < length; i++) {\n    /** @type {Record<string, any>} */\n    const obj = {}\n    for (const key of keys) {\n      if (struct[key].length !== length) throw new Error('parquet struct parsing error')\n      obj[key] = struct[key][i]\n    }\n    if (depth) {\n      out.push(invertStruct(obj, depth - 1)) // deeper\n    } else {\n      out.push(obj)\n    }\n  }\n  return out\n}\n", "import { readVarInt, readZigZagBigInt } from './thrift.js'\n\n/**\n * @import {DataReader} from '../src/types.d.ts'\n * @param {DataReader} reader\n * @param {number} count number of values to read\n * @param {Int32Array | BigInt64Array} output\n */\nexport function deltaBinaryUnpack(reader, count, output) {\n  const int32 = output instanceof Int32Array\n  const blockSize = readVarInt(reader)\n  const miniblockPerBlock = readVarInt(reader)\n  readVarInt(reader) // assert(=== count)\n  let value = readZigZagBigInt(reader) // first value\n  let outputIndex = 0\n  output[outputIndex++] = int32 ? Number(value) : value\n\n  const valuesPerMiniblock = blockSize / miniblockPerBlock\n\n  while (outputIndex < count) {\n    // new block\n    const minDelta = readZigZagBigInt(reader)\n    const bitWidths = new Uint8Array(miniblockPerBlock)\n    for (let i = 0; i < miniblockPerBlock; i++) {\n      bitWidths[i] = reader.view.getUint8(reader.offset++)\n    }\n\n    for (let i = 0; i < miniblockPerBlock && outputIndex < count; i++) {\n      // new miniblock\n      const bitWidth = BigInt(bitWidths[i])\n      if (bitWidth) {\n        let bitpackPos = 0n\n        let miniblockCount = valuesPerMiniblock\n        const mask = (1n << bitWidth) - 1n\n        while (miniblockCount && outputIndex < count) {\n          let bits = BigInt(reader.view.getUint8(reader.offset)) >> bitpackPos & mask // TODO: don't re-read value every time\n          bitpackPos += bitWidth\n          while (bitpackPos >= 8) {\n            bitpackPos -= 8n\n            reader.offset++\n            if (bitpackPos) {\n              bits |= BigInt(reader.view.getUint8(reader.offset)) << bitWidth - bitpackPos & mask\n            }\n          }\n          const delta = minDelta + bits\n          value += delta\n          output[outputIndex++] = int32 ? Number(value) : value\n          miniblockCount--\n        }\n        if (miniblockCount) {\n          // consume leftover miniblock\n          reader.offset += Math.ceil((miniblockCount * Number(bitWidth) + Number(bitpackPos)) / 8)\n        }\n      } else {\n        for (let j = 0; j < valuesPerMiniblock && outputIndex < count; j++) {\n          value += minDelta\n          output[outputIndex++] = int32 ? Number(value) : value\n        }\n      }\n    }\n  }\n}\n\n/**\n * @param {DataReader} reader\n * @param {number} count\n * @param {Uint8Array[]} output\n */\nexport function deltaLengthByteArray(reader, count, output) {\n  const lengths = new Int32Array(count)\n  deltaBinaryUnpack(reader, count, lengths)\n  for (let i = 0; i < count; i++) {\n    output[i] = new Uint8Array(reader.view.buffer, reader.view.byteOffset + reader.offset, lengths[i])\n    reader.offset += lengths[i]\n  }\n}\n\n/**\n * @param {DataReader} reader\n * @param {number} count\n * @param {Uint8Array[]} output\n */\nexport function deltaByteArray(reader, count, output) {\n  const prefixData = new Int32Array(count)\n  deltaBinaryUnpack(reader, count, prefixData)\n  const suffixData = new Int32Array(count)\n  deltaBinaryUnpack(reader, count, suffixData)\n\n  for (let i = 0; i < count; i++) {\n    const suffix = new Uint8Array(reader.view.buffer, reader.view.byteOffset + reader.offset, suffixData[i])\n    if (prefixData[i]) {\n      // copy from previous value\n      output[i] = new Uint8Array(prefixData[i] + suffixData[i])\n      output[i].set(output[i - 1].subarray(0, prefixData[i]))\n      output[i].set(suffix, prefixData[i])\n    } else {\n      output[i] = suffix\n    }\n    reader.offset += suffixData[i]\n  }\n}\n", "import { readVarInt } from './thrift.js'\n\n/**\n * Minimum bits needed to store value.\n *\n * @param {number} value\n * @returns {number}\n */\nexport function bitWidth(value) {\n  return 32 - Math.clz32(value)\n}\n\n/**\n * Read values from a run-length encoded/bit-packed hybrid encoding.\n *\n * If length is zero, then read int32 length at the start.\n *\n * @param {DataReader} reader\n * @param {number} width - bitwidth\n * @param {DecodedArray} output\n * @param {number} [length] - length of the encoded data\n */\nexport function readRleBitPackedHybrid(reader, width, output, length) {\n  if (length === undefined) {\n    length = reader.view.getUint32(reader.offset, true)\n    reader.offset += 4\n  }\n  const startOffset = reader.offset\n  let seen = 0\n  while (seen < output.length) {\n    const header = readVarInt(reader)\n    if (header & 1) {\n      // bit-packed\n      seen = readBitPacked(reader, header, width, output, seen)\n    } else {\n      // rle\n      const count = header >>> 1\n      readRle(reader, count, width, output, seen)\n      seen += count\n    }\n  }\n  reader.offset = startOffset + length // duckdb writes an empty block\n}\n\n/**\n * Run-length encoding: read value with bitWidth and repeat it count times.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @param {number} bitWidth\n * @param {DecodedArray} output\n * @param {number} seen\n */\nfunction readRle(reader, count, bitWidth, output, seen) {\n  const width = bitWidth + 7 >> 3\n  let value = 0\n  for (let i = 0; i < width; i++) {\n    value |= reader.view.getUint8(reader.offset++) << (i << 3)\n  }\n  // assert(value < 1 << bitWidth)\n\n  // repeat value count times\n  for (let i = 0; i < count; i++) {\n    output[seen + i] = value\n  }\n}\n\n/**\n * Read a bit-packed run of the rle/bitpack hybrid.\n * Supports width > 8 (crossing bytes).\n *\n * @param {DataReader} reader\n * @param {number} header - bit-pack header\n * @param {number} bitWidth\n * @param {DecodedArray} output\n * @param {number} seen\n * @returns {number} total output values so far\n */\nfunction readBitPacked(reader, header, bitWidth, output, seen) {\n  let count = header >> 1 << 3 // values to read\n  const mask = (1 << bitWidth) - 1\n\n  let data = 0\n  if (reader.offset < reader.view.byteLength) {\n    data = reader.view.getUint8(reader.offset++)\n  } else if (mask) {\n    // sometimes out-of-bounds reads are masked out\n    throw new Error(`parquet bitpack offset ${reader.offset} out of range`)\n  }\n  let left = 8\n  let right = 0\n\n  // read values\n  while (count) {\n    // if we have crossed a byte boundary, shift the data\n    if (right > 8) {\n      right -= 8\n      left -= 8\n      data >>>= 8\n    } else if (left - right < bitWidth) {\n      // if we don't have bitWidth number of bits to read, read next byte\n      data |= reader.view.getUint8(reader.offset) << left\n      reader.offset++\n      left += 8\n    } else {\n      if (seen < output.length) {\n        // emit value\n        output[seen++] = data >> right & mask\n      }\n      count--\n      right += bitWidth\n    }\n  }\n\n  return seen\n}\n\n/**\n * @param {DataReader} reader\n * @param {number} count\n * @param {ParquetType} type\n * @param {number | undefined} typeLength\n * @returns {DecodedArray}\n */\nexport function byteStreamSplit(reader, count, type, typeLength) {\n  const width = byteWidth(type, typeLength)\n  const bytes = new Uint8Array(count * width)\n  for (let b = 0; b < width; b++) {\n    for (let i = 0; i < count; i++) {\n      bytes[i * width + b] = reader.view.getUint8(reader.offset++)\n    }\n  }\n  // interpret bytes as typed array\n  if (type === 'FLOAT') return new Float32Array(bytes.buffer)\n  else if (type === 'DOUBLE') return new Float64Array(bytes.buffer)\n  else if (type === 'INT32') return new Int32Array(bytes.buffer)\n  else if (type === 'INT64') return new BigInt64Array(bytes.buffer)\n  else if (type === 'FIXED_LEN_BYTE_ARRAY') {\n    // split into arrays of typeLength\n    const split = new Array(count)\n    for (let i = 0; i < count; i++) {\n      split[i] = bytes.subarray(i * width, (i + 1) * width)\n    }\n    return split\n  }\n  throw new Error(`parquet byte_stream_split unsupported type: ${type}`)\n}\n\n/**\n * @import {DataReader, DecodedArray, ParquetType} from '../src/types.d.ts'\n * @param {ParquetType} type\n * @param {number | undefined} typeLength\n * @returns {number}\n */\nfunction byteWidth(type, typeLength) {\n  switch (type) {\n  case 'INT32':\n  case 'FLOAT':\n    return 4\n  case 'INT64':\n  case 'DOUBLE':\n    return 8\n  case 'FIXED_LEN_BYTE_ARRAY':\n    if (!typeLength) throw new Error('parquet byteWidth missing type_length')\n    return typeLength\n  default:\n    throw new Error(`parquet unsupported type: ${type}`)\n  }\n}\n", "/**\n * Read `count` values of the given type from the reader.view.\n *\n * @param {DataReader} reader - buffer to read data from\n * @param {ParquetType} type - parquet type of the data\n * @param {number} count - number of values to read\n * @param {number | undefined} fixedLength - length of each fixed length byte array\n * @returns {DecodedArray} array of values\n */\nexport function readPlain(reader, type, count, fixedLength) {\n  if (count === 0) return []\n  if (type === 'BOOLEAN') {\n    return readPlainBoolean(reader, count)\n  } else if (type === 'INT32') {\n    return readPlainInt32(reader, count)\n  } else if (type === 'INT64') {\n    return readPlainInt64(reader, count)\n  } else if (type === 'INT96') {\n    return readPlainInt96(reader, count)\n  } else if (type === 'FLOAT') {\n    return readPlainFloat(reader, count)\n  } else if (type === 'DOUBLE') {\n    return readPlainDouble(reader, count)\n  } else if (type === 'BYTE_ARRAY') {\n    return readPlainByteArray(reader, count)\n  } else if (type === 'FIXED_LEN_BYTE_ARRAY') {\n    if (!fixedLength) throw new Error('parquet missing fixed length')\n    return readPlainByteArrayFixed(reader, count, fixedLength)\n  } else {\n    throw new Error(`parquet unhandled type: ${type}`)\n  }\n}\n\n/**\n * Read `count` boolean values.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @returns {boolean[]}\n */\nfunction readPlainBoolean(reader, count) {\n  const values = new Array(count)\n  for (let i = 0; i < count; i++) {\n    const byteOffset = reader.offset + (i / 8 | 0)\n    const bitOffset = i % 8\n    const byte = reader.view.getUint8(byteOffset)\n    values[i] = (byte & 1 << bitOffset) !== 0\n  }\n  reader.offset += Math.ceil(count / 8)\n  return values\n}\n\n/**\n * Read `count` int32 values.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @returns {Int32Array}\n */\nfunction readPlainInt32(reader, count) {\n  const values = (reader.view.byteOffset + reader.offset) % 4\n    ? new Int32Array(align(reader.view.buffer, reader.view.byteOffset + reader.offset, count * 4))\n    : new Int32Array(reader.view.buffer, reader.view.byteOffset + reader.offset, count)\n  reader.offset += count * 4\n  return values\n}\n\n/**\n * Read `count` int64 values.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @returns {BigInt64Array}\n */\nfunction readPlainInt64(reader, count) {\n  const values = (reader.view.byteOffset + reader.offset) % 8\n    ? new BigInt64Array(align(reader.view.buffer, reader.view.byteOffset + reader.offset, count * 8))\n    : new BigInt64Array(reader.view.buffer, reader.view.byteOffset + reader.offset, count)\n  reader.offset += count * 8\n  return values\n}\n\n/**\n * Read `count` int96 values.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @returns {bigint[]}\n */\nfunction readPlainInt96(reader, count) {\n  const values = new Array(count)\n  for (let i = 0; i < count; i++) {\n    const low = reader.view.getBigInt64(reader.offset + i * 12, true)\n    const high = reader.view.getInt32(reader.offset + i * 12 + 8, true)\n    values[i] = BigInt(high) << 64n | low\n  }\n  reader.offset += count * 12\n  return values\n}\n\n/**\n * Read `count` float values.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @returns {Float32Array}\n */\nfunction readPlainFloat(reader, count) {\n  const values = (reader.view.byteOffset + reader.offset) % 4\n    ? new Float32Array(align(reader.view.buffer, reader.view.byteOffset + reader.offset, count * 4))\n    : new Float32Array(reader.view.buffer, reader.view.byteOffset + reader.offset, count)\n  reader.offset += count * 4\n  return values\n}\n\n/**\n * Read `count` double values.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @returns {Float64Array}\n */\nfunction readPlainDouble(reader, count) {\n  const values = (reader.view.byteOffset + reader.offset) % 8\n    ? new Float64Array(align(reader.view.buffer, reader.view.byteOffset + reader.offset, count * 8))\n    : new Float64Array(reader.view.buffer, reader.view.byteOffset + reader.offset, count)\n  reader.offset += count * 8\n  return values\n}\n\n/**\n * Read `count` byte array values.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @returns {Uint8Array[]}\n */\nfunction readPlainByteArray(reader, count) {\n  const values = new Array(count)\n  for (let i = 0; i < count; i++) {\n    const length = reader.view.getUint32(reader.offset, true)\n    reader.offset += 4\n    values[i] = new Uint8Array(reader.view.buffer, reader.view.byteOffset + reader.offset, length)\n    reader.offset += length\n  }\n  return values\n}\n\n/**\n * Read a fixed length byte array.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @param {number} fixedLength\n * @returns {Uint8Array[]}\n */\nfunction readPlainByteArrayFixed(reader, count, fixedLength) {\n  // assert(reader.view.byteLength - reader.offset >= count * fixedLength)\n  const values = new Array(count)\n  for (let i = 0; i < count; i++) {\n    values[i] = new Uint8Array(reader.view.buffer, reader.view.byteOffset + reader.offset, fixedLength)\n    reader.offset += fixedLength\n  }\n  return values\n}\n\n/**\n * Create a new buffer with the offset and size.\n *\n * @import {DataReader, DecodedArray, ParquetType} from '../src/types.d.ts'\n * @param {ArrayBufferLike} buffer\n * @param {number} offset\n * @param {number} size\n * @returns {ArrayBuffer}\n */\nfunction align(buffer, offset, size) {\n  const aligned = new ArrayBuffer(size)\n  new Uint8Array(aligned).set(new Uint8Array(buffer, offset, size))\n  return aligned\n}\n", "/**\n * The MIT License (MIT)\n * Copyright (c) 2016 Zhipeng Jia\n * https://github.com/zhipeng-jia/snappyjs\n */\n\nconst WORD_MASK = [0, 0xff, 0xffff, 0xffffff, 0xffffffff]\n\n/**\n * Copy bytes from one array to another\n *\n * @param {Uint8Array} fromArray source array\n * @param {number} fromPos source position\n * @param {Uint8Array} toArray destination array\n * @param {number} toPos destination position\n * @param {number} length number of bytes to copy\n */\nfunction copyBytes(fromArray, fromPos, toArray, toPos, length) {\n  for (let i = 0; i < length; i++) {\n    toArray[toPos + i] = fromArray[fromPos + i]\n  }\n}\n\n/**\n * Decompress snappy data.\n * Accepts an output buffer to avoid allocating a new buffer for each call.\n *\n * @param {Uint8Array} input compressed data\n * @param {Uint8Array} output output buffer\n */\nexport function snappyUncompress(input, output) {\n  const inputLength = input.byteLength\n  const outputLength = output.byteLength\n  let pos = 0\n  let outPos = 0\n\n  // skip preamble (contains uncompressed length as varint)\n  while (pos < inputLength) {\n    const c = input[pos]\n    pos++\n    if (c < 128) {\n      break\n    }\n  }\n  if (outputLength && pos >= inputLength) {\n    throw new Error('invalid snappy length header')\n  }\n\n  while (pos < inputLength) {\n    const c = input[pos]\n    let len = 0\n    pos++\n\n    if (pos >= inputLength) {\n      throw new Error('missing eof marker')\n    }\n\n    // There are two types of elements, literals and copies (back references)\n    if ((c & 0x3) === 0) {\n      // Literals are uncompressed data stored directly in the byte stream\n      let len = (c >>> 2) + 1\n      // Longer literal length is encoded in multiple bytes\n      if (len > 60) {\n        if (pos + 3 >= inputLength) {\n          throw new Error('snappy error literal pos + 3 >= inputLength')\n        }\n        const lengthSize = len - 60 // length bytes - 1\n        len = input[pos]\n          + (input[pos + 1] << 8)\n          + (input[pos + 2] << 16)\n          + (input[pos + 3] << 24)\n        len = (len & WORD_MASK[lengthSize]) + 1\n        pos += lengthSize\n      }\n      if (pos + len > inputLength) {\n        throw new Error('snappy error literal exceeds input length')\n      }\n      copyBytes(input, pos, output, outPos, len)\n      pos += len\n      outPos += len\n    } else {\n      // Copy elements\n      let offset = 0 // offset back from current position to read\n      switch (c & 0x3) {\n      case 1:\n        // Copy with 1-byte offset\n        len = (c >>> 2 & 0x7) + 4\n        offset = input[pos] + (c >>> 5 << 8)\n        pos++\n        break\n      case 2:\n        // Copy with 2-byte offset\n        if (inputLength <= pos + 1) {\n          throw new Error('snappy error end of input')\n        }\n        len = (c >>> 2) + 1\n        offset = input[pos] + (input[pos + 1] << 8)\n        pos += 2\n        break\n      case 3:\n        // Copy with 4-byte offset\n        if (inputLength <= pos + 3) {\n          throw new Error('snappy error end of input')\n        }\n        len = (c >>> 2) + 1\n        offset = input[pos]\n          + (input[pos + 1] << 8)\n          + (input[pos + 2] << 16)\n          + (input[pos + 3] << 24)\n        pos += 4\n        break\n      default:\n        break\n      }\n      if (offset === 0 || isNaN(offset)) {\n        throw new Error(`invalid offset ${offset} pos ${pos} inputLength ${inputLength}`)\n      }\n      if (offset > outPos) {\n        throw new Error('cannot copy from before start of buffer')\n      }\n      copyBytes(output, outPos - offset, output, outPos, len)\n      outPos += len\n    }\n  }\n\n  if (outPos !== outputLength) throw new Error('premature end of input')\n}\n", "import { deltaBinaryUnpack, deltaByteArray, deltaLengthByteArray } from './delta.js'\nimport { bitWidth, byteStreamSplit, readRleBitPackedHybrid } from './encoding.js'\nimport { readPlain } from './plain.js'\nimport { getMaxDefinitionLevel, getMaxRepetitionLevel } from './schema.js'\nimport { snappyUncompress } from './snappy.js'\n\n/**\n * Read a data page from uncompressed reader.\n *\n * @param {Uint8Array} bytes raw page data (should already be decompressed)\n * @param {DataPageHeader} daph data page header\n * @param {ColumnDecoder} columnDecoder\n * @returns {DataPage} definition levels, repetition levels, and array of values\n */\nexport function readDataPage(bytes, daph, { type, element, schemaPath }) {\n  const view = new DataView(bytes.buffer, bytes.byteOffset, bytes.byteLength)\n  const reader = { view, offset: 0 }\n  /** @type {DecodedArray} */\n  let dataPage\n\n  // repetition and definition levels\n  const repetitionLevels = readRepetitionLevels(reader, daph, schemaPath)\n  // assert(!repetitionLevels.length || repetitionLevels.length === daph.num_values)\n  const { definitionLevels, numNulls } = readDefinitionLevels(reader, daph, schemaPath)\n  // assert(!definitionLevels.length || definitionLevels.length === daph.num_values)\n\n  // read values based on encoding\n  const nValues = daph.num_values - numNulls\n  if (daph.encoding === 'PLAIN') {\n    dataPage = readPlain(reader, type, nValues, element.type_length)\n  } else if (\n    daph.encoding === 'PLAIN_DICTIONARY' ||\n    daph.encoding === 'RLE_DICTIONARY' ||\n    daph.encoding === 'RLE'\n  ) {\n    const bitWidth = type === 'BOOLEAN' ? 1 : view.getUint8(reader.offset++)\n    if (bitWidth) {\n      dataPage = new Array(nValues)\n      if (type === 'BOOLEAN') {\n        readRleBitPackedHybrid(reader, bitWidth, dataPage)\n        dataPage = dataPage.map(x => !!x) // convert to boolean\n      } else {\n        // assert(daph.encoding.endsWith('_DICTIONARY'))\n        readRleBitPackedHybrid(reader, bitWidth, dataPage, view.byteLength - reader.offset)\n      }\n    } else {\n      dataPage = new Uint8Array(nValues) // nValue zeroes\n    }\n  } else if (daph.encoding === 'BYTE_STREAM_SPLIT') {\n    dataPage = byteStreamSplit(reader, nValues, type, element.type_length)\n  } else if (daph.encoding === 'DELTA_BINARY_PACKED') {\n    const int32 = type === 'INT32'\n    dataPage = int32 ? new Int32Array(nValues) : new BigInt64Array(nValues)\n    deltaBinaryUnpack(reader, nValues, dataPage)\n  } else if (daph.encoding === 'DELTA_LENGTH_BYTE_ARRAY') {\n    dataPage = new Array(nValues)\n    deltaLengthByteArray(reader, nValues, dataPage)\n  } else {\n    throw new Error(`parquet unsupported encoding: ${daph.encoding}`)\n  }\n\n  return { definitionLevels, repetitionLevels, dataPage }\n}\n\n/**\n * @import {ColumnDecoder, CompressionCodec, Compressors, DataPage, DataPageHeader, DataPageHeaderV2, DataReader, DecodedArray, PageHeader, SchemaTree} from '../src/types.d.ts'\n * @param {DataReader} reader data view for the page\n * @param {DataPageHeader} daph data page header\n * @param {SchemaTree[]} schemaPath\n * @returns {any[]} repetition levels and number of bytes read\n */\nfunction readRepetitionLevels(reader, daph, schemaPath) {\n  if (schemaPath.length > 1) {\n    const maxRepetitionLevel = getMaxRepetitionLevel(schemaPath)\n    if (maxRepetitionLevel) {\n      const values = new Array(daph.num_values)\n      readRleBitPackedHybrid(reader, bitWidth(maxRepetitionLevel), values)\n      return values\n    }\n  }\n  return []\n}\n\n/**\n * @param {DataReader} reader data view for the page\n * @param {DataPageHeader} daph data page header\n * @param {SchemaTree[]} schemaPath\n * @returns {{ definitionLevels: number[], numNulls: number }} definition levels\n */\nfunction readDefinitionLevels(reader, daph, schemaPath) {\n  const maxDefinitionLevel = getMaxDefinitionLevel(schemaPath)\n  if (!maxDefinitionLevel) return { definitionLevels: [], numNulls: 0 }\n\n  const definitionLevels = new Array(daph.num_values)\n  readRleBitPackedHybrid(reader, bitWidth(maxDefinitionLevel), definitionLevels)\n\n  // count nulls\n  let numNulls = daph.num_values\n  for (const def of definitionLevels) {\n    if (def === maxDefinitionLevel) numNulls--\n  }\n  if (numNulls === 0) definitionLevels.length = 0\n\n  return { definitionLevels, numNulls }\n}\n\n/**\n * @param {Uint8Array} compressedBytes\n * @param {number} uncompressed_page_size\n * @param {CompressionCodec} codec\n * @param {Compressors | undefined} compressors\n * @returns {Uint8Array}\n */\nexport function decompressPage(compressedBytes, uncompressed_page_size, codec, compressors) {\n  /** @type {Uint8Array} */\n  let page\n  const customDecompressor = compressors?.[codec]\n  if (codec === 'UNCOMPRESSED') {\n    page = compressedBytes\n  } else if (customDecompressor) {\n    page = customDecompressor(compressedBytes, uncompressed_page_size)\n  } else if (codec === 'SNAPPY') {\n    page = new Uint8Array(uncompressed_page_size)\n    snappyUncompress(compressedBytes, page)\n  } else {\n    throw new Error(`parquet unsupported compression codec: ${codec}`)\n  }\n  if (page?.length !== uncompressed_page_size) {\n    throw new Error(`parquet decompressed page length ${page?.length} does not match header ${uncompressed_page_size}`)\n  }\n  return page\n}\n\n\n/**\n * Read a data page from the given Uint8Array.\n *\n * @param {Uint8Array} compressedBytes raw page data\n * @param {PageHeader} ph page header\n * @param {ColumnDecoder} columnDecoder\n * @returns {DataPage} definition levels, repetition levels, and array of values\n */\nexport function readDataPageV2(compressedBytes, ph, columnDecoder) {\n  const view = new DataView(compressedBytes.buffer, compressedBytes.byteOffset, compressedBytes.byteLength)\n  const reader = { view, offset: 0 }\n  const { type, element, schemaPath, codec, compressors } = columnDecoder\n  const daph2 = ph.data_page_header_v2\n  if (!daph2) throw new Error('parquet data page header v2 is undefined')\n\n  // repetition levels\n  const repetitionLevels = readRepetitionLevelsV2(reader, daph2, schemaPath)\n  reader.offset = daph2.repetition_levels_byte_length // readVarInt() => len for boolean v2?\n\n  // definition levels\n  const definitionLevels = readDefinitionLevelsV2(reader, daph2, schemaPath)\n  // assert(reader.offset === daph2.repetition_levels_byte_length + daph2.definition_levels_byte_length)\n\n  const uncompressedPageSize = ph.uncompressed_page_size - daph2.definition_levels_byte_length - daph2.repetition_levels_byte_length\n\n  let page = compressedBytes.subarray(reader.offset)\n  if (daph2.is_compressed !== false) {\n    page = decompressPage(page, uncompressedPageSize, codec, compressors)\n  }\n  const pageView = new DataView(page.buffer, page.byteOffset, page.byteLength)\n  const pageReader = { view: pageView, offset: 0 }\n\n  // read values based on encoding\n  /** @type {DecodedArray} */\n  let dataPage\n  const nValues = daph2.num_values - daph2.num_nulls\n  if (daph2.encoding === 'PLAIN') {\n    dataPage = readPlain(pageReader, type, nValues, element.type_length)\n  } else if (daph2.encoding === 'RLE') {\n    // assert(type === 'BOOLEAN')\n    dataPage = new Array(nValues)\n    readRleBitPackedHybrid(pageReader, 1, dataPage)\n    dataPage = dataPage.map(x => !!x)\n  } else if (\n    daph2.encoding === 'PLAIN_DICTIONARY' ||\n    daph2.encoding === 'RLE_DICTIONARY'\n  ) {\n    const bitWidth = pageView.getUint8(pageReader.offset++)\n    dataPage = new Array(nValues)\n    readRleBitPackedHybrid(pageReader, bitWidth, dataPage, uncompressedPageSize - 1)\n  } else if (daph2.encoding === 'DELTA_BINARY_PACKED') {\n    const int32 = type === 'INT32'\n    dataPage = int32 ? new Int32Array(nValues) : new BigInt64Array(nValues)\n    deltaBinaryUnpack(pageReader, nValues, dataPage)\n  } else if (daph2.encoding === 'DELTA_LENGTH_BYTE_ARRAY') {\n    dataPage = new Array(nValues)\n    deltaLengthByteArray(pageReader, nValues, dataPage)\n  } else if (daph2.encoding === 'DELTA_BYTE_ARRAY') {\n    dataPage = new Array(nValues)\n    deltaByteArray(pageReader, nValues, dataPage)\n  } else if (daph2.encoding === 'BYTE_STREAM_SPLIT') {\n    dataPage = byteStreamSplit(pageReader, nValues, type, element.type_length)\n  } else {\n    throw new Error(`parquet unsupported encoding: ${daph2.encoding}`)\n  }\n\n  return { definitionLevels, repetitionLevels, dataPage }\n}\n\n/**\n * @param {DataReader} reader\n * @param {DataPageHeaderV2} daph2 data page header v2\n * @param {SchemaTree[]} schemaPath\n * @returns {any[]} repetition levels\n */\nfunction readRepetitionLevelsV2(reader, daph2, schemaPath) {\n  const maxRepetitionLevel = getMaxRepetitionLevel(schemaPath)\n  if (!maxRepetitionLevel) return []\n\n  const values = new Array(daph2.num_values)\n  readRleBitPackedHybrid(reader, bitWidth(maxRepetitionLevel), values, daph2.repetition_levels_byte_length)\n  return values\n}\n\n/**\n * @param {DataReader} reader\n * @param {DataPageHeaderV2} daph2 data page header v2\n * @param {SchemaTree[]} schemaPath\n * @returns {number[] | undefined} definition levels\n */\nfunction readDefinitionLevelsV2(reader, daph2, schemaPath) {\n  const maxDefinitionLevel = getMaxDefinitionLevel(schemaPath)\n  if (maxDefinitionLevel) {\n    // V2 we know the length\n    const values = new Array(daph2.num_values)\n    readRleBitPackedHybrid(reader, bitWidth(maxDefinitionLevel), values, daph2.definition_levels_byte_length)\n    return values\n  }\n}\n", "import { assembleLists } from './assemble.js'\nimport { Encodings, PageTypes } from './constants.js'\nimport { convert, convertWithDictionary } from './convert.js'\nimport { decompressPage, readDataPage, readDataPageV2 } from './datapage.js'\nimport { readPlain } from './plain.js'\nimport { isFlatColumn } from './schema.js'\nimport { deserializeTCompactProtocol } from './thrift.js'\n\n/**\n * Parse column data from a buffer.\n *\n * @param {DataReader} reader\n * @param {RowGroupSelect} rowGroupSelect row group selection\n * @param {ColumnDecoder} columnDecoder column decoder params\n * @param {(chunk: SubColumnData) => void} [onPage] callback for each page\n * @returns {DecodedArray[]}\n */\nexport function readColumn(reader, { groupStart, selectStart, selectEnd }, columnDecoder, onPage) {\n  const { pathInSchema, schemaPath } = columnDecoder\n  const isFlat = isFlatColumn(schemaPath)\n  /** @type {DecodedArray[]} */\n  const chunks = []\n  /** @type {DecodedArray | undefined} */\n  let dictionary = undefined\n  /** @type {DecodedArray | undefined} */\n  let lastChunk = undefined\n  let rowCount = 0\n\n  const emitLastChunk = onPage && (() => {\n    lastChunk && onPage({\n      pathInSchema,\n      columnData: lastChunk,\n      rowStart: groupStart + rowCount - lastChunk.length,\n      rowEnd: groupStart + rowCount,\n    })\n  })\n\n  while (isFlat ? rowCount < selectEnd : reader.offset < reader.view.byteLength - 1) {\n    if (reader.offset >= reader.view.byteLength - 1) break // end of reader\n\n    // read page header\n    const header = parquetHeader(reader)\n    if (header.type === 'DICTIONARY_PAGE') {\n      // assert(!dictionary)\n      dictionary = readPage(reader, header, columnDecoder, dictionary, undefined, 0)\n      dictionary = convert(dictionary, columnDecoder)\n    } else {\n      const lastChunkLength = lastChunk?.length || 0\n      const values = readPage(reader, header, columnDecoder, dictionary, lastChunk, selectStart - rowCount)\n      if (lastChunk === values) {\n        // continued from previous page\n        rowCount += values.length - lastChunkLength\n      } else {\n        // TODO: don't emit empty chunks\n        emitLastChunk?.()\n        chunks.push(values)\n        rowCount += values.length\n        lastChunk = values\n      }\n    }\n  }\n  emitLastChunk?.()\n\n  return chunks\n}\n\n/**\n * Read a page (data or dictionary) from a buffer.\n *\n * @param {DataReader} reader\n * @param {PageHeader} header\n * @param {ColumnDecoder} columnDecoder\n * @param {DecodedArray | undefined} dictionary\n * @param {DecodedArray | undefined} previousChunk\n * @param {number} pageStart skip this many rows in the page\n * @returns {DecodedArray}\n */\nexport function readPage(reader, header, columnDecoder, dictionary, previousChunk, pageStart) {\n  const { type, element, schemaPath, codec, compressors } = columnDecoder\n  // read compressed_page_size bytes\n  const compressedBytes = new Uint8Array(\n    reader.view.buffer, reader.view.byteOffset + reader.offset, header.compressed_page_size\n  )\n  reader.offset += header.compressed_page_size\n\n  // parse page data by type\n  if (header.type === 'DATA_PAGE') {\n    const daph = header.data_page_header\n    if (!daph) throw new Error('parquet data page header is undefined')\n\n    // skip unnecessary non-nested pages\n    if (pageStart > daph.num_values && isFlatColumn(schemaPath)) {\n      return new Array(daph.num_values) // TODO: don't allocate array\n    }\n\n    const page = decompressPage(compressedBytes, Number(header.uncompressed_page_size), codec, compressors)\n    const { definitionLevels, repetitionLevels, dataPage } = readDataPage(page, daph, columnDecoder)\n    // assert(!daph.statistics?.null_count || daph.statistics.null_count === BigInt(daph.num_values - dataPage.length))\n\n    // convert types, dereference dictionary, and assemble lists\n    let values = convertWithDictionary(dataPage, dictionary, daph.encoding, columnDecoder)\n    if (repetitionLevels.length || definitionLevels?.length) {\n      const output = Array.isArray(previousChunk) ? previousChunk : []\n      return assembleLists(output, definitionLevels, repetitionLevels, values, schemaPath)\n    } else {\n      // wrap nested flat data by depth\n      for (let i = 2; i < schemaPath.length; i++) {\n        if (schemaPath[i].element.repetition_type !== 'REQUIRED') {\n          values = Array.from(values, e => [e])\n        }\n      }\n      return values\n    }\n  } else if (header.type === 'DATA_PAGE_V2') {\n    const daph2 = header.data_page_header_v2\n    if (!daph2) throw new Error('parquet data page header v2 is undefined')\n\n    // skip unnecessary pages\n    if (pageStart > daph2.num_rows) {\n      return new Array(daph2.num_values) // TODO: don't allocate array\n    }\n\n    const { definitionLevels, repetitionLevels, dataPage } =\n      readDataPageV2(compressedBytes, header, columnDecoder)\n\n    // convert types, dereference dictionary, and assemble lists\n    const values = convertWithDictionary(dataPage, dictionary, daph2.encoding, columnDecoder)\n    const output = Array.isArray(previousChunk) ? previousChunk : []\n    return assembleLists(output, definitionLevels, repetitionLevels, values, schemaPath)\n  } else if (header.type === 'DICTIONARY_PAGE') {\n    const diph = header.dictionary_page_header\n    if (!diph) throw new Error('parquet dictionary page header is undefined')\n\n    const page = decompressPage(\n      compressedBytes, Number(header.uncompressed_page_size), codec, compressors\n    )\n\n    const reader = { view: new DataView(page.buffer, page.byteOffset, page.byteLength), offset: 0 }\n    return readPlain(reader, type, diph.num_values, element.type_length)\n  } else {\n    throw new Error(`parquet unsupported page type: ${header.type}`)\n  }\n}\n\n/**\n * Read parquet header from a buffer.\n *\n * @import {ColumnDecoder, DataReader, DecodedArray, PageHeader, RowGroupSelect, SubColumnData} from '../src/types.d.ts'\n * @param {DataReader} reader\n * @returns {PageHeader}\n */\nfunction parquetHeader(reader) {\n  const header = deserializeTCompactProtocol(reader)\n\n  // Parse parquet header from thrift data\n  const type = PageTypes[header.field_1]\n  const uncompressed_page_size = header.field_2\n  const compressed_page_size = header.field_3\n  const crc = header.field_4\n  const data_page_header = header.field_5 && {\n    num_values: header.field_5.field_1,\n    encoding: Encodings[header.field_5.field_2],\n    definition_level_encoding: Encodings[header.field_5.field_3],\n    repetition_level_encoding: Encodings[header.field_5.field_4],\n    statistics: header.field_5.field_5 && {\n      max: header.field_5.field_5.field_1,\n      min: header.field_5.field_5.field_2,\n      null_count: header.field_5.field_5.field_3,\n      distinct_count: header.field_5.field_5.field_4,\n      max_value: header.field_5.field_5.field_5,\n      min_value: header.field_5.field_5.field_6,\n    },\n  }\n  const index_page_header = header.field_6\n  const dictionary_page_header = header.field_7 && {\n    num_values: header.field_7.field_1,\n    encoding: Encodings[header.field_7.field_2],\n    is_sorted: header.field_7.field_3,\n  }\n  const data_page_header_v2 = header.field_8 && {\n    num_values: header.field_8.field_1,\n    num_nulls: header.field_8.field_2,\n    num_rows: header.field_8.field_3,\n    encoding: Encodings[header.field_8.field_4],\n    definition_levels_byte_length: header.field_8.field_5,\n    repetition_levels_byte_length: header.field_8.field_6,\n    is_compressed: header.field_8.field_7 === undefined ? true : header.field_8.field_7, // default true\n    statistics: header.field_8.field_8,\n  }\n\n  return {\n    type,\n    uncompressed_page_size,\n    compressed_page_size,\n    crc,\n    data_page_header,\n    index_page_header,\n    dictionary_page_header,\n    data_page_header_v2,\n  }\n}\n", "import { assembleNested } from './assemble.js'\nimport { readColumn } from './column.js'\nimport { DEFAULT_PARSERS } from './convert.js'\nimport { readOffsetIndex } from './indexes.js'\nimport { getSchemaPath } from './schema.js'\nimport { flatten } from './utils.js'\n\n/**\n * @import {AsyncColumn, AsyncRowGroup, DecodedArray, GroupPlan, ParquetParsers, ParquetReadOptions, QueryPlan, SchemaTree} from './types.js'\n */\n/**\n * Read a row group from a file-like object.\n *\n * @param {ParquetReadOptions} options\n * @param {QueryPlan} plan\n * @param {GroupPlan} groupPlan\n * @returns {AsyncRowGroup} resolves to column data\n */\nexport function readRowGroup(options, { metadata }, groupPlan) {\n  const { file, compressors, utf8 } = options\n\n  /** @type {AsyncColumn[]} */\n  const asyncColumns = []\n  /** @type {ParquetParsers} */\n  const parsers = { ...DEFAULT_PARSERS, ...options.parsers }\n\n  // read column data\n  for (const chunkPlan of groupPlan.chunks) {\n    const { columnMetadata } = chunkPlan\n    const schemaPath = getSchemaPath(metadata.schema, columnMetadata.path_in_schema)\n    const columnDecoder = {\n      pathInSchema: columnMetadata.path_in_schema,\n      type: columnMetadata.type,\n      element: schemaPath[schemaPath.length - 1].element,\n      schemaPath,\n      codec: columnMetadata.codec,\n      parsers,\n      compressors,\n      utf8,\n    }\n\n    // non-offset-index case\n    if (!('offsetIndex' in chunkPlan)) {\n      asyncColumns.push({\n        pathInSchema: columnMetadata.path_in_schema,\n        data: Promise.resolve(file.slice(chunkPlan.range.startByte, chunkPlan.range.endByte))\n          .then(buffer => {\n            const reader = { view: new DataView(buffer), offset: 0 }\n            return {\n              pageSkip: 0,\n              data: readColumn(reader, groupPlan, columnDecoder, options.onPage),\n            }\n          }),\n      })\n      continue\n    }\n\n    // offset-index case\n    asyncColumns.push({\n      pathInSchema: columnMetadata.path_in_schema,\n      // fetch offset index\n      data: Promise.resolve(file.slice(chunkPlan.offsetIndex.startByte, chunkPlan.offsetIndex.endByte))\n        .then(async arrayBuffer => {\n          const offsetIndex = readOffsetIndex({ view: new DataView(arrayBuffer), offset: 0 })\n          // use offset index to read only necessary pages\n          const { selectStart, selectEnd } = groupPlan\n          const pages = offsetIndex.page_locations\n          let startByte = NaN\n          let endByte = NaN\n          let pageSkip = 0\n          for (let i = 0; i < pages.length; i++) {\n            const page = pages[i]\n            const pageStart = Number(page.first_row_index)\n            const pageEnd = i + 1 < pages.length\n              ? Number(pages[i + 1].first_row_index)\n              : groupPlan.groupRows // last page extends to end of row group\n            // check if page overlaps with [selectStart, selectEnd)\n            if (pageStart < selectEnd && pageEnd > selectStart) {\n              if (Number.isNaN(startByte)) {\n                startByte = Number(page.offset)\n                pageSkip = pageStart\n              }\n              endByte = Number(page.offset) + page.compressed_page_size\n            }\n          }\n          const buffer = await file.slice(startByte, endByte)\n          const reader = { view: new DataView(buffer), offset: 0 }\n          // adjust row selection for skipped pages\n          const adjustedGroupPlan = pageSkip ? {\n            ...groupPlan,\n            groupStart: groupPlan.groupStart + pageSkip,\n            selectStart: groupPlan.selectStart - pageSkip,\n            selectEnd: groupPlan.selectEnd - pageSkip,\n          } : groupPlan\n          return {\n            data: readColumn(reader, adjustedGroupPlan, columnDecoder, options.onPage),\n            pageSkip,\n          }\n        }),\n    })\n  }\n\n  return { groupStart: groupPlan.groupStart, groupRows: groupPlan.groupRows, asyncColumns }\n}\n\n/**\n * @overload\n * @param {AsyncRowGroup} asyncGroup\n * @param {number} selectStart\n * @param {number} selectEnd\n * @param {string[] | undefined} columns\n * @param {'object'} rowFormat\n * @returns {Promise<Record<string, any>[]>} resolves to row data\n */\n/**\n * @overload\n * @param {AsyncRowGroup} asyncGroup\n * @param {number} selectStart\n * @param {number} selectEnd\n * @param {string[] | undefined} columns\n * @param {'array'} [rowFormat]\n * @returns {Promise<any[][]>} resolves to row data\n */\n/**\n * @param {AsyncRowGroup} asyncGroup\n * @param {number} selectStart\n * @param {number} selectEnd\n * @param {string[] | undefined} columns\n * @param {'object' | 'array'} [rowFormat]\n * @returns {Promise<Record<string, any>[] | any[][]>} resolves to row data\n */\nexport async function asyncGroupToRows({ asyncColumns }, selectStart, selectEnd, columns, rowFormat) {\n  // TODO: do it without flatten\n  const asyncPages = await Promise.all(asyncColumns.map(async ({ data }) => {\n    const pages = await data\n    return {\n      ...pages,\n      data: flatten(pages.data),\n    }\n  }))\n\n  // careful mapping of column order for rowFormat: array\n  const includedColumnNames = asyncColumns\n    .map(child => child.pathInSchema[0])\n    .filter(name => !columns || columns.includes(name))\n  const columnOrder = columns ?? includedColumnNames\n  const columnIndexes = columnOrder.map(name => asyncColumns.findIndex(column => column.pathInSchema[0] === name))\n\n  // transpose columns into rows\n  const selectCount = selectEnd - selectStart\n  if (rowFormat === 'object') {\n    /** @type {Record<string, any>[]} */\n    const groupData = Array(selectCount)\n    for (let selectRow = 0; selectRow < selectCount; selectRow++) {\n      const row = selectStart + selectRow\n      // return each row as an object\n      /** @type {Record<string, any>} */\n      const rowData = {}\n      for (let i = 0; i < asyncColumns.length; i++) {\n        const { data, pageSkip } = asyncPages[i]\n        rowData[asyncColumns[i].pathInSchema[0]] = data[row - pageSkip]\n      }\n      groupData[selectRow] = rowData\n    }\n    return groupData\n  }\n\n  /** @type {any[][]} */\n  const groupData = Array(selectCount)\n  for (let selectRow = 0; selectRow < selectCount; selectRow++) {\n    const row = selectStart + selectRow\n    // return each row as an array\n    const rowData = Array(asyncColumns.length)\n    for (let i = 0; i < columnOrder.length; i++) {\n      const colIdx = columnIndexes[i]\n      if (colIdx >= 0) {\n        const { data, pageSkip } = asyncPages[colIdx]\n        rowData[i] = data[row - pageSkip]\n      }\n    }\n    groupData[selectRow] = rowData\n  }\n  return groupData\n}\n\n/**\n * Assemble physical columns into top-level columns asynchronously.\n *\n * @param {AsyncRowGroup} asyncRowGroup\n * @param {SchemaTree} schemaTree\n * @returns {AsyncRowGroup}\n */\nexport function assembleAsync(asyncRowGroup, schemaTree) {\n  const { asyncColumns } = asyncRowGroup\n  /** @type {AsyncColumn[]} */\n  const assembled = []\n  for (const child of schemaTree.children) {\n    if (child.children.length) {\n      const childColumns = asyncColumns.filter(column => column.pathInSchema[0] === child.element.name)\n      if (!childColumns.length) continue\n\n      // wait for all child columns to be read\n      /** @type {Map<string, DecodedArray>} */\n      const flatData = new Map()\n      const data = Promise.all(childColumns.map(column => {\n        return column.data.then(({ data }) => {\n          flatData.set(column.pathInSchema.join('.'), flatten(data))\n        })\n      })).then(() => {\n        // assemble the column\n        assembleNested(flatData, child)\n        const flatColumn = flatData.get(child.path.join('.'))\n        if (!flatColumn) throw new Error('parquet column data not assembled')\n        return { data: [flatColumn], pageSkip: 0 }\n      })\n\n      assembled.push({ pathInSchema: child.path, data })\n    } else {\n      // leaf node, return the column\n      const asyncColumn = asyncColumns.find(column => column.pathInSchema[0] === child.element.name)\n      if (asyncColumn) {\n        assembled.push(asyncColumn)\n      }\n    }\n  }\n  return { ...asyncRowGroup, asyncColumns: assembled }\n}\n", "import { columnsNeededForFilter, matchFilter } from './filter.js'\nimport { parquetMetadataAsync, parquetSchema } from './metadata.js'\nimport { parquetPlan, prefetchAsyncBuffer } from './plan.js'\nimport { assembleAsync, asyncGroupToRows, readRowGroup } from './rowgroup.js'\nimport { concat, flatten } from './utils.js'\n\n/**\n * @import {AsyncRowGroup, DecodedArray, ParquetReadOptions, BaseParquetReadOptions} from '../src/types.js'\n */\n/**\n * Read parquet data rows from a file-like object.\n * Reads the minimal number of row groups and columns to satisfy the request.\n *\n * Returns a void promise when complete.\n * Errors are thrown on the returned promise.\n * Data is returned in callbacks onComplete, onChunk, onPage, NOT the return promise.\n * See parquetReadObjects for a more convenient API.\n *\n * @param {ParquetReadOptions} options read options\n * @returns {Promise<void>} resolves when all requested rows and columns are parsed, all errors are thrown here\n */\nexport async function parquetRead(options) {\n  // load metadata if not provided\n  options.metadata ??= await parquetMetadataAsync(options.file, options)\n\n  const { rowStart = 0, rowEnd, columns, onChunk, onComplete, rowFormat, filter, filterStrict = true } = options\n\n  // Filter requires object format to match column names\n  if (filter && rowFormat !== 'object') {\n    throw new Error('parquet filter requires rowFormat: \"object\"')\n  }\n\n  // Include filter columns in the read plan\n  const filterColumns = columnsNeededForFilter(filter)\n  if (filterColumns.length) {\n    const schemaColumns = parquetSchema(options.metadata).children.map(c => c.element.name)\n    const missingColumns = filterColumns.filter(c => !schemaColumns.includes(c))\n    if (missingColumns.length) {\n      throw new Error(`parquet filter columns not found: ${missingColumns.join(', ')}`)\n    }\n  }\n  let readColumns = columns\n  let requiresProjection = false\n  if (columns && filter) {\n    const missingFilterColumns = filterColumns.filter(c => !columns.includes(c))\n    if (missingFilterColumns.length) {\n      readColumns = [...columns, ...missingFilterColumns]\n      requiresProjection = true\n    }\n  }\n\n  // read row groups with expanded columns\n  const readOptions = readColumns !== columns ? { ...options, columns: readColumns } : options\n  const asyncGroups = parquetReadAsync(readOptions)\n\n  // skip assembly if no onComplete or onChunk, but wait for reading to finish\n  if (!onComplete && !onChunk) {\n    for (const { asyncColumns } of asyncGroups) {\n      for (const { data } of asyncColumns) await data\n    }\n    return\n  }\n\n  // assemble struct columns\n  const schemaTree = parquetSchema(options.metadata)\n  const assembled = asyncGroups.map(arg => assembleAsync(arg, schemaTree))\n\n  // onChunk emit all chunks (don't await)\n  if (onChunk) {\n    for (const asyncGroup of assembled) {\n      for (const asyncColumn of asyncGroup.asyncColumns) {\n        asyncColumn.data.then(({ data, pageSkip }) => {\n          let rowStart = asyncGroup.groupStart + pageSkip\n          for (const columnData of data) {\n            onChunk({\n              columnName: asyncColumn.pathInSchema[0],\n              columnData,\n              rowStart,\n              rowEnd: rowStart + columnData.length,\n            })\n            rowStart += columnData.length\n          }\n        })\n      }\n    }\n  }\n\n  // onComplete transpose column chunks to rows\n  if (onComplete) {\n    // loosen the types to avoid duplicate code\n    /** @type {any[]} */\n    const rows = []\n    for (const asyncGroup of assembled) {\n      // filter to rows in range\n      const selectStart = Math.max(rowStart - asyncGroup.groupStart, 0)\n      const selectEnd = Math.min((rowEnd ?? Infinity) - asyncGroup.groupStart, asyncGroup.groupRows)\n      // transpose column chunks to rows in output\n      const groupData = rowFormat === 'object' ?\n        await asyncGroupToRows(asyncGroup, selectStart, selectEnd, readColumns, 'object') :\n        await asyncGroupToRows(asyncGroup, selectStart, selectEnd, columns, 'array')\n\n      // Apply filter and projection\n      if (filter) {\n        // eslint-disable-next-line no-extra-parens\n        for (const row of /** @type {Record<string, any>[]} */ (groupData)) {\n          if (matchFilter(row, filter, filterStrict)) {\n            if (requiresProjection && columns) {\n              for (const col of filterColumns) {\n                if (!columns.includes(col)) delete row[col]\n              }\n            }\n            rows.push(row)\n          }\n        }\n      } else {\n        concat(rows, groupData)\n      }\n    }\n    onComplete(rows)\n  } else {\n    // wait for all async groups to finish (complete takes care of this)\n    for (const { asyncColumns } of assembled) {\n      for (const { data } of asyncColumns) await data\n    }\n  }\n}\n\n/**\n * @param {ParquetReadOptions} options read options\n * @returns {AsyncRowGroup[]}\n */\nexport function parquetReadAsync(options) {\n  if (!options.metadata) throw new Error('parquet requires metadata')\n  // TODO: validate options (start, end, columns, etc)\n\n  // prefetch byte ranges\n  const plan = parquetPlan(options)\n  options.file = prefetchAsyncBuffer(options.file, plan)\n\n  // read row groups\n  return plan.groups.map(groupPlan => readRowGroup(options, plan, groupPlan))\n}\n\n/**\n * Reads a single column from a parquet file.\n *\n * @param {BaseParquetReadOptions} options\n * @returns {Promise<DecodedArray>}\n */\nexport async function parquetReadColumn(options) {\n  if (options.columns?.length !== 1) {\n    throw new Error('parquetReadColumn expected columns: [columnName]')\n  }\n  options.metadata ??= await parquetMetadataAsync(options.file, options)\n  const asyncGroups = parquetReadAsync(options)\n\n  // assemble struct columns\n  const schemaTree = parquetSchema(options.metadata)\n  const assembled = asyncGroups.map(arg => assembleAsync(arg, schemaTree))\n\n  /** @type {DecodedArray[]} */\n  const columnData = []\n  for (const rg of assembled) {\n    columnData.push(flatten((await rg.asyncColumns[0].data).data))\n  }\n  return flatten(columnData)\n}\n\n/**\n * This is a helper function to read parquet row data as a promise.\n * It is a wrapper around the more configurable parquetRead function.\n *\n * @param {Omit<ParquetReadOptions, 'onComplete'>} options\n * @returns {Promise<Record<string, any>[]>} resolves when all requested rows and columns are parsed\n */\nexport function parquetReadObjects(options) {\n  return new Promise((onComplete, reject) => {\n    parquetRead({\n      ...options,\n      rowFormat: 'object', // force object output\n      onComplete,\n    }).catch(reject)\n  })\n}\n", "import { parquetMetadataAsync, parquetSchema } from './metadata.js'\nimport { parquetReadColumn, parquetReadObjects } from './read.js'\n\n/**\n * @import {BaseParquetReadOptions} from '../src/types.js'\n */\n/**\n * Wraps parquetRead with orderBy support.\n * This is a parquet-aware query engine that can read a subset of rows and columns.\n * Accepts optional orderBy column name to sort the results.\n * Note that using orderBy may SIGNIFICANTLY increase the query time.\n *\n * @param {BaseParquetReadOptions & { orderBy?: string }} options\n * @returns {Promise<Record<string, any>[]>} resolves when all requested rows and columns are parsed\n */\nexport async function parquetQuery(options) {\n  if (!options.file || !(options.file.byteLength >= 0)) {\n    throw new Error('parquet expected AsyncBuffer')\n  }\n  options.metadata ??= await parquetMetadataAsync(options.file, options)\n\n  const { metadata, rowStart = 0, columns, orderBy, filter } = options\n  if (rowStart < 0) throw new Error('parquet rowStart must be positive')\n  const rowEnd = options.rowEnd ?? Number(metadata.num_rows)\n\n  // Validate orderBy column exists\n  if (orderBy) {\n    const allColumns = parquetSchema(options.metadata).children.map(c => c.element.name)\n    if (!allColumns.includes(orderBy)) {\n      throw new Error(`parquet orderBy column not found: ${orderBy}`)\n    }\n  }\n\n  if (filter && !orderBy && rowEnd < metadata.num_rows) {\n    // iterate through row groups and filter until we have enough rows\n    /** @type {Record<string, any>[]} */\n    const filteredRows = []\n    let groupStart = 0\n    for (const group of metadata.row_groups) {\n      const groupEnd = groupStart + Number(group.num_rows)\n      // TODO: if expected > group size, start fetching next groups\n      const groupData = await parquetReadObjects({\n        ...options, rowStart: groupStart, rowEnd: groupEnd,\n      })\n      filteredRows.push(...groupData)\n      if (filteredRows.length >= rowEnd) break\n      groupStart = groupEnd\n    }\n    return filteredRows.slice(rowStart, rowEnd)\n  } else if (filter && orderBy) {\n    // read all rows with orderBy column included for sorting\n    const readColumns = columns && !columns.includes(orderBy)\n      ? [...columns, orderBy]\n      : columns\n\n    const results = await parquetReadObjects({\n      ...options, rowStart: undefined, rowEnd: undefined, columns: readColumns,\n    })\n\n    // sort by orderBy column\n    results.sort((a, b) => compare(a[orderBy], b[orderBy]))\n\n    // project out orderBy column if not originally requested\n    if (readColumns !== columns) {\n      for (const row of results) {\n        delete row[orderBy]\n      }\n    }\n\n    return results.slice(rowStart, rowEnd)\n  } else if (filter) {\n    // filter without orderBy, read all matching rows\n    const results = await parquetReadObjects({\n      ...options, rowStart: undefined, rowEnd: undefined,\n    })\n    return results.slice(rowStart, rowEnd)\n  } else if (typeof orderBy === 'string') {\n    // sorted but unfiltered: fetch orderBy column first\n    const orderColumn = await parquetReadColumn({\n      ...options, rowStart: undefined, rowEnd: undefined, columns: [orderBy],\n    })\n\n    // compute row groups to fetch\n    const sortedIndices = Array.from(orderColumn, (_, index) => index)\n      .sort((a, b) => compare(orderColumn[a], orderColumn[b]))\n      .slice(rowStart, rowEnd)\n\n    const sparseData = await parquetReadRows({ ...options, rows: sortedIndices })\n    // warning: the type Record<string, any> & {__index__: number})[] is simplified into Record<string, any>[]\n    // when returning. The data contains the __index__ property, but it's not exposed as such.\n    const data = sortedIndices.map(index => sparseData[index])\n    return data\n  } else {\n    return await parquetReadObjects(options)\n  }\n}\n\n/**\n * Reads a list rows from a parquet file, reading only the row groups that contain the rows.\n * Returns a sparse array of rows.\n * @param {BaseParquetReadOptions & { rows: number[] }} options\n * @returns {Promise<(Record<string, any> & {__index__: number})[]>}\n */\nasync function parquetReadRows(options) {\n  const { file, rows } = options\n  options.metadata ??= await parquetMetadataAsync(file, options)\n  const { row_groups: rowGroups } = options.metadata\n  // Compute row groups to fetch\n  const groupIncluded = Array(rowGroups.length).fill(false)\n  let groupStart = 0\n  const groupEnds = rowGroups.map(group => groupStart += Number(group.num_rows))\n  for (const index of rows) {\n    const groupIndex = groupEnds.findIndex(end => index < end)\n    groupIncluded[groupIndex] = true\n  }\n\n  // Compute row ranges to fetch\n  const rowRanges = []\n  let rangeStart\n  groupStart = 0\n  for (let i = 0; i < groupIncluded.length; i++) {\n    const groupEnd = groupStart + Number(rowGroups[i].num_rows)\n    if (groupIncluded[i]) {\n      if (rangeStart === undefined) {\n        rangeStart = groupStart\n      }\n    } else {\n      if (rangeStart !== undefined) {\n        rowRanges.push([rangeStart, groupEnd])\n        rangeStart = undefined\n      }\n    }\n    groupStart = groupEnd\n  }\n  if (rangeStart !== undefined) {\n    rowRanges.push([rangeStart, groupStart])\n  }\n\n  // Fetch by row group and map to rows\n  /** @type {(Record<string, any> & {__index__: number})[]} */\n  const sparseData = Array(Number(options.metadata.num_rows))\n  for (const [rangeStart, rangeEnd] of rowRanges) {\n    // TODO: fetch in parallel\n    const groupData = await parquetReadObjects({ ...options, rowStart: rangeStart, rowEnd: rangeEnd })\n    for (let i = rangeStart; i < rangeEnd; i++) {\n      // warning: if the row contains a column named __index__, it will overwrite the index.\n      sparseData[i] = { __index__: i, ...groupData[i - rangeStart] }\n    }\n  }\n  return sparseData\n}\n\n/**\n * @param {any} a\n * @param {any} b\n * @returns {number}\n */\nfunction compare(a, b) {\n  if (a < b) return -1\n  if (a > b) return 1\n  return 0 // TODO: null handling\n}\n"],
  "mappings": ";;;AAEO,IAAM,eAAe;AAAA,EAC1B;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AACF;AAGO,IAAM,YAAY;AAAA,EACvB;AAAA,EACA;AAAA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AACF;AAGO,IAAM,uBAAuB;AAAA,EAClC;AAAA,EACA;AAAA,EACA;AACF;AAGO,IAAM,iBAAiB;AAAA,EAC5B;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AACF;AAGO,IAAM,oBAAoB;AAAA,EAC/B;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AACF;AAGO,IAAM,YAAY;AAAA,EACvB;AAAA,EACA;AAAA,EACA;AAAA,EACA;AACF;AAGO,IAAM,iBAAiB;AAAA,EAC5B;AAAA,EACA;AAAA,EACA;AACF;AAGO,IAAM,8BAA8B;AAAA,EACzC;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AACF;;;ACvFO,SAAS,aAAa,QAAQ;AACnC,QAAM,QAAQ,SAAS,MAAM;AAE7B,MAAI,MAAM,SAAS,GAAG;AACpB,WAAO,EAAE,MAAM,SAAS,aAAa,aAAa,QAAQ,KAAK,EAAE;AAAA,EACnE,WAAW,MAAM,SAAS,GAAG;AAC3B,WAAO,EAAE,MAAM,cAAc,aAAa,SAAS,QAAQ,KAAK,EAAE;AAAA,EACpE,WAAW,MAAM,SAAS,GAAG;AAC3B,WAAO,EAAE,MAAM,WAAW,aAAa,YAAY,QAAQ,KAAK,EAAE;AAAA,EACpE,WAAW,MAAM,SAAS,GAAG;AAC3B,UAAM,SAAS,CAAC;AAChB,aAAS,IAAI,GAAG,IAAI,MAAM,OAAO,KAAK;AACpC,aAAO,KAAK,aAAa,QAAQ,SAAS,MAAM,CAAC,CAAC;AAAA,IACpD;AACA,WAAO,EAAE,MAAM,cAAc,aAAa,OAAO;AAAA,EACnD,WAAW,MAAM,SAAS,GAAG;AAC3B,UAAM,QAAQ,CAAC;AACf,aAAS,IAAI,GAAG,IAAI,MAAM,OAAO,KAAK;AACpC,YAAM,KAAK,SAAS,QAAQ,SAAS,MAAM,CAAC,CAAC;AAAA,IAC/C;AACA,WAAO,EAAE,MAAM,mBAAmB,aAAa,MAAM;AAAA,EACvD,WAAW,MAAM,SAAS,GAAG;AAC3B,UAAM,WAAW,CAAC;AAClB,aAAS,IAAI,GAAG,IAAI,MAAM,OAAO,KAAK;AACpC,eAAS,KAAK,YAAY,QAAQ,SAAS,MAAM,CAAC,CAAC;AAAA,IACrD;AACA,WAAO,EAAE,MAAM,gBAAgB,aAAa,SAAS;AAAA,EACvD,WAAW,MAAM,SAAS,GAAG;AAC3B,UAAM,aAAa,CAAC;AACpB,aAAS,IAAI,GAAG,IAAI,MAAM,OAAO,KAAK;AACpC,iBAAW,KAAK,aAAa,MAAM,CAAC;AAAA,IACtC;AACA,WAAO,EAAE,MAAM,sBAAsB,WAAW;AAAA,EAClD,OAAO;AACL,UAAM,IAAI,MAAM,8BAA8B,MAAM,IAAI,EAAE;AAAA,EAC5D;AACF;AAgBA,SAAS,SAAS,QAAQ;AACxB,QAAM,EAAE,KAAK,IAAI;AACjB,QAAM,eAAe,KAAK,SAAS,OAAO,QAAQ,MAAM;AACxD,QAAM,UAAU,KAAK,UAAU,OAAO,QAAQ,YAAY;AAC1D,SAAO,UAAU;AAEjB,QAAM,OAAO,UAAU;AACvB,QAAM,QAAQ,KAAK,MAAM,UAAU,GAAI;AAEvC,MAAI,QAAQ;AACZ,MAAI,OAAO,KAAK,QAAQ,GAAG;AACzB,YAAQ,KAAK,UAAU,OAAO,QAAQ,YAAY;AAClD,WAAO,UAAU;AAAA,EACnB;AAGA,MAAI,MAAM;AACV,MAAI,MAAO;AACX,MAAI,UAAU,EAAG;AAEjB,SAAO,EAAE,cAAc,MAAM,KAAK,MAAM;AAC1C;AAOA,SAAS,aAAa,QAAQ,OAAO;AACnC,QAAM,SAAS,CAAC;AAChB,WAAS,IAAI,GAAG,IAAI,MAAM,KAAK,KAAK;AAClC,UAAM,QAAQ,OAAO,KAAK,WAAW,OAAO,QAAQ,MAAM,YAAY;AACtE,WAAO,UAAU;AACjB,WAAO,KAAK,KAAK;AAAA,EACnB;AACA,SAAO;AACT;AAOA,SAAS,SAAS,QAAQ,OAAO;AAC/B,QAAM,SAAS,CAAC;AAChB,WAAS,IAAI,GAAG,IAAI,MAAM,OAAO,KAAK;AACpC,WAAO,KAAK,aAAa,QAAQ,KAAK,CAAC;AAAA,EACzC;AACA,SAAO;AACT;AAOA,SAAS,YAAY,QAAQ,OAAO;AAClC,QAAM,EAAE,KAAK,IAAI;AACjB,QAAM,QAAQ,CAAC;AACf,WAAS,IAAI,GAAG,IAAI,MAAM,OAAO,KAAK;AACpC,UAAM,QAAQ,KAAK,UAAU,OAAO,QAAQ,MAAM,YAAY;AAC9D,WAAO,UAAU;AACjB,UAAM,KAAK,SAAS,QAAQ,EAAE,GAAG,OAAO,MAAM,CAAC,CAAC;AAAA,EAClD;AACA,SAAO;AACT;;;ACtHA,IAAM,UAAU,IAAI,YAAY;AAMzB,IAAM,kBAAkB;AAAA,EAC7B,0BAA0B,QAAQ;AAChC,WAAO,IAAI,KAAK,OAAO,MAAM,CAAC;AAAA,EAChC;AAAA,EACA,0BAA0B,QAAQ;AAChC,WAAO,IAAI,KAAK,OAAO,SAAS,KAAK,CAAC;AAAA,EACxC;AAAA,EACA,yBAAyB,OAAO;AAC9B,WAAO,IAAI,KAAK,OAAO,QAAQ,QAAQ,CAAC;AAAA,EAC1C;AAAA,EACA,aAAa,MAAM;AACjB,WAAO,IAAI,KAAK,OAAO,KAAQ;AAAA,EACjC;AAAA,EACA,gBAAgB,OAAO;AACrB,WAAO,SAAS,QAAQ,OAAO,KAAK;AAAA,EACtC;AAAA,EACA,kBAAkB,OAAO;AACvB,WAAO,SAAS,aAAa,EAAE,MAAM,IAAI,SAAS,MAAM,QAAQ,MAAM,YAAY,MAAM,UAAU,GAAG,QAAQ,EAAE,CAAC;AAAA,EAClH;AAAA,EACA,mBAAmB,OAAO;AACxB,WAAO,SAAS,aAAa,EAAE,MAAM,IAAI,SAAS,MAAM,QAAQ,MAAM,YAAY,MAAM,UAAU,GAAG,QAAQ,EAAE,CAAC;AAAA,EAClH;AACF;AAWO,SAAS,sBAAsB,MAAM,YAAY,UAAU,eAAe;AAC/E,MAAI,cAAc,SAAS,SAAS,aAAa,GAAG;AAClD,QAAI,SAAS;AACb,QAAI,gBAAgB,cAAc,EAAE,sBAAsB,aAAa;AAErE,eAAS,IAAI,WAAW,YAAY,KAAK,MAAM;AAAA,IACjD;AACA,aAAS,IAAI,GAAG,IAAI,KAAK,QAAQ,KAAK;AACpC,aAAO,CAAC,IAAI,WAAW,KAAK,CAAC,CAAC;AAAA,IAChC;AACA,WAAO;AAAA,EACT,OAAO;AACL,WAAO,QAAQ,MAAM,aAAa;AAAA,EACpC;AACF;AASO,SAAS,QAAQ,MAAM,eAAe;AAC3C,QAAM,EAAE,SAAS,SAAS,OAAO,KAAK,IAAI;AAC1C,QAAM,EAAE,MAAM,gBAAgB,OAAO,cAAc,MAAM,IAAI;AAC7D,MAAI,UAAU,WAAW;AACvB,UAAM,QAAQ,QAAQ,SAAS;AAC/B,UAAM,SAAS,MAAM,CAAC;AACtB,UAAM,MAAM,IAAI,MAAM,KAAK,MAAM;AACjC,aAAS,IAAI,GAAG,IAAI,IAAI,QAAQ,KAAK;AACnC,UAAI,KAAK,CAAC,aAAa,YAAY;AACjC,YAAI,CAAC,IAAI,aAAa,KAAK,CAAC,CAAC,IAAI;AAAA,MACnC,OAAO;AACL,YAAI,CAAC,IAAI,OAAO,KAAK,CAAC,CAAC,IAAI;AAAA,MAC7B;AAAA,IACF;AACA,WAAO;AAAA,EACT;AACA,MAAI,CAAC,SAAS,SAAS,SAAS;AAC9B,WAAO,MAAM,KAAK,IAAI,EAAE,IAAI,OAAK,QAAQ,yBAAyB,gBAAgB,CAAC,CAAC,CAAC;AAAA,EACvF;AACA,MAAI,UAAU,QAAQ;AACpB,WAAO,MAAM,KAAK,IAAI,EAAE,IAAI,OAAK,QAAQ,aAAa,CAAC,CAAC;AAAA,EAC1D;AACA,MAAI,UAAU,oBAAoB;AAChC,WAAO,MAAM,KAAK,IAAI,EAAE,IAAI,OAAK,QAAQ,0BAA0B,CAAC,CAAC;AAAA,EACvE;AACA,MAAI,UAAU,oBAAoB;AAChC,WAAO,MAAM,KAAK,IAAI,EAAE,IAAI,OAAK,QAAQ,0BAA0B,CAAC,CAAC;AAAA,EACvE;AACA,MAAI,UAAU,QAAQ;AACpB,WAAO,KAAK,IAAI,OAAK,KAAK,MAAM,QAAQ,OAAO,CAAC,CAAC,CAAC;AAAA,EACpD;AACA,MAAI,UAAU,QAAQ;AACpB,UAAM,IAAI,MAAM,4BAA4B;AAAA,EAC9C;AACA,MAAI,UAAU,YAAY;AACxB,UAAM,IAAI,MAAM,gCAAgC;AAAA,EAClD;AACA,MAAI,OAAO,SAAS,YAAY;AAC9B,WAAO,KAAK,IAAI,OAAK,QAAQ,kBAAkB,CAAC,CAAC;AAAA,EACnD;AACA,MAAI,OAAO,SAAS,aAAa;AAC/B,WAAO,KAAK,IAAI,OAAK,QAAQ,mBAAmB,CAAC,CAAC;AAAA,EACpD;AACA,MAAI,UAAU,UAAU,OAAO,SAAS,YAAY,QAAQ,SAAS,cAAc;AACjF,WAAO,KAAK,IAAI,OAAK,QAAQ,gBAAgB,CAAC,CAAC;AAAA,EACjD;AACA,MAAI,UAAU,aAAa,OAAO,SAAS,aAAa,MAAM,aAAa,MAAM,CAAC,MAAM,UAAU;AAChG,QAAI,gBAAgB,eAAe;AACjC,aAAO,IAAI,eAAe,KAAK,QAAQ,KAAK,YAAY,KAAK,MAAM;AAAA,IACrE;AACA,UAAM,MAAM,IAAI,eAAe,KAAK,MAAM;AAC1C,aAAS,IAAI,GAAG,IAAI,IAAI,QAAQ,IAAK,KAAI,CAAC,IAAI,OAAO,KAAK,CAAC,CAAC;AAC5D,WAAO;AAAA,EACT;AACA,MAAI,UAAU,aAAa,OAAO,SAAS,aAAa,MAAM,aAAa,MAAM,CAAC,MAAM,UAAU;AAChG,QAAI,gBAAgB,YAAY;AAC9B,aAAO,IAAI,YAAY,KAAK,QAAQ,KAAK,YAAY,KAAK,MAAM;AAAA,IAClE;AACA,UAAM,MAAM,IAAI,YAAY,KAAK,MAAM;AACvC,aAAS,IAAI,GAAG,IAAI,IAAI,QAAQ,IAAK,KAAI,CAAC,IAAI,KAAK,CAAC;AACpD,WAAO;AAAA,EACT;AACA,MAAI,OAAO,SAAS,WAAW;AAC7B,WAAO,MAAM,KAAK,IAAI,EAAE,IAAI,YAAY;AAAA,EAC1C;AACA,MAAI,OAAO,SAAS,aAAa;AAC/B,UAAM,EAAE,KAAK,IAAI;AAEjB,QAAI,SAAS,QAAQ;AACrB,QAAI,SAAS,SAAU,UAAS,QAAQ;AACxC,QAAI,SAAS,QAAS,UAAS,QAAQ;AACvC,UAAM,MAAM,IAAI,MAAM,KAAK,MAAM;AACjC,aAAS,IAAI,GAAG,IAAI,IAAI,QAAQ,KAAK;AACnC,UAAI,CAAC,IAAI,OAAO,KAAK,CAAC,CAAC;AAAA,IACzB;AACA,WAAO;AAAA,EACT;AACA,SAAO;AACT;AAMO,SAAS,aAAa,OAAO;AAClC,MAAI,CAAC,MAAM,OAAQ,QAAO;AAE1B,MAAI,QAAQ;AACZ,aAAW,QAAQ,OAAO;AACxB,YAAQ,QAAQ,OAAO,OAAO,IAAI;AAAA,EACpC;AAGA,QAAM,OAAO,MAAM,SAAS;AAC5B,MAAI,SAAS,MAAM,OAAO,OAAO,CAAC,GAAG;AACnC,aAAS,MAAM,OAAO,IAAI;AAAA,EAC5B;AAEA,SAAO,OAAO,KAAK;AACrB;AAOA,SAAS,gBAAgB,OAAO;AAC9B,QAAM,QAAQ,SAAS,OAAO;AAC9B,QAAM,OAAO,QAAQ;AACrB,SAAO,OAAO,kBAAkB;AAClC;AAMO,SAAS,aAAa,OAAO;AAClC,MAAI,CAAC,MAAO,QAAO;AACnB,QAAM,QAAQ,MAAM,CAAC,KAAK,IAAI,MAAM,CAAC;AACrC,QAAM,OAAO,SAAS,KAAK,KAAK;AAChC,QAAM,MAAM,SAAS,KAAK;AAC1B,QAAM,OAAO,QAAQ;AACrB,MAAI,QAAQ,EAAG,QAAO,OAAO,KAAK,OAAO,OAAO;AAChD,MAAI,QAAQ,GAAM,QAAO,OAAO,MAAM,OAAO;AAC7C,SAAO,OAAO,MAAM,MAAM,OAAO,IAAI,OAAO;AAC9C;;;ACxLA,SAAS,WAAW,QAAQ,WAAW,MAAM;AAC3C,QAAM,UAAU,OAAO,SAAS;AAChC,QAAM,WAAW,CAAC;AAClB,MAAI,QAAQ;AAGZ,MAAI,QAAQ,cAAc;AACxB,WAAO,SAAS,SAAS,QAAQ,cAAc;AAC7C,YAAM,eAAe,OAAO,YAAY,KAAK;AAC7C,YAAM,QAAQ,WAAW,QAAQ,YAAY,OAAO,CAAC,GAAG,MAAM,aAAa,IAAI,CAAC;AAChF,eAAS,MAAM;AACf,eAAS,KAAK,KAAK;AAAA,IACrB;AAAA,EACF;AAEA,SAAO,EAAE,OAAO,SAAS,UAAU,KAAK;AAC1C;AASO,SAAS,cAAc,QAAQ,MAAM;AAC1C,MAAI,OAAO,WAAW,QAAQ,GAAG,CAAC,CAAC;AACnC,QAAM,OAAO,CAAC,IAAI;AAClB,aAAW,QAAQ,MAAM;AACvB,UAAM,QAAQ,KAAK,SAAS,KAAK,CAAAA,WAASA,OAAM,QAAQ,SAAS,IAAI;AACrE,QAAI,CAAC,MAAO,OAAM,IAAI,MAAM,qCAAqC,IAAI,EAAE;AACvE,SAAK,KAAK,KAAK;AACf,WAAO;AAAA,EACT;AACA,SAAO;AACT;AAQO,SAAS,mBAAmBC,aAAY;AAE7C,QAAM,UAAU,CAAC;AAEjB,WAAS,SAAS,MAAM;AACtB,QAAI,KAAK,SAAS,QAAQ;AACxB,iBAAW,SAAS,KAAK,UAAU;AACjC,iBAAS,KAAK;AAAA,MAChB;AAAA,IACF,OAAO;AACL,cAAQ,KAAK,KAAK,KAAK,KAAK,GAAG,CAAC;AAAA,IAClC;AAAA,EACF;AACA,WAASA,WAAU;AACnB,SAAO;AACT;AAQO,SAAS,sBAAsB,YAAY;AAChD,MAAI,WAAW;AACf,aAAW,EAAE,QAAQ,KAAK,YAAY;AACpC,QAAI,QAAQ,oBAAoB,YAAY;AAC1C;AAAA,IACF;AAAA,EACF;AACA,SAAO;AACT;AAQO,SAAS,sBAAsB,YAAY;AAChD,MAAI,WAAW;AACf,aAAW,EAAE,QAAQ,KAAK,WAAW,MAAM,CAAC,GAAG;AAC7C,QAAI,QAAQ,oBAAoB,YAAY;AAC1C;AAAA,IACF;AAAA,EACF;AACA,SAAO;AACT;AAQO,SAAS,WAAW,QAAQ;AACjC,MAAI,CAAC,OAAQ,QAAO;AACpB,MAAI,OAAO,QAAQ,mBAAmB,OAAQ,QAAO;AACrD,MAAI,OAAO,SAAS,SAAS,EAAG,QAAO;AAEvC,QAAM,aAAa,OAAO,SAAS,CAAC;AACpC,MAAI,WAAW,SAAS,SAAS,EAAG,QAAO;AAC3C,MAAI,WAAW,QAAQ,oBAAoB,WAAY,QAAO;AAE9D,SAAO;AACT;AAQO,SAAS,UAAU,QAAQ;AAChC,MAAI,CAAC,OAAQ,QAAO;AACpB,MAAI,OAAO,QAAQ,mBAAmB,MAAO,QAAO;AACpD,MAAI,OAAO,SAAS,SAAS,EAAG,QAAO;AAEvC,QAAM,aAAa,OAAO,SAAS,CAAC;AACpC,MAAI,WAAW,SAAS,WAAW,EAAG,QAAO;AAC7C,MAAI,WAAW,QAAQ,oBAAoB,WAAY,QAAO;AAE9D,QAAM,WAAW,WAAW,SAAS,KAAK,WAAS,MAAM,QAAQ,SAAS,KAAK;AAC/E,MAAI,UAAU,QAAQ,oBAAoB,WAAY,QAAO;AAE7D,QAAM,aAAa,WAAW,SAAS,KAAK,WAAS,MAAM,QAAQ,SAAS,OAAO;AACnF,MAAI,YAAY,QAAQ,oBAAoB,WAAY,QAAO;AAE/D,SAAO;AACT;AAQO,SAAS,aAAa,YAAY;AACvC,MAAI,WAAW,WAAW,EAAG,QAAO;AACpC,QAAM,CAAC,EAAE,MAAM,IAAI;AACnB,MAAI,OAAO,QAAQ,oBAAoB,WAAY,QAAO;AAC1D,MAAI,OAAO,SAAS,OAAQ,QAAO;AACnC,SAAO;AACT;;;AC1JO,IAAM,cAAc;AAAA,EACzB,MAAM;AAAA,EACN,MAAM;AAAA,EACN,OAAO;AAAA,EACP,MAAM;AAAA,EACN,KAAK;AAAA,EACL,KAAK;AAAA,EACL,KAAK;AAAA,EACL,QAAQ;AAAA,EACR,QAAQ;AAAA,EACR,MAAM;AAAA,EACN,KAAK;AAAA,EACL,KAAK;AAAA,EACL,QAAQ;AAAA,EACR,MAAM;AACR;AAQO,SAAS,4BAA4B,QAAQ;AAClD,MAAI,UAAU;AAEd,QAAM,QAAQ,CAAC;AAEf,SAAO,OAAO,SAAS,OAAO,KAAK,YAAY;AAE7C,UAAM,CAAC,MAAM,KAAK,UAAU,IAAI,eAAe,QAAQ,OAAO;AAC9D,cAAU;AAEV,QAAI,SAAS,YAAY,MAAM;AAC7B;AAAA,IACF;AAGA,UAAM,SAAS,GAAG,EAAE,IAAI,YAAY,QAAQ,IAAI;AAAA,EAClD;AAEA,SAAO;AACT;AAUA,SAAS,YAAY,QAAQ,MAAM;AACjC,UAAQ,MAAM;AAAA,IACd,KAAK,YAAY;AACf,aAAO;AAAA,IACT,KAAK,YAAY;AACf,aAAO;AAAA,IACT,KAAK,YAAY;AAEf,aAAO,OAAO,KAAK,QAAQ,OAAO,QAAQ;AAAA,IAC5C,KAAK,YAAY;AAAA,IACjB,KAAK,YAAY;AACf,aAAO,WAAW,MAAM;AAAA,IAC1B,KAAK,YAAY;AACf,aAAO,iBAAiB,MAAM;AAAA,IAChC,KAAK,YAAY,QAAQ;AACvB,YAAM,QAAQ,OAAO,KAAK,WAAW,OAAO,QAAQ,IAAI;AACxD,aAAO,UAAU;AACjB,aAAO;AAAA,IACT;AAAA,IACA,KAAK,YAAY,QAAQ;AACvB,YAAM,eAAe,WAAW,MAAM;AACtC,YAAM,WAAW,IAAI,WAAW,OAAO,KAAK,QAAQ,OAAO,KAAK,aAAa,OAAO,QAAQ,YAAY;AACxG,aAAO,UAAU;AACjB,aAAO;AAAA,IACT;AAAA,IACA,KAAK,YAAY,MAAM;AACrB,YAAM,OAAO,OAAO,KAAK,SAAS,OAAO,QAAQ;AACjD,YAAM,WAAW,OAAO;AACxB,UAAI,WAAW,QAAQ;AACvB,UAAI,aAAa,IAAI;AACnB,mBAAW,WAAW,MAAM;AAAA,MAC9B;AACA,YAAM,WAAW,aAAa,YAAY,QAAQ,aAAa,YAAY;AAC3E,YAAM,SAAS,IAAI,MAAM,QAAQ;AACjC,eAAS,IAAI,GAAG,IAAI,UAAU,KAAK;AACjC,eAAO,CAAC,IAAI,WAAW,YAAY,QAAQ,YAAY,IAAI,MAAM,IAAI,YAAY,QAAQ,QAAQ;AAAA,MACnG;AACA,aAAO;AAAA,IACT;AAAA,IACA,KAAK,YAAY,QAAQ;AAEvB,YAAM,eAAe,CAAC;AACtB,UAAI,UAAU;AACd,aAAO,MAAM;AACX,cAAM,CAAC,WAAW,KAAK,UAAU,IAAI,eAAe,QAAQ,OAAO;AACnE,kBAAU;AACV,YAAI,cAAc,YAAY,MAAM;AAClC;AAAA,QACF;AACA,qBAAa,SAAS,GAAG,EAAE,IAAI,YAAY,QAAQ,SAAS;AAAA,MAC9D;AACA,aAAO;AAAA,IACT;AAAA;AAAA,IAEA;AACE,YAAM,IAAI,MAAM,0BAA0B,IAAI,EAAE;AAAA,EAClD;AACF;AASO,SAAS,WAAW,QAAQ;AACjC,MAAI,SAAS;AACb,MAAI,QAAQ;AACZ,SAAO,MAAM;AACX,UAAM,OAAO,OAAO,KAAK,SAAS,OAAO,QAAQ;AACjD,eAAW,OAAO,QAAS;AAC3B,QAAI,EAAE,OAAO,MAAO;AAClB,aAAO;AAAA,IACT;AACA,aAAS;AAAA,EACX;AACF;AAQA,SAAS,cAAc,QAAQ;AAC7B,MAAI,SAAS;AACb,MAAI,QAAQ;AACZ,SAAO,MAAM;AACX,UAAM,OAAO,OAAO,KAAK,SAAS,OAAO,QAAQ;AACjD,cAAU,OAAO,OAAO,GAAI,KAAK;AACjC,QAAI,EAAE,OAAO,MAAO;AAClB,aAAO;AAAA,IACT;AACA,aAAS;AAAA,EACX;AACF;AASO,SAAS,WAAW,QAAQ;AACjC,QAAM,SAAS,WAAW,MAAM;AAEhC,SAAO,WAAW,IAAI,EAAE,SAAS;AACnC;AASO,SAAS,iBAAiB,QAAQ;AACvC,QAAM,SAAS,cAAc,MAAM;AAEnC,SAAO,UAAU,KAAK,EAAE,SAAS;AACnC;AASA,SAAS,eAAe,QAAQ,SAAS;AACvC,QAAM,OAAO,OAAO,KAAK,SAAS,OAAO,QAAQ;AACjD,QAAM,OAAO,OAAO;AACpB,MAAI,SAAS,YAAY,MAAM;AAE7B,WAAO,CAAC,GAAG,GAAG,OAAO;AAAA,EACvB;AACA,QAAM,QAAQ,QAAQ;AACtB,QAAM,MAAM,QAAQ,UAAU,QAAQ,WAAW,MAAM;AACvD,SAAO,CAAC,MAAM,KAAK,GAAG;AACxB;;;AC5LO,SAAS,eAAe,QAAQ,oBAAoB;AAGzD,QAAM,UAAU,oBAAI,IAAI;AACxB,QAAM,MAAM,oBAAoB,KAAK,CAAC,EAAE,IAAI,MAAM,QAAQ,KAAK,GAAG;AAClE,QAAM,kBAAkB,OAAO,KAAK,MAAM,GAAG,GAAG,YAAY,CAAC;AAC7D,aAAW,CAAC,MAAM,MAAM,KAAK,OAAO,QAAQ,cAAc,GAAG;AAC3D,QAAI,OAAO,aAAa,OAAO;AAC7B;AAAA,IACF;AACA,UAAM,OAAO,OAAO,UAAU,cAAc,cAAc;AAC1D,UAAM,KAAK,OAAO,KAAK,MAAM,OAAO,KAAK,MAAM,CAAC;AAChD,UAAM,MAAM,KAAK,GAAG,GAAG,SAAS,IAAI,GAAG,KAAK,SAAS,CAAC,KAAK;AAE3D,YAAQ,IAAI,MAAM,EAAE,MAAM,IAAI,CAAC;AAAA,EACjC;AAIA,WAAS,IAAI,GAAG,IAAI,OAAO,QAAQ,KAAK;AACtC,UAAM,UAAU,OAAO,CAAC;AACxB,UAAM,EAAE,cAAc,MAAM,cAAc,iBAAiB,KAAK,IAAI;AACpE,QAAI,cAAc;AAChB,WAAK;AACL;AAAA,IACF;AACA,QAAI,SAAS,gBAAgB,iBAAiB,UAAa,oBAAoB,YAAY;AACzF,cAAQ,eAAe,QAAQ,IAAI,IAAI;AAAA,IACzC;AAAA,EACF;AACF;;;AC9BO,IAAM,0BAA0B,KAAK;AAE5C,IAAMC,WAAU,IAAI,YAAY;AAChC,SAAS,OAAiC,OAAO;AAC/C,SAAO,SAASA,SAAQ,OAAO,KAAK;AACtC;AA0BA,eAAsB,qBAAqB,aAAa,EAAE,SAAS,mBAAmB,yBAAyB,aAAa,KAAK,IAAI,CAAC,GAAG;AACvI,MAAI,CAAC,eAAe,EAAE,YAAY,cAAc,GAAI,OAAM,IAAI,MAAM,8BAA8B;AAGlG,QAAM,eAAe,KAAK,IAAI,GAAG,YAAY,aAAa,gBAAgB;AAC1E,QAAM,eAAe,MAAM,YAAY,MAAM,cAAc,YAAY,UAAU;AAGjF,QAAM,aAAa,IAAI,SAAS,YAAY;AAC5C,MAAI,WAAW,UAAU,aAAa,aAAa,GAAG,IAAI,MAAM,WAAY;AAC1E,UAAM,IAAI,MAAM,uCAAuC;AAAA,EACzD;AAIA,QAAM,iBAAiB,WAAW,UAAU,aAAa,aAAa,GAAG,IAAI;AAC7E,MAAI,iBAAiB,YAAY,aAAa,GAAG;AAC/C,UAAM,IAAI,MAAM,2BAA2B,cAAc,6BAA6B,YAAY,aAAa,CAAC,EAAE;AAAA,EACpH;AAGA,MAAI,iBAAiB,IAAI,kBAAkB;AAEzC,UAAM,iBAAiB,YAAY,aAAa,iBAAiB;AACjE,UAAM,iBAAiB,MAAM,YAAY,MAAM,gBAAgB,YAAY;AAE3E,UAAM,iBAAiB,IAAI,YAAY,iBAAiB,CAAC;AACzD,UAAM,eAAe,IAAI,WAAW,cAAc;AAClD,iBAAa,IAAI,IAAI,WAAW,cAAc,CAAC;AAC/C,iBAAa,IAAI,IAAI,WAAW,YAAY,GAAG,eAAe,cAAc;AAC5E,WAAO,gBAAgB,gBAAgB,EAAE,SAAS,WAAW,CAAC;AAAA,EAChE,OAAO;AAEL,WAAO,gBAAgB,cAAc,EAAE,SAAS,WAAW,CAAC;AAAA,EAC9D;AACF;AAUO,SAAS,gBAAgB,aAAa,EAAE,SAAS,aAAa,KAAK,IAAI,CAAC,GAAG;AAChF,MAAI,EAAE,uBAAuB,aAAc,OAAM,IAAI,MAAM,8BAA8B;AACzF,QAAM,OAAO,IAAI,SAAS,WAAW;AAGrC,YAAU,EAAE,GAAG,iBAAiB,GAAG,QAAQ;AAG3C,MAAI,KAAK,aAAa,GAAG;AACvB,UAAM,IAAI,MAAM,2BAA2B;AAAA,EAC7C;AACA,MAAI,KAAK,UAAU,KAAK,aAAa,GAAG,IAAI,MAAM,WAAY;AAC5D,UAAM,IAAI,MAAM,uCAAuC;AAAA,EACzD;AAIA,QAAM,uBAAuB,KAAK,aAAa;AAC/C,QAAM,iBAAiB,KAAK,UAAU,sBAAsB,IAAI;AAChE,MAAI,iBAAiB,KAAK,aAAa,GAAG;AAExC,UAAM,IAAI,MAAM,2BAA2B,cAAc,6BAA6B,KAAK,aAAa,CAAC,EAAE;AAAA,EAC7G;AAEA,QAAM,iBAAiB,uBAAuB;AAC9C,QAAM,SAAS,EAAE,MAAM,QAAQ,eAAe;AAC9C,QAAM,WAAW,4BAA4B,MAAM;AAGnD,QAAM,UAAU,SAAS;AAEzB,QAAM,SAAS,SAAS,QAAQ,IAAI,CAAoB,WAAW;AAAA,IACjE,MAAM,aAAa,MAAM,OAAO;AAAA,IAChC,aAAa,MAAM;AAAA,IACnB,iBAAiB,qBAAqB,MAAM,OAAO;AAAA,IACnD,MAAM,OAAO,MAAM,OAAO;AAAA,IAC1B,cAAc,MAAM;AAAA,IACpB,gBAAgB,eAAe,MAAM,OAAO;AAAA,IAC5C,OAAO,MAAM;AAAA,IACb,WAAW,MAAM;AAAA,IACjB,UAAU,MAAM;AAAA,IAChB,cAAc,YAAY,MAAM,QAAQ;AAAA,EAC1C,EAAE;AAEF,QAAM,eAAe,OAAO,OAAO,OAAK,EAAE,IAAI;AAC9C,QAAM,WAAW,SAAS;AAC1B,QAAM,aAAa,SAAS,QAAQ,IAAI,CAAoB,cAAc;AAAA,IACxE,SAAS,SAAS,QAAQ,IAAI,CAAoB,QAA8B,iBAAiB;AAAA,MAC/F,WAAW,OAAO,OAAO,OAAO;AAAA,MAChC,aAAa,OAAO;AAAA,MACpB,WAAW,OAAO,WAAW;AAAA,QAC3B,MAAM,aAAa,OAAO,QAAQ,OAAO;AAAA,QACzC,WAAW,OAAO,QAAQ,SAAS,IAAI,CAAuB,MAAM,UAAU,CAAC,CAAC;AAAA,QAChF,gBAAgB,OAAO,QAAQ,QAAQ,IAAI,MAAM;AAAA,QACjD,OAAO,kBAAkB,OAAO,QAAQ,OAAO;AAAA,QAC/C,YAAY,OAAO,QAAQ;AAAA,QAC3B,yBAAyB,OAAO,QAAQ;AAAA,QACxC,uBAAuB,OAAO,QAAQ;AAAA,QACtC,oBAAoB,OAAO,QAAQ,SAAS,IAAI,CAAoB,QAAQ;AAAA,UAC1E,KAAK,OAAO,GAAG,OAAO;AAAA,UACtB,OAAO,OAAO,GAAG,OAAO;AAAA,QAC1B,EAAE;AAAA,QACF,kBAAkB,OAAO,QAAQ;AAAA,QACjC,mBAAmB,OAAO,QAAQ;AAAA,QAClC,wBAAwB,OAAO,QAAQ;AAAA,QACvC,YAAY,aAAa,OAAO,QAAQ,UAAU,aAAa,WAAW,GAAG,OAAO;AAAA,QACpF,gBAAgB,OAAO,QAAQ,UAAU,IAAI,CAAoB,kBAAkB;AAAA,UACjF,WAAW,UAAU,aAAa,OAAO;AAAA,UACzC,UAAU,UAAU,aAAa,OAAO;AAAA,UACxC,OAAO,aAAa;AAAA,QACtB,EAAE;AAAA,QACF,qBAAqB,OAAO,QAAQ;AAAA,QACpC,qBAAqB,OAAO,QAAQ;AAAA,QACpC,iBAAiB,OAAO,QAAQ,YAAY;AAAA,UAC1C,iCAAiC,OAAO,QAAQ,SAAS;AAAA,UACzD,4BAA4B,OAAO,QAAQ,SAAS;AAAA,UACpD,4BAA4B,OAAO,QAAQ,SAAS;AAAA,QACtD;AAAA,QACA,uBAAuB,OAAO,QAAQ,YAAY;AAAA,UAChD,MAAM,OAAO,QAAQ,SAAS,WAAW;AAAA,YACvC,MAAM,OAAO,QAAQ,SAAS,QAAQ;AAAA,YACtC,MAAM,OAAO,QAAQ,SAAS,QAAQ;AAAA,YACtC,MAAM,OAAO,QAAQ,SAAS,QAAQ;AAAA,YACtC,MAAM,OAAO,QAAQ,SAAS,QAAQ;AAAA,YACtC,MAAM,OAAO,QAAQ,SAAS,QAAQ;AAAA,YACtC,MAAM,OAAO,QAAQ,SAAS,QAAQ;AAAA,YACtC,MAAM,OAAO,QAAQ,SAAS,QAAQ;AAAA,YACtC,MAAM,OAAO,QAAQ,SAAS,QAAQ;AAAA,UACxC;AAAA,UACA,kBAAkB,OAAO,QAAQ,SAAS;AAAA,QAC5C;AAAA,MACF;AAAA,MACA,qBAAqB,OAAO;AAAA,MAC5B,qBAAqB,OAAO;AAAA,MAC5B,qBAAqB,OAAO;AAAA,MAC5B,qBAAqB,OAAO;AAAA,MAC5B,iBAAiB,OAAO;AAAA,MACxB,2BAA2B,OAAO;AAAA,IACpC,EAAE;AAAA,IACF,iBAAiB,SAAS;AAAA,IAC1B,UAAU,SAAS;AAAA,IACnB,iBAAiB,SAAS,SAAS,IAAI,CAAoB,mBAAmB;AAAA,MAC5E,YAAY,cAAc;AAAA,MAC1B,YAAY,cAAc;AAAA,MAC1B,aAAa,cAAc;AAAA,IAC7B,EAAE;AAAA,IACF,aAAa,SAAS;AAAA,IACtB,uBAAuB,SAAS;AAAA,IAChC,SAAS,SAAS;AAAA,EACpB,EAAE;AAEF,QAAM,qBAAqB,SAAS,SAAS,IAAI,CAAoB,QAAQ;AAAA,IAC3E,KAAK,OAAO,GAAG,OAAO;AAAA,IACtB,OAAO,OAAO,GAAG,OAAO;AAAA,EAC1B,EAAE;AACF,QAAM,aAAa,OAAO,SAAS,OAAO;AAE1C,MAAI,YAAY;AACd,mBAAe,QAAQ,kBAAkB;AAAA,EAC3C;AAEA,SAAO;AAAA,IACL;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA,iBAAiB;AAAA,EACnB;AACF;AAQO,SAAS,cAAc,EAAE,OAAO,GAAG;AACxC,SAAO,cAAc,QAAQ,CAAC,CAAC,EAAE,CAAC;AACpC;AAMA,SAAS,YAAYC,cAAa;AAChC,MAAIA,cAAa,QAAS,QAAO,EAAE,MAAM,SAAS;AAClD,MAAIA,cAAa,QAAS,QAAO,EAAE,MAAM,MAAM;AAC/C,MAAIA,cAAa,QAAS,QAAO,EAAE,MAAM,OAAO;AAChD,MAAIA,cAAa,QAAS,QAAO,EAAE,MAAM,OAAO;AAChD,MAAIA,cAAa,QAAS,QAAO;AAAA,IAC/B,MAAM;AAAA,IACN,OAAOA,aAAY,QAAQ;AAAA,IAC3B,WAAWA,aAAY,QAAQ;AAAA,EACjC;AACA,MAAIA,cAAa,QAAS,QAAO,EAAE,MAAM,OAAO;AAChD,MAAIA,cAAa,QAAS,QAAO;AAAA,IAC/B,MAAM;AAAA,IACN,iBAAiBA,aAAY,QAAQ;AAAA,IACrC,MAAM,SAASA,aAAY,QAAQ,OAAO;AAAA,EAC5C;AACA,MAAIA,cAAa,QAAS,QAAO;AAAA,IAC/B,MAAM;AAAA,IACN,iBAAiBA,aAAY,QAAQ;AAAA,IACrC,MAAM,SAASA,aAAY,QAAQ,OAAO;AAAA,EAC5C;AACA,MAAIA,cAAa,SAAU,QAAO;AAAA,IAChC,MAAM;AAAA,IACN,UAAUA,aAAY,SAAS;AAAA,IAC/B,UAAUA,aAAY,SAAS;AAAA,EACjC;AACA,MAAIA,cAAa,SAAU,QAAO,EAAE,MAAM,OAAO;AACjD,MAAIA,cAAa,SAAU,QAAO,EAAE,MAAM,OAAO;AACjD,MAAIA,cAAa,SAAU,QAAO,EAAE,MAAM,OAAO;AACjD,MAAIA,cAAa,SAAU,QAAO,EAAE,MAAM,OAAO;AACjD,MAAIA,cAAa,SAAU,QAAO,EAAE,MAAM,UAAU;AACpD,MAAIA,cAAa,SAAU,QAAO;AAAA,IAChC,MAAM;AAAA,IACN,uBAAuBA,aAAY,SAAS;AAAA,EAC9C;AACA,MAAIA,cAAa,SAAU,QAAO;AAAA,IAChC,MAAM;AAAA,IACN,KAAK,OAAOA,aAAY,SAAS,OAAO;AAAA,EAC1C;AACA,MAAIA,cAAa,SAAU,QAAO;AAAA,IAChC,MAAM;AAAA,IACN,KAAK,OAAOA,aAAY,SAAS,OAAO;AAAA,IACxC,WAAW,4BAA4BA,aAAY,SAAS,OAAO;AAAA,EACrE;AACA,SAAOA;AACT;AAMA,SAAS,SAAS,MAAM;AACtB,MAAI,KAAK,QAAS,QAAO;AACzB,MAAI,KAAK,QAAS,QAAO;AACzB,MAAI,KAAK,QAAS,QAAO;AACzB,QAAM,IAAI,MAAM,4BAA4B;AAC9C;AAWA,SAAS,aAAa,OAAO,QAAQ,SAAS;AAC5C,SAAO,SAAS;AAAA,IACd,KAAK,gBAAgB,MAAM,SAAS,QAAQ,OAAO;AAAA,IACnD,KAAK,gBAAgB,MAAM,SAAS,QAAQ,OAAO;AAAA,IACnD,YAAY,MAAM;AAAA,IAClB,gBAAgB,MAAM;AAAA,IACtB,WAAW,gBAAgB,MAAM,SAAS,QAAQ,OAAO;AAAA,IACzD,WAAW,gBAAgB,MAAM,SAAS,QAAQ,OAAO;AAAA,IACzD,oBAAoB,MAAM;AAAA,IAC1B,oBAAoB,MAAM;AAAA,EAC5B;AACF;AAQO,SAAS,gBAAgB,OAAO,QAAQ,SAAS;AACtD,QAAM,EAAE,MAAM,gBAAgB,aAAa,IAAI;AAC/C,MAAI,UAAU,OAAW,QAAO;AAChC,MAAI,SAAS,UAAW,QAAO,MAAM,CAAC,MAAM;AAC5C,MAAI,SAAS,aAAc,QAAO,QAAQ,gBAAgB,KAAK;AAC/D,QAAM,OAAO,IAAI,SAAS,MAAM,QAAQ,MAAM,YAAY,MAAM,UAAU;AAC1E,MAAI,SAAS,WAAW,KAAK,eAAe,EAAG,QAAO,KAAK,WAAW,GAAG,IAAI;AAC7E,MAAI,SAAS,YAAY,KAAK,eAAe,EAAG,QAAO,KAAK,WAAW,GAAG,IAAI;AAC9E,MAAI,SAAS,WAAW,mBAAmB,OAAQ,QAAO,QAAQ,aAAa,KAAK,SAAS,GAAG,IAAI,CAAC;AACrG,MAAI,SAAS,WAAW,mBAAmB,mBAAoB,QAAO,QAAQ,0BAA0B,KAAK,YAAY,GAAG,IAAI,CAAC;AACjI,MAAI,SAAS,WAAW,mBAAmB,mBAAoB,QAAO,QAAQ,0BAA0B,KAAK,YAAY,GAAG,IAAI,CAAC;AACjI,MAAI,SAAS,WAAW,cAAc,SAAS,eAAe,cAAc,SAAS,QAAS,QAAO,QAAQ,yBAAyB,KAAK,YAAY,GAAG,IAAI,CAAC;AAC/J,MAAI,SAAS,WAAW,cAAc,SAAS,eAAe,cAAc,SAAS,SAAU,QAAO,QAAQ,0BAA0B,KAAK,YAAY,GAAG,IAAI,CAAC;AACjK,MAAI,SAAS,WAAW,cAAc,SAAS,YAAa,QAAO,QAAQ,0BAA0B,KAAK,YAAY,GAAG,IAAI,CAAC;AAC9H,MAAI,SAAS,WAAW,KAAK,eAAe,EAAG,QAAO,KAAK,SAAS,GAAG,IAAI;AAC3E,MAAI,SAAS,WAAW,KAAK,eAAe,EAAG,QAAO,KAAK,YAAY,GAAG,IAAI;AAC9E,MAAI,mBAAmB,UAAW,QAAO,aAAa,KAAK,IAAI,MAAM,EAAE,OAAO,SAAS;AACvF,MAAI,cAAc,SAAS,UAAW,QAAO,aAAa,KAAK;AAC/D,MAAI,SAAS,uBAAwB,QAAO;AAE5C,SAAO;AACT;;;ACpUO,SAAS,gBAAgB,QAAQ,QAAQ,UAAU,QAAW;AACnE,YAAU,EAAE,GAAG,iBAAiB,GAAG,QAAQ;AAE3C,QAAM,SAAS,4BAA4B,MAAM;AACjD,SAAO;AAAA,IACL,YAAY,OAAO;AAAA,IACnB,YAAY,OAAO,QAAQ,IAAI,CAAoB,MAAM,gBAAgB,GAAG,QAAQ,OAAO,CAAC;AAAA,IAC5F,YAAY,OAAO,QAAQ,IAAI,CAAoB,MAAM,gBAAgB,GAAG,QAAQ,OAAO,CAAC;AAAA,IAC5F,gBAAgB,eAAe,OAAO,OAAO;AAAA,IAC7C,aAAa,OAAO;AAAA,IACpB,6BAA6B,OAAO;AAAA,IACpC,6BAA6B,OAAO;AAAA,EACtC;AACF;AAMO,SAAS,gBAAgB,QAAQ;AACtC,QAAM,SAAS,4BAA4B,MAAM;AACjD,SAAO;AAAA,IACL,gBAAgB,OAAO,QAAQ,IAAI,YAAY;AAAA,IAC/C,iCAAiC,OAAO;AAAA,EAC1C;AACF;AAOA,SAAS,aAAa,KAAK;AACzB,SAAO;AAAA,IACL,QAAQ,IAAI;AAAA,IACZ,sBAAsB,IAAI;AAAA,IAC1B,iBAAiB,IAAI;AAAA,EACvB;AACF;;;ACzCO,SAAS,OAAO,KAAK;AAC1B,MAAI,QAAQ,OAAW,QAAO;AAC9B,MAAI,OAAO,QAAQ,SAAU,QAAO,OAAO,GAAG;AAC9C,MAAI,MAAM,QAAQ,GAAG,EAAG,QAAO,IAAI,IAAI,MAAM;AAC7C,MAAI,eAAe,WAAY,QAAO,MAAM,KAAK,GAAG;AACpD,MAAI,eAAe,KAAM,QAAO,IAAI,YAAY;AAChD,MAAI,eAAe,QAAQ;AAEzB,UAAM,SAAS,CAAC;AAChB,eAAW,OAAO,OAAO,KAAK,GAAG,GAAG;AAClC,UAAI,IAAI,GAAG,MAAM,OAAW;AAC5B,aAAO,GAAG,IAAI,OAAO,IAAI,GAAG,CAAC;AAAA,IAC/B;AACA,WAAO;AAAA,EACT;AACA,SAAO;AACT;AAQO,SAAS,OAAO,KAAK,KAAK;AAC/B,QAAM,QAAQ;AACd,WAAS,IAAI,GAAG,IAAI,IAAI,QAAQ,KAAK,OAAO;AAC1C,QAAI,KAAK,GAAG,IAAI,MAAM,GAAG,IAAI,KAAK,CAAC;AAAA,EACrC;AACF;AAUO,SAAS,OAAO,GAAG,GAAG,SAAS,MAAM;AAE1C,MAAI,SAAS,MAAM,IAAI,KAAK,EAAG,QAAO;AACtC,MAAI,aAAa,cAAc,aAAa,WAAY,QAAO,OAAO,MAAM,KAAK,CAAC,GAAG,MAAM,KAAK,CAAC,GAAG,MAAM;AAC1G,MAAI,CAAC,KAAK,CAAC,KAAK,OAAO,MAAM,OAAO,EAAG,QAAO;AAC9C,MAAI,MAAM,QAAQ,CAAC,KAAK,MAAM,QAAQ,CAAC,GAAG;AACxC,QAAI,EAAE,WAAW,EAAE,OAAQ,QAAO;AAClC,aAAS,IAAI,GAAG,IAAI,EAAE,QAAQ,KAAK;AACjC,UAAI,CAAC,OAAO,EAAE,CAAC,GAAG,EAAE,CAAC,GAAG,MAAM,EAAG,QAAO;AAAA,IAC1C;AACA,WAAO;AAAA,EACT;AACA,MAAI,OAAO,MAAM,SAAU,QAAO;AAClC,QAAM,QAAQ,OAAO,KAAK,CAAC;AAC3B,MAAI,MAAM,WAAW,OAAO,KAAK,CAAC,EAAE,OAAQ,QAAO;AACnD,aAAW,KAAK,OAAO;AACrB,QAAI,CAAC,OAAO,EAAE,CAAC,GAAG,EAAE,CAAC,GAAG,MAAM,EAAG,QAAO;AAAA,EAC1C;AACA,SAAO;AACT;AAWA,eAAe,4BAA4B,KAAK,cAAc,CAAC,GAAG,UAAU,WAAW,OAAO;AAC5F,QAAM,aAAa,IAAI,gBAAgB;AACvC,QAAM,UAAU,IAAI,QAAQ,YAAY,OAAO;AAC/C,UAAQ,IAAI,SAAS,WAAW;AAEhC,QAAM,MAAM,MAAM,QAAQ,KAAK;AAAA,IAC7B,GAAG;AAAA,IACH;AAAA,IACA,QAAQ,WAAW;AAAA,EACrB,CAAC;AAED,MAAI,CAAC,IAAI,GAAI,OAAM,IAAI,MAAM,2BAA2B,IAAI,MAAM,EAAE;AAGpE,MAAI,IAAI,WAAW,KAAK;AACtB,UAAM,eAAe,IAAI,QAAQ,IAAI,eAAe;AACpD,QAAI,CAAC,aAAc,OAAM,IAAI,MAAM,8BAA8B;AAGjE,UAAM,QAAQ,aAAa,MAAM,sBAAsB;AACvD,QAAI,CAAC,MAAO,OAAM,IAAI,MAAM,iCAAiC,YAAY,EAAE;AAE3E,WAAO,SAAS,MAAM,CAAC,CAAC;AAAA,EAC1B;AAGA,MAAI,IAAI,WAAW,KAAK;AACtB,UAAM,gBAAgB,IAAI,QAAQ,IAAI,gBAAgB;AAGtD,eAAW,MAAM;AAEjB,QAAI,cAAe,QAAO,SAAS,aAAa;AAAA,EAClD;AAEA,QAAM,IAAI,MAAM,mEAAmE;AACrF;AAaA,eAAsB,kBAAkB,KAAK,aAAa,aAAa;AACrE,QAAM,QAAQ,eAAe,WAAW;AACxC,QAAM,MAAM,MAAM,MAAM,KAAK,EAAE,GAAG,aAAa,QAAQ,OAAO,CAAC;AAG/D,MAAI,IAAI,WAAW,KAAK;AACtB,WAAO,4BAA4B,KAAK,aAAa,KAAK;AAAA,EAC5D;AAEA,MAAI,CAAC,IAAI,GAAI,OAAM,IAAI,MAAM,qBAAqB,IAAI,MAAM,EAAE;AAC9D,QAAM,SAAS,IAAI,QAAQ,IAAI,gBAAgB;AAE/C,MAAI,CAAC,QAAQ;AACX,WAAO,4BAA4B,KAAK,aAAa,KAAK;AAAA,EAC5D;AACA,SAAO,SAAS,MAAM;AACxB;AAeA,eAAsB,mBAAmB,EAAE,KAAK,YAAY,aAAa,OAAO,YAAY,GAAG;AAC7F,MAAI,CAAC,IAAK,OAAM,IAAI,MAAM,aAAa;AACvC,QAAM,QAAQ,eAAe,WAAW;AAExC,iBAAe,MAAM,kBAAkB,KAAK,aAAa,KAAK;AAM9D,MAAI,SAAS;AACb,QAAM,OAAO,eAAe,CAAC;AAE7B,SAAO;AAAA,IACL;AAAA,IACA,MAAM,MAAM,OAAO,KAAK;AACtB,UAAI,QAAQ;AACV,eAAO,OAAO,KAAK,CAAAC,YAAUA,QAAO,MAAM,OAAO,GAAG,CAAC;AAAA,MACvD;AAEA,YAAM,UAAU,IAAI,QAAQ,KAAK,OAAO;AACxC,YAAM,SAAS,QAAQ,SAAY,KAAK,MAAM;AAC9C,cAAQ,IAAI,SAAS,SAAS,KAAK,IAAI,MAAM,EAAE;AAE/C,YAAM,MAAM,MAAM,MAAM,KAAK,EAAE,GAAG,MAAM,QAAQ,CAAC;AACjD,UAAI,CAAC,IAAI,MAAM,CAAC,IAAI,KAAM,OAAM,IAAI,MAAM,gBAAgB,IAAI,MAAM,EAAE;AAEtE,UAAI,IAAI,WAAW,KAAK;AAEtB,iBAAS,IAAI,YAAY;AACzB,eAAO,OAAO,KAAK,CAAAA,YAAUA,QAAO,MAAM,OAAO,GAAG,CAAC;AAAA,MACvD,WAAW,IAAI,WAAW,KAAK;AAE7B,eAAO,IAAI,YAAY;AAAA,MACzB,OAAO;AACL,cAAM,IAAI,MAAM,yCAAyC,IAAI,MAAM,EAAE;AAAA,MACvE;AAAA,IACF;AAAA,EACF;AACF;AAUO,SAAS,kBAAkB,EAAE,YAAY,MAAM,GAAG,EAAE,UAAU,wBAAwB,IAAI,CAAC,GAAG;AACnG,MAAI,aAAa,SAAS;AAExB,UAAM,SAAS,MAAM,GAAG,UAAU;AAClC,WAAO;AAAA,MACL;AAAA,MACA,MAAM,MAAM,OAAO,KAAK;AACtB,gBAAQ,MAAM,QAAQ,MAAM,OAAO,GAAG;AAAA,MACxC;AAAA,IACF;AAAA,EACF;AACA,QAAM,QAAQ,oBAAI,IAAI;AACtB,SAAO;AAAA,IACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAMA,MAAM,OAAO,KAAK;AAChB,YAAM,MAAM,SAAS,OAAO,KAAK,UAAU;AAC3C,YAAM,SAAS,MAAM,IAAI,GAAG;AAC5B,UAAI,OAAQ,QAAO;AAEnB,YAAM,UAAU,MAAM,OAAO,GAAG;AAChC,YAAM,IAAI,KAAK,OAAO;AACtB,aAAO;AAAA,IACT;AAAA,EACF;AACF;AAaA,SAAS,SAAS,OAAO,KAAK,MAAM;AAClC,MAAI,QAAQ,GAAG;AACb,QAAI,QAAQ,OAAW,OAAM,IAAI,MAAM,yBAAyB,KAAK,KAAK,GAAG,GAAG;AAChF,QAAI,SAAS,OAAW,QAAO,GAAG,KAAK;AACvC,WAAO,GAAG,OAAO,KAAK,IAAI,IAAI;AAAA,EAChC,WAAW,QAAQ,QAAW;AAC5B,QAAI,QAAQ,IAAK,OAAM,IAAI,MAAM,wBAAwB,KAAK,KAAK,GAAG,GAAG;AACzE,WAAO,GAAG,KAAK,IAAI,GAAG;AAAA,EACxB,WAAW,SAAS,QAAW;AAC7B,WAAO,GAAG,KAAK;AAAA,EACjB,OAAO;AACL,WAAO,GAAG,KAAK,IAAI,IAAI;AAAA,EACzB;AACF;AAQO,SAAS,QAAQ,QAAQ;AAC9B,MAAI,CAAC,OAAQ,QAAO,CAAC;AACrB,MAAI,OAAO,WAAW,EAAG,QAAO,OAAO,CAAC;AAExC,QAAM,SAAS,CAAC;AAChB,aAAW,SAAS,QAAQ;AAC1B,WAAO,QAAQ,KAAK;AAAA,EACtB;AACA,SAAO;AACT;;;AC1QO,SAAS,uBAAuB,QAAQ;AAC7C,MAAI,CAAC,OAAQ,QAAO,CAAC;AAErB,QAAM,UAAU,CAAC;AACjB,MAAI,UAAU,UAAU,MAAM,QAAQ,OAAO,IAAI,GAAG;AAClD,YAAQ,KAAK,GAAG,OAAO,KAAK,QAAQ,sBAAsB,CAAC;AAAA,EAC7D,WAAW,SAAS,UAAU,MAAM,QAAQ,OAAO,GAAG,GAAG;AACvD,YAAQ,KAAK,GAAG,OAAO,IAAI,QAAQ,sBAAsB,CAAC;AAAA,EAC5D,WAAW,UAAU,UAAU,MAAM,QAAQ,OAAO,IAAI,GAAG;AACzD,YAAQ,KAAK,GAAG,OAAO,KAAK,QAAQ,sBAAsB,CAAC;AAAA,EAC7D,OAAO;AACL,YAAQ,KAAK,GAAG,OAAO,KAAK,MAAM,CAAC;AAAA,EACrC;AACA,SAAO;AACT;AAUO,SAAS,YAAY,QAAQ,QAAQ,SAAS,MAAM;AACzD,MAAI,UAAU,UAAU,MAAM,QAAQ,OAAO,IAAI,GAAG;AAClD,WAAO,OAAO,KAAK,MAAM,cAAY,YAAY,QAAQ,UAAU,MAAM,CAAC;AAAA,EAC5E;AACA,MAAI,SAAS,UAAU,MAAM,QAAQ,OAAO,GAAG,GAAG;AAChD,WAAO,OAAO,IAAI,KAAK,cAAY,YAAY,QAAQ,UAAU,MAAM,CAAC;AAAA,EAC1E;AACA,MAAI,UAAU,UAAU,MAAM,QAAQ,OAAO,IAAI,GAAG;AAClD,WAAO,CAAC,OAAO,KAAK,KAAK,cAAY,YAAY,QAAQ,UAAU,MAAM,CAAC;AAAA,EAC5E;AAEA,SAAO,OAAO,QAAQ,MAAM,EAAE,MAAM,CAAC,CAAC,OAAO,SAAS,MAAM;AAC1D,UAAM,QAAQ,OAAO,KAAK;AAG1B,QAAI,OAAO,cAAc,YAAY,cAAc,QAAQ,MAAM,QAAQ,SAAS,GAAG;AACnF,aAAO,OAAO,OAAO,WAAW,MAAM;AAAA,IACxC;AAEA,WAAO,OAAO,QAAQ,aAAa,CAAC,CAAC,EAAE,MAAM,CAAC,CAAC,UAAU,MAAM,MAAM;AACnE,UAAI,aAAa,MAAO,QAAO,QAAQ;AACvC,UAAI,aAAa,OAAQ,QAAO,SAAS;AACzC,UAAI,aAAa,MAAO,QAAO,QAAQ;AACvC,UAAI,aAAa,OAAQ,QAAO,SAAS;AACzC,UAAI,aAAa,MAAO,QAAO,OAAO,OAAO,QAAQ,MAAM;AAC3D,UAAI,aAAa,MAAO,QAAO,CAAC,OAAO,OAAO,QAAQ,MAAM;AAC5D,UAAI,aAAa,MAAO,QAAO,MAAM,QAAQ,MAAM,KAAK,OAAO,SAAS,KAAK;AAC7E,UAAI,aAAa,OAAQ,QAAO,MAAM,QAAQ,MAAM,KAAK,CAAC,OAAO,SAAS,KAAK;AAC/E,UAAI,aAAa,OAAQ,QAAO,CAAC,YAAY,EAAE,CAAC,KAAK,GAAG,MAAM,GAAG,EAAE,CAAC,KAAK,GAAG,OAAO,GAAG,MAAM;AAC5F,aAAO;AAAA,IACT,CAAC;AAAA,EACH,CAAC;AACH;AAYO,SAAS,gBAAgB,EAAE,UAAU,iBAAiB,QAAQ,SAAS,KAAK,GAAG;AACpF,MAAI,CAAC,OAAQ,QAAO;AAGpB,MAAI,UAAU,UAAU,MAAM,QAAQ,OAAO,IAAI,GAAG;AAElD,WAAO,OAAO,KAAK,KAAK,eAAa,gBAAgB,EAAE,UAAU,iBAAiB,QAAQ,WAAW,OAAO,CAAC,CAAC;AAAA,EAChH;AACA,MAAI,SAAS,UAAU,MAAM,QAAQ,OAAO,GAAG,GAAG;AAEhD,WAAO,OAAO,IAAI,MAAM,eAAa,gBAAgB,EAAE,UAAU,iBAAiB,QAAQ,WAAW,OAAO,CAAC,CAAC;AAAA,EAChH;AACA,MAAI,UAAU,UAAU,MAAM,QAAQ,OAAO,IAAI,GAAG;AAGlD,WAAO;AAAA,EACT;AAGA,aAAW,CAAC,OAAO,SAAS,KAAK,OAAO,QAAQ,MAAM,GAAG;AAEvD,UAAM,cAAc,gBAAgB,QAAQ,KAAK;AACjD,QAAI,gBAAgB,GAAI;AAExB,UAAM,QAAQ,SAAS,QAAQ,WAAW,EAAE,WAAW;AACvD,QAAI,CAAC,MAAO;AAEZ,UAAM,EAAE,KAAK,KAAK,WAAW,UAAU,IAAI;AAC3C,UAAM,SAAS,cAAc,SAAY,YAAY;AACrD,UAAM,SAAS,cAAc,SAAY,YAAY;AAErD,QAAI,WAAW,UAAa,WAAW,OAAW;AAGlD,eAAW,CAAC,UAAU,MAAM,KAAK,OAAO,QAAQ,aAAa,CAAC,CAAC,GAAG;AAChE,UAAI,aAAa,SAAS,UAAU,OAAQ,QAAO;AACnD,UAAI,aAAa,UAAU,SAAS,OAAQ,QAAO;AACnD,UAAI,aAAa,SAAS,UAAU,OAAQ,QAAO;AACnD,UAAI,aAAa,UAAU,SAAS,OAAQ,QAAO;AACnD,UAAI,aAAa,UAAU,SAAS,UAAU,SAAS,QAAS,QAAO;AACvE,UAAI,aAAa,SAAS,OAAO,QAAQ,QAAQ,MAAM,KAAK,OAAO,QAAQ,QAAQ,MAAM,EAAG,QAAO;AACnG,UAAI,aAAa,SAAS,MAAM,QAAQ,MAAM,KAAK,OAAO,MAAM,OAAK,IAAI,UAAU,IAAI,MAAM,EAAG,QAAO;AACvG,UAAI,aAAa,UAAU,MAAM,QAAQ,MAAM,KAAK,OAAO,QAAQ,QAAQ,MAAM,KAAK,OAAO,SAAS,MAAM,EAAG,QAAO;AAAA,IACxH;AAAA,EACF;AAEA,SAAO;AACT;;;AC1HA,IAAM,WAAW,KAAK;AAYf,SAAS,YAAY,EAAE,UAAU,WAAW,GAAG,SAAS,UAAU,SAAS,QAAQ,eAAe,MAAM,iBAAiB,MAAM,GAAG;AACvI,MAAI,CAAC,SAAU,OAAM,IAAI,MAAM,+BAA+B;AAE9D,QAAM,SAAS,CAAC;AAEhB,QAAM,UAAU,CAAC;AAEjB,QAAM,UAAU,CAAC;AACjB,QAAM,kBAAkB,mBAAmB,cAAc,QAAQ,CAAC;AAGlE,MAAI,aAAa;AACjB,aAAW,YAAY,SAAS,YAAY;AAC1C,UAAM,YAAY,OAAO,SAAS,QAAQ;AAC1C,UAAM,WAAW,aAAa;AAE9B,QAAI,YAAY,KAAK,WAAW,YAAY,aAAa,UAAU,CAAC,gBAAgB,EAAE,UAAU,iBAAiB,QAAQ,QAAQ,aAAa,CAAC,GAAG;AAEhJ,YAAM,SAAS,CAAC;AAChB,UAAI,iBAAiB;AACrB,UAAI,eAAe;AAEnB,iBAAW,SAAS,SAAS,SAAS;AACpC,cAAM,OAAO,MAAM;AACnB,YAAI,MAAM,UAAW,OAAM,IAAI,MAAM,iCAAiC;AACtE,YAAI,CAAC,KAAM,OAAM,IAAI,MAAM,sCAAsC;AAEjE,YAAI,CAAC,WAAW,QAAQ,SAAS,KAAK,eAAe,CAAC,CAAC,GAAG;AAExD,gBAAM,eAAe,KAAK,0BAA0B,KAAK;AACzD,gBAAM,YAAY,OAAO,YAAY;AACrC,gBAAM,UAAU,OAAO,eAAe,KAAK,qBAAqB;AAEhE,cAAI,YAAY,eAAgB,kBAAiB;AACjD,cAAI,UAAU,aAAc,gBAAe;AAE3C,cAAI,kBAAkB,MAAM,uBAAuB,MAAM,qBAAqB;AAC5E,kBAAM,mBAAmB,OAAO,MAAM,mBAAmB;AACzD,mBAAO,KAAK;AAAA,cACV,gBAAgB;AAAA,cAChB,aAAa;AAAA,gBACX,WAAW;AAAA,gBACX,SAAS,mBAAmB,MAAM;AAAA,cACpC;AAAA,cACA,QAAQ,EAAE,WAAW,QAAQ;AAAA,YAC/B,CAAC;AAAA,UACH,OAAO;AACL,mBAAO,KAAK;AAAA,cACV,gBAAgB;AAAA,cAChB,OAAO,EAAE,WAAW,QAAQ;AAAA,YAC9B,CAAC;AAAA,UACH;AAAA,QAEF;AAAA,MACF;AACA,YAAM,cAAc,KAAK,IAAI,WAAW,YAAY,CAAC;AACrD,YAAM,YAAY,KAAK,IAAI,SAAS,YAAY,SAAS;AACzD,aAAO,KAAK,EAAE,QAAQ,UAAU,YAAY,WAAW,aAAa,UAAU,CAAC;AAI/E,UAAI;AACJ,iBAAW,SAAS,QAAQ;AAC1B,YAAI,iBAAiB,OAAO;AAC1B,kBAAQ,KAAK,MAAM,WAAW;AAAA,QAChC,OAAO;AACL,gBAAM,EAAE,MAAM,IAAI;AAClB,cAAI,SAAS;AACX,oBAAQ,KAAK,KAAK;AAAA,UACpB,WAAW,OAAO,MAAM,UAAU,IAAI,aAAa,UAAU;AAE3D,gBAAI,UAAU,MAAM;AAAA,UACtB,OAAO;AAEL,gBAAI,IAAK,SAAQ,KAAK,GAAG;AACzB,kBAAM,EAAE,GAAG,MAAM;AAAA,UACnB;AAAA,QACF;AAAA,MACF;AACA,UAAI,IAAK,SAAQ,KAAK,GAAG;AAAA,IAC3B;AAEA,iBAAa;AAAA,EACf;AACA,MAAI,CAAC,SAAS,MAAM,EAAG,UAAS;AAChC,UAAQ,KAAK,GAAG,OAAO;AAEvB,SAAO,EAAE,UAAU,UAAU,QAAQ,SAAS,SAAS,OAAO;AAChE;AASO,SAAS,oBAAoB,MAAM,EAAE,QAAQ,GAAG;AAErD,QAAM,WAAW,QAAQ,IAAI,CAAC,EAAE,WAAW,QAAQ,MAAM,KAAK,MAAM,WAAW,OAAO,CAAC;AACvF,SAAO;AAAA,IACL,YAAY,KAAK;AAAA,IACjB,MAAM,OAAO,MAAM,KAAK,YAAY;AAElC,YAAM,QAAQ,QAAQ,UAAU,CAAC,EAAE,WAAW,QAAQ,MAAM,aAAa,SAAS,OAAO,OAAO;AAChG,UAAI,QAAQ,GAAG;AAEb,eAAO,KAAK,MAAM,OAAO,GAAG;AAAA,MAC9B;AACA,UAAI,QAAQ,KAAK,EAAE,cAAc,SAAS,QAAQ,KAAK,EAAE,YAAY,KAAK;AAExE,cAAM,cAAc,QAAQ,QAAQ,KAAK,EAAE;AAC3C,cAAM,YAAY,MAAM,QAAQ,KAAK,EAAE;AACvC,YAAI,SAAS,KAAK,aAAa,SAAS;AACtC,iBAAO,SAAS,KAAK,EAAE,KAAK,YAAU,OAAO,MAAM,aAAa,SAAS,CAAC;AAAA,QAC5E,OAAO;AACL,iBAAO,SAAS,KAAK,EAAE,MAAM,aAAa,SAAS;AAAA,QACrD;AAAA,MACF,OAAO;AACL,eAAO,SAAS,KAAK;AAAA,MACvB;AAAA,IACF;AAAA,EACF;AACF;;;AC/HO,SAAS,cAAc,QAAQ,kBAAkB,kBAAkB,QAAQ,YAAY;AAC5F,QAAM,IAAI,kBAAkB,UAAU,iBAAiB;AACvD,MAAI,CAAC,EAAG,QAAO;AACf,QAAM,qBAAqB,sBAAsB,UAAU;AAC3D,QAAM,iBAAiB,WAAW,IAAI,CAAC,EAAE,QAAQ,MAAM,QAAQ,eAAe;AAC9E,MAAI,aAAa;AAGjB,QAAM,iBAAiB,CAAC,MAAM;AAC9B,MAAI,mBAAmB;AACvB,MAAI,eAAe;AACnB,MAAI,kBAAkB;AACtB,MAAI,kBAAkB;AAEtB,MAAI,iBAAiB,CAAC,GAAG;AAEvB,WAAO,eAAe,eAAe,SAAS,KAAK,kBAAkB,iBAAiB,CAAC,GAAG;AACxF;AACA,UAAI,eAAe,YAAY,MAAM,YAAY;AAE/C,2BAAmB,iBAAiB,GAAG,EAAE;AACzC,uBAAe,KAAK,gBAAgB;AACpC;AAAA,MACF;AACA,UAAI,eAAe,YAAY,MAAM,WAAY;AAAA,IACnD;AAAA,EACF;AAEA,WAAS,IAAI,GAAG,IAAI,GAAG,KAAK;AAE1B,UAAM,MAAM,kBAAkB,SAAS,iBAAiB,CAAC,IAAI;AAC7D,UAAM,MAAM,iBAAiB,CAAC;AAG9B,WAAO,iBAAiB,MAAM,mBAAmB,eAAe,YAAY,MAAM,aAAa;AAC7F,UAAI,eAAe,YAAY,MAAM,YAAY;AAC/C,uBAAe,IAAI;AACnB;AAAA,MACF;AACA,UAAI,eAAe,YAAY,MAAM,WAAY;AACjD;AAAA,IACF;AAEA,uBAAmB,eAAe,GAAG,EAAE;AAGvC,YACG,eAAe,eAAe,SAAS,KAAK,eAAe,eAAe,CAAC,MAAM,gBACjF,kBAAkB,OAAO,eAAe,eAAe,CAAC,MAAM,aAC/D;AACA;AACA,UAAI,eAAe,YAAY,MAAM,YAAY;AAE/C,cAAM,UAAU,CAAC;AACjB,yBAAiB,KAAK,OAAO;AAC7B,2BAAmB;AACnB,uBAAe,KAAK,OAAO;AAC3B;AAAA,MACF;AACA,UAAI,eAAe,YAAY,MAAM,WAAY;AAAA,IACnD;AAGA,QAAI,QAAQ,oBAAoB;AAE9B,uBAAiB,KAAK,OAAO,YAAY,CAAC;AAAA,IAC5C,WAAW,iBAAiB,eAAe,SAAS,GAAG;AACrD,uBAAiB,KAAK,IAAI;AAAA,IAC5B,OAAO;AACL,uBAAiB,KAAK,CAAC,CAAC;AAAA,IAC1B;AAAA,EACF;AAGA,MAAI,CAAC,OAAO,QAAQ;AAElB,aAAS,IAAI,GAAG,IAAI,oBAAoB,KAAK;AAE3C,YAAM,UAAU,CAAC;AACjB,uBAAiB,KAAK,OAAO;AAC7B,yBAAmB;AAAA,IACrB;AAAA,EACF;AAEA,SAAO;AACT;AAUO,SAAS,eAAe,eAAe,QAAQ,QAAQ,GAAG;AAC/D,QAAM,OAAO,OAAO,KAAK,KAAK,GAAG;AACjC,QAAM,WAAW,OAAO,QAAQ,oBAAoB;AACpD,QAAM,YAAY,WAAW,QAAQ,IAAI;AAEzC,MAAI,WAAW,MAAM,GAAG;AACtB,QAAI,UAAU,OAAO,SAAS,CAAC;AAC/B,QAAI,WAAW;AACf,QAAI,QAAQ,SAAS,WAAW,GAAG;AACjC,gBAAU,QAAQ,SAAS,CAAC;AAC5B;AAAA,IACF;AACA,mBAAe,eAAe,SAAS,QAAQ;AAE/C,UAAM,YAAY,QAAQ,KAAK,KAAK,GAAG;AACvC,UAAM,SAAS,cAAc,IAAI,SAAS;AAC1C,QAAI,CAAC,OAAQ,OAAM,IAAI,MAAM,oCAAoC;AACjE,QAAI,SAAU,gBAAe,QAAQ,KAAK;AAC1C,kBAAc,IAAI,MAAM,MAAM;AAC9B,kBAAc,OAAO,SAAS;AAC9B;AAAA,EACF;AAEA,MAAI,UAAU,MAAM,GAAG;AACrB,UAAM,UAAU,OAAO,SAAS,CAAC,EAAE,QAAQ;AAG3C,mBAAe,eAAe,OAAO,SAAS,CAAC,EAAE,SAAS,CAAC,GAAG,YAAY,CAAC;AAC3E,mBAAe,eAAe,OAAO,SAAS,CAAC,EAAE,SAAS,CAAC,GAAG,YAAY,CAAC;AAE3E,UAAM,OAAO,cAAc,IAAI,GAAG,IAAI,IAAI,OAAO,MAAM;AACvD,UAAM,SAAS,cAAc,IAAI,GAAG,IAAI,IAAI,OAAO,QAAQ;AAE3D,QAAI,CAAC,KAAM,OAAM,IAAI,MAAM,iCAAiC;AAC5D,QAAI,CAAC,OAAQ,OAAM,IAAI,MAAM,mCAAmC;AAChE,QAAI,KAAK,WAAW,OAAO,QAAQ;AACjC,YAAM,IAAI,MAAM,8CAA8C;AAAA,IAChE;AAEA,UAAM,MAAM,aAAa,MAAM,QAAQ,SAAS;AAChD,QAAI,SAAU,gBAAe,KAAK,KAAK;AAEvC,kBAAc,OAAO,GAAG,IAAI,IAAI,OAAO,MAAM;AAC7C,kBAAc,OAAO,GAAG,IAAI,IAAI,OAAO,QAAQ;AAC/C,kBAAc,IAAI,MAAM,GAAG;AAC3B;AAAA,EACF;AAGA,MAAI,OAAO,SAAS,QAAQ;AAE1B,UAAM,cAAc,OAAO,QAAQ,oBAAoB,aAAa,QAAQ,QAAQ;AAEpF,UAAM,SAAS,CAAC;AAChB,eAAW,SAAS,OAAO,UAAU;AACnC,qBAAe,eAAe,OAAO,WAAW;AAChD,YAAM,YAAY,cAAc,IAAI,MAAM,KAAK,KAAK,GAAG,CAAC;AACxD,UAAI,CAAC,UAAW,OAAM,IAAI,MAAM,mCAAmC;AACnE,aAAO,MAAM,QAAQ,IAAI,IAAI;AAAA,IAC/B;AAEA,eAAW,SAAS,OAAO,UAAU;AACnC,oBAAc,OAAO,MAAM,KAAK,KAAK,GAAG,CAAC;AAAA,IAC3C;AAEA,UAAM,WAAW,aAAa,QAAQ,WAAW;AACjD,QAAI,SAAU,gBAAe,UAAU,KAAK;AAC5C,kBAAc,IAAI,MAAM,QAAQ;AAAA,EAClC;AACF;AAOA,SAAS,eAAe,KAAK,OAAO;AAClC,WAAS,IAAI,GAAG,IAAI,IAAI,QAAQ,KAAK;AACnC,QAAI,OAAO;AACT,qBAAe,IAAI,CAAC,GAAG,QAAQ,CAAC;AAAA,IAClC,OAAO;AACL,UAAI,CAAC,IAAI,IAAI,CAAC,EAAE,CAAC;AAAA,IACnB;AAAA,EACF;AACF;AAQA,SAAS,aAAa,MAAM,QAAQ,OAAO;AACzC,QAAM,MAAM,CAAC;AACb,WAAS,IAAI,GAAG,IAAI,KAAK,QAAQ,KAAK;AACpC,QAAI,OAAO;AACT,UAAI,KAAK,aAAa,KAAK,CAAC,GAAG,OAAO,CAAC,GAAG,QAAQ,CAAC,CAAC;AAAA,IACtD,OAAO;AACL,UAAI,KAAK,CAAC,GAAG;AAEX,cAAM,MAAM,CAAC;AACb,iBAAS,IAAI,GAAG,IAAI,KAAK,CAAC,EAAE,QAAQ,KAAK;AACvC,gBAAM,QAAQ,OAAO,CAAC,EAAE,CAAC;AACzB,cAAI,KAAK,CAAC,EAAE,CAAC,CAAC,IAAI,UAAU,SAAY,OAAO;AAAA,QACjD;AACA,YAAI,KAAK,GAAG;AAAA,MACd,OAAO;AACL,YAAI,KAAK,MAAS;AAAA,MACpB;AAAA,IACF;AAAA,EACF;AACA,SAAO;AACT;AASA,SAAS,aAAa,QAAQ,OAAO;AACnC,QAAM,OAAO,OAAO,KAAK,MAAM;AAC/B,QAAM,SAAS,OAAO,KAAK,CAAC,CAAC,GAAG;AAChC,QAAM,MAAM,CAAC;AACb,WAAS,IAAI,GAAG,IAAI,QAAQ,KAAK;AAE/B,UAAM,MAAM,CAAC;AACb,eAAW,OAAO,MAAM;AACtB,UAAI,OAAO,GAAG,EAAE,WAAW,OAAQ,OAAM,IAAI,MAAM,8BAA8B;AACjF,UAAI,GAAG,IAAI,OAAO,GAAG,EAAE,CAAC;AAAA,IAC1B;AACA,QAAI,OAAO;AACT,UAAI,KAAK,aAAa,KAAK,QAAQ,CAAC,CAAC;AAAA,IACvC,OAAO;AACL,UAAI,KAAK,GAAG;AAAA,IACd;AAAA,EACF;AACA,SAAO;AACT;;;AC/OO,SAAS,kBAAkB,QAAQ,OAAO,QAAQ;AACvD,QAAM,QAAQ,kBAAkB;AAChC,QAAM,YAAY,WAAW,MAAM;AACnC,QAAM,oBAAoB,WAAW,MAAM;AAC3C,aAAW,MAAM;AACjB,MAAI,QAAQ,iBAAiB,MAAM;AACnC,MAAI,cAAc;AAClB,SAAO,aAAa,IAAI,QAAQ,OAAO,KAAK,IAAI;AAEhD,QAAM,qBAAqB,YAAY;AAEvC,SAAO,cAAc,OAAO;AAE1B,UAAM,WAAW,iBAAiB,MAAM;AACxC,UAAM,YAAY,IAAI,WAAW,iBAAiB;AAClD,aAAS,IAAI,GAAG,IAAI,mBAAmB,KAAK;AAC1C,gBAAU,CAAC,IAAI,OAAO,KAAK,SAAS,OAAO,QAAQ;AAAA,IACrD;AAEA,aAAS,IAAI,GAAG,IAAI,qBAAqB,cAAc,OAAO,KAAK;AAEjE,YAAMC,YAAW,OAAO,UAAU,CAAC,CAAC;AACpC,UAAIA,WAAU;AACZ,YAAI,aAAa;AACjB,YAAI,iBAAiB;AACrB,cAAM,QAAQ,MAAMA,aAAY;AAChC,eAAO,kBAAkB,cAAc,OAAO;AAC5C,cAAI,OAAO,OAAO,OAAO,KAAK,SAAS,OAAO,MAAM,CAAC,KAAK,aAAa;AACvE,wBAAcA;AACd,iBAAO,cAAc,GAAG;AACtB,0BAAc;AACd,mBAAO;AACP,gBAAI,YAAY;AACd,sBAAQ,OAAO,OAAO,KAAK,SAAS,OAAO,MAAM,CAAC,KAAKA,YAAW,aAAa;AAAA,YACjF;AAAA,UACF;AACA,gBAAM,QAAQ,WAAW;AACzB,mBAAS;AACT,iBAAO,aAAa,IAAI,QAAQ,OAAO,KAAK,IAAI;AAChD;AAAA,QACF;AACA,YAAI,gBAAgB;AAElB,iBAAO,UAAU,KAAK,MAAM,iBAAiB,OAAOA,SAAQ,IAAI,OAAO,UAAU,KAAK,CAAC;AAAA,QACzF;AAAA,MACF,OAAO;AACL,iBAAS,IAAI,GAAG,IAAI,sBAAsB,cAAc,OAAO,KAAK;AAClE,mBAAS;AACT,iBAAO,aAAa,IAAI,QAAQ,OAAO,KAAK,IAAI;AAAA,QAClD;AAAA,MACF;AAAA,IACF;AAAA,EACF;AACF;AAOO,SAAS,qBAAqB,QAAQ,OAAO,QAAQ;AAC1D,QAAM,UAAU,IAAI,WAAW,KAAK;AACpC,oBAAkB,QAAQ,OAAO,OAAO;AACxC,WAAS,IAAI,GAAG,IAAI,OAAO,KAAK;AAC9B,WAAO,CAAC,IAAI,IAAI,WAAW,OAAO,KAAK,QAAQ,OAAO,KAAK,aAAa,OAAO,QAAQ,QAAQ,CAAC,CAAC;AACjG,WAAO,UAAU,QAAQ,CAAC;AAAA,EAC5B;AACF;AAOO,SAAS,eAAe,QAAQ,OAAO,QAAQ;AACpD,QAAM,aAAa,IAAI,WAAW,KAAK;AACvC,oBAAkB,QAAQ,OAAO,UAAU;AAC3C,QAAM,aAAa,IAAI,WAAW,KAAK;AACvC,oBAAkB,QAAQ,OAAO,UAAU;AAE3C,WAAS,IAAI,GAAG,IAAI,OAAO,KAAK;AAC9B,UAAM,SAAS,IAAI,WAAW,OAAO,KAAK,QAAQ,OAAO,KAAK,aAAa,OAAO,QAAQ,WAAW,CAAC,CAAC;AACvG,QAAI,WAAW,CAAC,GAAG;AAEjB,aAAO,CAAC,IAAI,IAAI,WAAW,WAAW,CAAC,IAAI,WAAW,CAAC,CAAC;AACxD,aAAO,CAAC,EAAE,IAAI,OAAO,IAAI,CAAC,EAAE,SAAS,GAAG,WAAW,CAAC,CAAC,CAAC;AACtD,aAAO,CAAC,EAAE,IAAI,QAAQ,WAAW,CAAC,CAAC;AAAA,IACrC,OAAO;AACL,aAAO,CAAC,IAAI;AAAA,IACd;AACA,WAAO,UAAU,WAAW,CAAC;AAAA,EAC/B;AACF;;;AC5FO,SAAS,SAAS,OAAO;AAC9B,SAAO,KAAK,KAAK,MAAM,KAAK;AAC9B;AAYO,SAAS,uBAAuB,QAAQ,OAAO,QAAQ,QAAQ;AACpE,MAAI,WAAW,QAAW;AACxB,aAAS,OAAO,KAAK,UAAU,OAAO,QAAQ,IAAI;AAClD,WAAO,UAAU;AAAA,EACnB;AACA,QAAM,cAAc,OAAO;AAC3B,MAAI,OAAO;AACX,SAAO,OAAO,OAAO,QAAQ;AAC3B,UAAM,SAAS,WAAW,MAAM;AAChC,QAAI,SAAS,GAAG;AAEd,aAAO,cAAc,QAAQ,QAAQ,OAAO,QAAQ,IAAI;AAAA,IAC1D,OAAO;AAEL,YAAM,QAAQ,WAAW;AACzB,cAAQ,QAAQ,OAAO,OAAO,QAAQ,IAAI;AAC1C,cAAQ;AAAA,IACV;AAAA,EACF;AACA,SAAO,SAAS,cAAc;AAChC;AAWA,SAAS,QAAQ,QAAQ,OAAOC,WAAU,QAAQ,MAAM;AACtD,QAAM,QAAQA,YAAW,KAAK;AAC9B,MAAI,QAAQ;AACZ,WAAS,IAAI,GAAG,IAAI,OAAO,KAAK;AAC9B,aAAS,OAAO,KAAK,SAAS,OAAO,QAAQ,MAAM,KAAK;AAAA,EAC1D;AAIA,WAAS,IAAI,GAAG,IAAI,OAAO,KAAK;AAC9B,WAAO,OAAO,CAAC,IAAI;AAAA,EACrB;AACF;AAaA,SAAS,cAAc,QAAQ,QAAQA,WAAU,QAAQ,MAAM;AAC7D,MAAI,QAAQ,UAAU,KAAK;AAC3B,QAAM,QAAQ,KAAKA,aAAY;AAE/B,MAAI,OAAO;AACX,MAAI,OAAO,SAAS,OAAO,KAAK,YAAY;AAC1C,WAAO,OAAO,KAAK,SAAS,OAAO,QAAQ;AAAA,EAC7C,WAAW,MAAM;AAEf,UAAM,IAAI,MAAM,0BAA0B,OAAO,MAAM,eAAe;AAAA,EACxE;AACA,MAAI,OAAO;AACX,MAAI,QAAQ;AAGZ,SAAO,OAAO;AAEZ,QAAI,QAAQ,GAAG;AACb,eAAS;AACT,cAAQ;AACR,gBAAU;AAAA,IACZ,WAAW,OAAO,QAAQA,WAAU;AAElC,cAAQ,OAAO,KAAK,SAAS,OAAO,MAAM,KAAK;AAC/C,aAAO;AACP,cAAQ;AAAA,IACV,OAAO;AACL,UAAI,OAAO,OAAO,QAAQ;AAExB,eAAO,MAAM,IAAI,QAAQ,QAAQ;AAAA,MACnC;AACA;AACA,eAASA;AAAA,IACX;AAAA,EACF;AAEA,SAAO;AACT;AASO,SAAS,gBAAgB,QAAQ,OAAO,MAAM,YAAY;AAC/D,QAAM,QAAQ,UAAU,MAAM,UAAU;AACxC,QAAM,QAAQ,IAAI,WAAW,QAAQ,KAAK;AAC1C,WAAS,IAAI,GAAG,IAAI,OAAO,KAAK;AAC9B,aAAS,IAAI,GAAG,IAAI,OAAO,KAAK;AAC9B,YAAM,IAAI,QAAQ,CAAC,IAAI,OAAO,KAAK,SAAS,OAAO,QAAQ;AAAA,IAC7D;AAAA,EACF;AAEA,MAAI,SAAS,QAAS,QAAO,IAAI,aAAa,MAAM,MAAM;AAAA,WACjD,SAAS,SAAU,QAAO,IAAI,aAAa,MAAM,MAAM;AAAA,WACvD,SAAS,QAAS,QAAO,IAAI,WAAW,MAAM,MAAM;AAAA,WACpD,SAAS,QAAS,QAAO,IAAI,cAAc,MAAM,MAAM;AAAA,WACvD,SAAS,wBAAwB;AAExC,UAAM,QAAQ,IAAI,MAAM,KAAK;AAC7B,aAAS,IAAI,GAAG,IAAI,OAAO,KAAK;AAC9B,YAAM,CAAC,IAAI,MAAM,SAAS,IAAI,QAAQ,IAAI,KAAK,KAAK;AAAA,IACtD;AACA,WAAO;AAAA,EACT;AACA,QAAM,IAAI,MAAM,+CAA+C,IAAI,EAAE;AACvE;AAQA,SAAS,UAAU,MAAM,YAAY;AACnC,UAAQ,MAAM;AAAA,IACd,KAAK;AAAA,IACL,KAAK;AACH,aAAO;AAAA,IACT,KAAK;AAAA,IACL,KAAK;AACH,aAAO;AAAA,IACT,KAAK;AACH,UAAI,CAAC,WAAY,OAAM,IAAI,MAAM,uCAAuC;AACxE,aAAO;AAAA,IACT;AACE,YAAM,IAAI,MAAM,6BAA6B,IAAI,EAAE;AAAA,EACrD;AACF;;;AC/JO,SAAS,UAAU,QAAQ,MAAM,OAAO,aAAa;AAC1D,MAAI,UAAU,EAAG,QAAO,CAAC;AACzB,MAAI,SAAS,WAAW;AACtB,WAAO,iBAAiB,QAAQ,KAAK;AAAA,EACvC,WAAW,SAAS,SAAS;AAC3B,WAAO,eAAe,QAAQ,KAAK;AAAA,EACrC,WAAW,SAAS,SAAS;AAC3B,WAAO,eAAe,QAAQ,KAAK;AAAA,EACrC,WAAW,SAAS,SAAS;AAC3B,WAAO,eAAe,QAAQ,KAAK;AAAA,EACrC,WAAW,SAAS,SAAS;AAC3B,WAAO,eAAe,QAAQ,KAAK;AAAA,EACrC,WAAW,SAAS,UAAU;AAC5B,WAAO,gBAAgB,QAAQ,KAAK;AAAA,EACtC,WAAW,SAAS,cAAc;AAChC,WAAO,mBAAmB,QAAQ,KAAK;AAAA,EACzC,WAAW,SAAS,wBAAwB;AAC1C,QAAI,CAAC,YAAa,OAAM,IAAI,MAAM,8BAA8B;AAChE,WAAO,wBAAwB,QAAQ,OAAO,WAAW;AAAA,EAC3D,OAAO;AACL,UAAM,IAAI,MAAM,2BAA2B,IAAI,EAAE;AAAA,EACnD;AACF;AASA,SAAS,iBAAiB,QAAQ,OAAO;AACvC,QAAM,SAAS,IAAI,MAAM,KAAK;AAC9B,WAAS,IAAI,GAAG,IAAI,OAAO,KAAK;AAC9B,UAAM,aAAa,OAAO,UAAU,IAAI,IAAI;AAC5C,UAAM,YAAY,IAAI;AACtB,UAAM,OAAO,OAAO,KAAK,SAAS,UAAU;AAC5C,WAAO,CAAC,KAAK,OAAO,KAAK,eAAe;AAAA,EAC1C;AACA,SAAO,UAAU,KAAK,KAAK,QAAQ,CAAC;AACpC,SAAO;AACT;AASA,SAAS,eAAe,QAAQ,OAAO;AACrC,QAAM,UAAU,OAAO,KAAK,aAAa,OAAO,UAAU,IACtD,IAAI,WAAW,MAAM,OAAO,KAAK,QAAQ,OAAO,KAAK,aAAa,OAAO,QAAQ,QAAQ,CAAC,CAAC,IAC3F,IAAI,WAAW,OAAO,KAAK,QAAQ,OAAO,KAAK,aAAa,OAAO,QAAQ,KAAK;AACpF,SAAO,UAAU,QAAQ;AACzB,SAAO;AACT;AASA,SAAS,eAAe,QAAQ,OAAO;AACrC,QAAM,UAAU,OAAO,KAAK,aAAa,OAAO,UAAU,IACtD,IAAI,cAAc,MAAM,OAAO,KAAK,QAAQ,OAAO,KAAK,aAAa,OAAO,QAAQ,QAAQ,CAAC,CAAC,IAC9F,IAAI,cAAc,OAAO,KAAK,QAAQ,OAAO,KAAK,aAAa,OAAO,QAAQ,KAAK;AACvF,SAAO,UAAU,QAAQ;AACzB,SAAO;AACT;AASA,SAAS,eAAe,QAAQ,OAAO;AACrC,QAAM,SAAS,IAAI,MAAM,KAAK;AAC9B,WAAS,IAAI,GAAG,IAAI,OAAO,KAAK;AAC9B,UAAM,MAAM,OAAO,KAAK,YAAY,OAAO,SAAS,IAAI,IAAI,IAAI;AAChE,UAAM,OAAO,OAAO,KAAK,SAAS,OAAO,SAAS,IAAI,KAAK,GAAG,IAAI;AAClE,WAAO,CAAC,IAAI,OAAO,IAAI,KAAK,MAAM;AAAA,EACpC;AACA,SAAO,UAAU,QAAQ;AACzB,SAAO;AACT;AASA,SAAS,eAAe,QAAQ,OAAO;AACrC,QAAM,UAAU,OAAO,KAAK,aAAa,OAAO,UAAU,IACtD,IAAI,aAAa,MAAM,OAAO,KAAK,QAAQ,OAAO,KAAK,aAAa,OAAO,QAAQ,QAAQ,CAAC,CAAC,IAC7F,IAAI,aAAa,OAAO,KAAK,QAAQ,OAAO,KAAK,aAAa,OAAO,QAAQ,KAAK;AACtF,SAAO,UAAU,QAAQ;AACzB,SAAO;AACT;AASA,SAAS,gBAAgB,QAAQ,OAAO;AACtC,QAAM,UAAU,OAAO,KAAK,aAAa,OAAO,UAAU,IACtD,IAAI,aAAa,MAAM,OAAO,KAAK,QAAQ,OAAO,KAAK,aAAa,OAAO,QAAQ,QAAQ,CAAC,CAAC,IAC7F,IAAI,aAAa,OAAO,KAAK,QAAQ,OAAO,KAAK,aAAa,OAAO,QAAQ,KAAK;AACtF,SAAO,UAAU,QAAQ;AACzB,SAAO;AACT;AASA,SAAS,mBAAmB,QAAQ,OAAO;AACzC,QAAM,SAAS,IAAI,MAAM,KAAK;AAC9B,WAAS,IAAI,GAAG,IAAI,OAAO,KAAK;AAC9B,UAAM,SAAS,OAAO,KAAK,UAAU,OAAO,QAAQ,IAAI;AACxD,WAAO,UAAU;AACjB,WAAO,CAAC,IAAI,IAAI,WAAW,OAAO,KAAK,QAAQ,OAAO,KAAK,aAAa,OAAO,QAAQ,MAAM;AAC7F,WAAO,UAAU;AAAA,EACnB;AACA,SAAO;AACT;AAUA,SAAS,wBAAwB,QAAQ,OAAO,aAAa;AAE3D,QAAM,SAAS,IAAI,MAAM,KAAK;AAC9B,WAAS,IAAI,GAAG,IAAI,OAAO,KAAK;AAC9B,WAAO,CAAC,IAAI,IAAI,WAAW,OAAO,KAAK,QAAQ,OAAO,KAAK,aAAa,OAAO,QAAQ,WAAW;AAClG,WAAO,UAAU;AAAA,EACnB;AACA,SAAO;AACT;AAWA,SAAS,MAAM,QAAQ,QAAQ,MAAM;AACnC,QAAM,UAAU,IAAI,YAAY,IAAI;AACpC,MAAI,WAAW,OAAO,EAAE,IAAI,IAAI,WAAW,QAAQ,QAAQ,IAAI,CAAC;AAChE,SAAO;AACT;;;AC7KA,IAAM,YAAY,CAAC,GAAG,KAAM,OAAQ,UAAU,UAAU;AAWxD,SAAS,UAAU,WAAW,SAAS,SAAS,OAAO,QAAQ;AAC7D,WAAS,IAAI,GAAG,IAAI,QAAQ,KAAK;AAC/B,YAAQ,QAAQ,CAAC,IAAI,UAAU,UAAU,CAAC;AAAA,EAC5C;AACF;AASO,SAAS,iBAAiB,OAAO,QAAQ;AAC9C,QAAM,cAAc,MAAM;AAC1B,QAAM,eAAe,OAAO;AAC5B,MAAI,MAAM;AACV,MAAI,SAAS;AAGb,SAAO,MAAM,aAAa;AACxB,UAAM,IAAI,MAAM,GAAG;AACnB;AACA,QAAI,IAAI,KAAK;AACX;AAAA,IACF;AAAA,EACF;AACA,MAAI,gBAAgB,OAAO,aAAa;AACtC,UAAM,IAAI,MAAM,8BAA8B;AAAA,EAChD;AAEA,SAAO,MAAM,aAAa;AACxB,UAAM,IAAI,MAAM,GAAG;AACnB,QAAI,MAAM;AACV;AAEA,QAAI,OAAO,aAAa;AACtB,YAAM,IAAI,MAAM,oBAAoB;AAAA,IACtC;AAGA,SAAK,IAAI,OAAS,GAAG;AAEnB,UAAIC,QAAO,MAAM,KAAK;AAEtB,UAAIA,OAAM,IAAI;AACZ,YAAI,MAAM,KAAK,aAAa;AAC1B,gBAAM,IAAI,MAAM,6CAA6C;AAAA,QAC/D;AACA,cAAM,aAAaA,OAAM;AACzB,QAAAA,OAAM,MAAM,GAAG,KACV,MAAM,MAAM,CAAC,KAAK,MAClB,MAAM,MAAM,CAAC,KAAK,OAClB,MAAM,MAAM,CAAC,KAAK;AACvB,QAAAA,QAAOA,OAAM,UAAU,UAAU,KAAK;AACtC,eAAO;AAAA,MACT;AACA,UAAI,MAAMA,OAAM,aAAa;AAC3B,cAAM,IAAI,MAAM,2CAA2C;AAAA,MAC7D;AACA,gBAAU,OAAO,KAAK,QAAQ,QAAQA,IAAG;AACzC,aAAOA;AACP,gBAAUA;AAAA,IACZ,OAAO;AAEL,UAAI,SAAS;AACb,cAAQ,IAAI,GAAK;AAAA,QACjB,KAAK;AAEH,iBAAO,MAAM,IAAI,KAAO;AACxB,mBAAS,MAAM,GAAG,KAAK,MAAM,KAAK;AAClC;AACA;AAAA,QACF,KAAK;AAEH,cAAI,eAAe,MAAM,GAAG;AAC1B,kBAAM,IAAI,MAAM,2BAA2B;AAAA,UAC7C;AACA,iBAAO,MAAM,KAAK;AAClB,mBAAS,MAAM,GAAG,KAAK,MAAM,MAAM,CAAC,KAAK;AACzC,iBAAO;AACP;AAAA,QACF,KAAK;AAEH,cAAI,eAAe,MAAM,GAAG;AAC1B,kBAAM,IAAI,MAAM,2BAA2B;AAAA,UAC7C;AACA,iBAAO,MAAM,KAAK;AAClB,mBAAS,MAAM,GAAG,KACb,MAAM,MAAM,CAAC,KAAK,MAClB,MAAM,MAAM,CAAC,KAAK,OAClB,MAAM,MAAM,CAAC,KAAK;AACvB,iBAAO;AACP;AAAA,QACF;AACE;AAAA,MACF;AACA,UAAI,WAAW,KAAK,MAAM,MAAM,GAAG;AACjC,cAAM,IAAI,MAAM,kBAAkB,MAAM,QAAQ,GAAG,gBAAgB,WAAW,EAAE;AAAA,MAClF;AACA,UAAI,SAAS,QAAQ;AACnB,cAAM,IAAI,MAAM,yCAAyC;AAAA,MAC3D;AACA,gBAAU,QAAQ,SAAS,QAAQ,QAAQ,QAAQ,GAAG;AACtD,gBAAU;AAAA,IACZ;AAAA,EACF;AAEA,MAAI,WAAW,aAAc,OAAM,IAAI,MAAM,wBAAwB;AACvE;;;AChHO,SAAS,aAAa,OAAO,MAAM,EAAE,MAAM,SAAS,WAAW,GAAG;AACvE,QAAM,OAAO,IAAI,SAAS,MAAM,QAAQ,MAAM,YAAY,MAAM,UAAU;AAC1E,QAAM,SAAS,EAAE,MAAM,QAAQ,EAAE;AAEjC,MAAI;AAGJ,QAAM,mBAAmB,qBAAqB,QAAQ,MAAM,UAAU;AAEtE,QAAM,EAAE,kBAAkB,SAAS,IAAI,qBAAqB,QAAQ,MAAM,UAAU;AAIpF,QAAM,UAAU,KAAK,aAAa;AAClC,MAAI,KAAK,aAAa,SAAS;AAC7B,eAAW,UAAU,QAAQ,MAAM,SAAS,QAAQ,WAAW;AAAA,EACjE,WACE,KAAK,aAAa,sBAClB,KAAK,aAAa,oBAClB,KAAK,aAAa,OAClB;AACA,UAAMC,YAAW,SAAS,YAAY,IAAI,KAAK,SAAS,OAAO,QAAQ;AACvE,QAAIA,WAAU;AACZ,iBAAW,IAAI,MAAM,OAAO;AAC5B,UAAI,SAAS,WAAW;AACtB,+BAAuB,QAAQA,WAAU,QAAQ;AACjD,mBAAW,SAAS,IAAI,OAAK,CAAC,CAAC,CAAC;AAAA,MAClC,OAAO;AAEL,+BAAuB,QAAQA,WAAU,UAAU,KAAK,aAAa,OAAO,MAAM;AAAA,MACpF;AAAA,IACF,OAAO;AACL,iBAAW,IAAI,WAAW,OAAO;AAAA,IACnC;AAAA,EACF,WAAW,KAAK,aAAa,qBAAqB;AAChD,eAAW,gBAAgB,QAAQ,SAAS,MAAM,QAAQ,WAAW;AAAA,EACvE,WAAW,KAAK,aAAa,uBAAuB;AAClD,UAAM,QAAQ,SAAS;AACvB,eAAW,QAAQ,IAAI,WAAW,OAAO,IAAI,IAAI,cAAc,OAAO;AACtE,sBAAkB,QAAQ,SAAS,QAAQ;AAAA,EAC7C,WAAW,KAAK,aAAa,2BAA2B;AACtD,eAAW,IAAI,MAAM,OAAO;AAC5B,yBAAqB,QAAQ,SAAS,QAAQ;AAAA,EAChD,OAAO;AACL,UAAM,IAAI,MAAM,iCAAiC,KAAK,QAAQ,EAAE;AAAA,EAClE;AAEA,SAAO,EAAE,kBAAkB,kBAAkB,SAAS;AACxD;AASA,SAAS,qBAAqB,QAAQ,MAAM,YAAY;AACtD,MAAI,WAAW,SAAS,GAAG;AACzB,UAAM,qBAAqB,sBAAsB,UAAU;AAC3D,QAAI,oBAAoB;AACtB,YAAM,SAAS,IAAI,MAAM,KAAK,UAAU;AACxC,6BAAuB,QAAQ,SAAS,kBAAkB,GAAG,MAAM;AACnE,aAAO;AAAA,IACT;AAAA,EACF;AACA,SAAO,CAAC;AACV;AAQA,SAAS,qBAAqB,QAAQ,MAAM,YAAY;AACtD,QAAM,qBAAqB,sBAAsB,UAAU;AAC3D,MAAI,CAAC,mBAAoB,QAAO,EAAE,kBAAkB,CAAC,GAAG,UAAU,EAAE;AAEpE,QAAM,mBAAmB,IAAI,MAAM,KAAK,UAAU;AAClD,yBAAuB,QAAQ,SAAS,kBAAkB,GAAG,gBAAgB;AAG7E,MAAI,WAAW,KAAK;AACpB,aAAW,OAAO,kBAAkB;AAClC,QAAI,QAAQ,mBAAoB;AAAA,EAClC;AACA,MAAI,aAAa,EAAG,kBAAiB,SAAS;AAE9C,SAAO,EAAE,kBAAkB,SAAS;AACtC;AASO,SAAS,eAAe,iBAAiB,wBAAwB,OAAO,aAAa;AAE1F,MAAI;AACJ,QAAM,qBAAqB,cAAc,KAAK;AAC9C,MAAI,UAAU,gBAAgB;AAC5B,WAAO;AAAA,EACT,WAAW,oBAAoB;AAC7B,WAAO,mBAAmB,iBAAiB,sBAAsB;AAAA,EACnE,WAAW,UAAU,UAAU;AAC7B,WAAO,IAAI,WAAW,sBAAsB;AAC5C,qBAAiB,iBAAiB,IAAI;AAAA,EACxC,OAAO;AACL,UAAM,IAAI,MAAM,0CAA0C,KAAK,EAAE;AAAA,EACnE;AACA,MAAI,MAAM,WAAW,wBAAwB;AAC3C,UAAM,IAAI,MAAM,oCAAoC,MAAM,MAAM,0BAA0B,sBAAsB,EAAE;AAAA,EACpH;AACA,SAAO;AACT;AAWO,SAAS,eAAe,iBAAiB,IAAI,eAAe;AACjE,QAAM,OAAO,IAAI,SAAS,gBAAgB,QAAQ,gBAAgB,YAAY,gBAAgB,UAAU;AACxG,QAAM,SAAS,EAAE,MAAM,QAAQ,EAAE;AACjC,QAAM,EAAE,MAAM,SAAS,YAAY,OAAO,YAAY,IAAI;AAC1D,QAAM,QAAQ,GAAG;AACjB,MAAI,CAAC,MAAO,OAAM,IAAI,MAAM,0CAA0C;AAGtE,QAAM,mBAAmB,uBAAuB,QAAQ,OAAO,UAAU;AACzE,SAAO,SAAS,MAAM;AAGtB,QAAM,mBAAmB,uBAAuB,QAAQ,OAAO,UAAU;AAGzE,QAAM,uBAAuB,GAAG,yBAAyB,MAAM,gCAAgC,MAAM;AAErG,MAAI,OAAO,gBAAgB,SAAS,OAAO,MAAM;AACjD,MAAI,MAAM,kBAAkB,OAAO;AACjC,WAAO,eAAe,MAAM,sBAAsB,OAAO,WAAW;AAAA,EACtE;AACA,QAAM,WAAW,IAAI,SAAS,KAAK,QAAQ,KAAK,YAAY,KAAK,UAAU;AAC3E,QAAM,aAAa,EAAE,MAAM,UAAU,QAAQ,EAAE;AAI/C,MAAI;AACJ,QAAM,UAAU,MAAM,aAAa,MAAM;AACzC,MAAI,MAAM,aAAa,SAAS;AAC9B,eAAW,UAAU,YAAY,MAAM,SAAS,QAAQ,WAAW;AAAA,EACrE,WAAW,MAAM,aAAa,OAAO;AAEnC,eAAW,IAAI,MAAM,OAAO;AAC5B,2BAAuB,YAAY,GAAG,QAAQ;AAC9C,eAAW,SAAS,IAAI,OAAK,CAAC,CAAC,CAAC;AAAA,EAClC,WACE,MAAM,aAAa,sBACnB,MAAM,aAAa,kBACnB;AACA,UAAMA,YAAW,SAAS,SAAS,WAAW,QAAQ;AACtD,eAAW,IAAI,MAAM,OAAO;AAC5B,2BAAuB,YAAYA,WAAU,UAAU,uBAAuB,CAAC;AAAA,EACjF,WAAW,MAAM,aAAa,uBAAuB;AACnD,UAAM,QAAQ,SAAS;AACvB,eAAW,QAAQ,IAAI,WAAW,OAAO,IAAI,IAAI,cAAc,OAAO;AACtE,sBAAkB,YAAY,SAAS,QAAQ;AAAA,EACjD,WAAW,MAAM,aAAa,2BAA2B;AACvD,eAAW,IAAI,MAAM,OAAO;AAC5B,yBAAqB,YAAY,SAAS,QAAQ;AAAA,EACpD,WAAW,MAAM,aAAa,oBAAoB;AAChD,eAAW,IAAI,MAAM,OAAO;AAC5B,mBAAe,YAAY,SAAS,QAAQ;AAAA,EAC9C,WAAW,MAAM,aAAa,qBAAqB;AACjD,eAAW,gBAAgB,YAAY,SAAS,MAAM,QAAQ,WAAW;AAAA,EAC3E,OAAO;AACL,UAAM,IAAI,MAAM,iCAAiC,MAAM,QAAQ,EAAE;AAAA,EACnE;AAEA,SAAO,EAAE,kBAAkB,kBAAkB,SAAS;AACxD;AAQA,SAAS,uBAAuB,QAAQ,OAAO,YAAY;AACzD,QAAM,qBAAqB,sBAAsB,UAAU;AAC3D,MAAI,CAAC,mBAAoB,QAAO,CAAC;AAEjC,QAAM,SAAS,IAAI,MAAM,MAAM,UAAU;AACzC,yBAAuB,QAAQ,SAAS,kBAAkB,GAAG,QAAQ,MAAM,6BAA6B;AACxG,SAAO;AACT;AAQA,SAAS,uBAAuB,QAAQ,OAAO,YAAY;AACzD,QAAM,qBAAqB,sBAAsB,UAAU;AAC3D,MAAI,oBAAoB;AAEtB,UAAM,SAAS,IAAI,MAAM,MAAM,UAAU;AACzC,2BAAuB,QAAQ,SAAS,kBAAkB,GAAG,QAAQ,MAAM,6BAA6B;AACxG,WAAO;AAAA,EACT;AACF;;;ACvNO,SAAS,WAAW,QAAQ,EAAE,YAAY,aAAa,UAAU,GAAG,eAAe,QAAQ;AAChG,QAAM,EAAE,cAAc,WAAW,IAAI;AACrC,QAAM,SAAS,aAAa,UAAU;AAEtC,QAAM,SAAS,CAAC;AAEhB,MAAI,aAAa;AAEjB,MAAI,YAAY;AAChB,MAAI,WAAW;AAEf,QAAM,gBAAgB,WAAW,MAAM;AACrC,iBAAa,OAAO;AAAA,MAClB;AAAA,MACA,YAAY;AAAA,MACZ,UAAU,aAAa,WAAW,UAAU;AAAA,MAC5C,QAAQ,aAAa;AAAA,IACvB,CAAC;AAAA,EACH;AAEA,SAAO,SAAS,WAAW,YAAY,OAAO,SAAS,OAAO,KAAK,aAAa,GAAG;AACjF,QAAI,OAAO,UAAU,OAAO,KAAK,aAAa,EAAG;AAGjD,UAAM,SAAS,cAAc,MAAM;AACnC,QAAI,OAAO,SAAS,mBAAmB;AAErC,mBAAa,SAAS,QAAQ,QAAQ,eAAe,YAAY,QAAW,CAAC;AAC7E,mBAAa,QAAQ,YAAY,aAAa;AAAA,IAChD,OAAO;AACL,YAAM,kBAAkB,WAAW,UAAU;AAC7C,YAAM,SAAS,SAAS,QAAQ,QAAQ,eAAe,YAAY,WAAW,cAAc,QAAQ;AACpG,UAAI,cAAc,QAAQ;AAExB,oBAAY,OAAO,SAAS;AAAA,MAC9B,OAAO;AAEL,wBAAgB;AAChB,eAAO,KAAK,MAAM;AAClB,oBAAY,OAAO;AACnB,oBAAY;AAAA,MACd;AAAA,IACF;AAAA,EACF;AACA,kBAAgB;AAEhB,SAAO;AACT;AAaO,SAAS,SAAS,QAAQ,QAAQ,eAAe,YAAY,eAAe,WAAW;AAC5F,QAAM,EAAE,MAAM,SAAS,YAAY,OAAO,YAAY,IAAI;AAE1D,QAAM,kBAAkB,IAAI;AAAA,IAC1B,OAAO,KAAK;AAAA,IAAQ,OAAO,KAAK,aAAa,OAAO;AAAA,IAAQ,OAAO;AAAA,EACrE;AACA,SAAO,UAAU,OAAO;AAGxB,MAAI,OAAO,SAAS,aAAa;AAC/B,UAAM,OAAO,OAAO;AACpB,QAAI,CAAC,KAAM,OAAM,IAAI,MAAM,uCAAuC;AAGlE,QAAI,YAAY,KAAK,cAAc,aAAa,UAAU,GAAG;AAC3D,aAAO,IAAI,MAAM,KAAK,UAAU;AAAA,IAClC;AAEA,UAAM,OAAO,eAAe,iBAAiB,OAAO,OAAO,sBAAsB,GAAG,OAAO,WAAW;AACtG,UAAM,EAAE,kBAAkB,kBAAkB,SAAS,IAAI,aAAa,MAAM,MAAM,aAAa;AAI/F,QAAI,SAAS,sBAAsB,UAAU,YAAY,KAAK,UAAU,aAAa;AACrF,QAAI,iBAAiB,UAAU,kBAAkB,QAAQ;AACvD,YAAM,SAAS,MAAM,QAAQ,aAAa,IAAI,gBAAgB,CAAC;AAC/D,aAAO,cAAc,QAAQ,kBAAkB,kBAAkB,QAAQ,UAAU;AAAA,IACrF,OAAO;AAEL,eAAS,IAAI,GAAG,IAAI,WAAW,QAAQ,KAAK;AAC1C,YAAI,WAAW,CAAC,EAAE,QAAQ,oBAAoB,YAAY;AACxD,mBAAS,MAAM,KAAK,QAAQ,OAAK,CAAC,CAAC,CAAC;AAAA,QACtC;AAAA,MACF;AACA,aAAO;AAAA,IACT;AAAA,EACF,WAAW,OAAO,SAAS,gBAAgB;AACzC,UAAM,QAAQ,OAAO;AACrB,QAAI,CAAC,MAAO,OAAM,IAAI,MAAM,0CAA0C;AAGtE,QAAI,YAAY,MAAM,UAAU;AAC9B,aAAO,IAAI,MAAM,MAAM,UAAU;AAAA,IACnC;AAEA,UAAM,EAAE,kBAAkB,kBAAkB,SAAS,IACnD,eAAe,iBAAiB,QAAQ,aAAa;AAGvD,UAAM,SAAS,sBAAsB,UAAU,YAAY,MAAM,UAAU,aAAa;AACxF,UAAM,SAAS,MAAM,QAAQ,aAAa,IAAI,gBAAgB,CAAC;AAC/D,WAAO,cAAc,QAAQ,kBAAkB,kBAAkB,QAAQ,UAAU;AAAA,EACrF,WAAW,OAAO,SAAS,mBAAmB;AAC5C,UAAM,OAAO,OAAO;AACpB,QAAI,CAAC,KAAM,OAAM,IAAI,MAAM,6CAA6C;AAExE,UAAM,OAAO;AAAA,MACX;AAAA,MAAiB,OAAO,OAAO,sBAAsB;AAAA,MAAG;AAAA,MAAO;AAAA,IACjE;AAEA,UAAMC,UAAS,EAAE,MAAM,IAAI,SAAS,KAAK,QAAQ,KAAK,YAAY,KAAK,UAAU,GAAG,QAAQ,EAAE;AAC9F,WAAO,UAAUA,SAAQ,MAAM,KAAK,YAAY,QAAQ,WAAW;AAAA,EACrE,OAAO;AACL,UAAM,IAAI,MAAM,kCAAkC,OAAO,IAAI,EAAE;AAAA,EACjE;AACF;AASA,SAAS,cAAc,QAAQ;AAC7B,QAAM,SAAS,4BAA4B,MAAM;AAGjD,QAAM,OAAO,UAAU,OAAO,OAAO;AACrC,QAAM,yBAAyB,OAAO;AACtC,QAAM,uBAAuB,OAAO;AACpC,QAAM,MAAM,OAAO;AACnB,QAAM,mBAAmB,OAAO,WAAW;AAAA,IACzC,YAAY,OAAO,QAAQ;AAAA,IAC3B,UAAU,UAAU,OAAO,QAAQ,OAAO;AAAA,IAC1C,2BAA2B,UAAU,OAAO,QAAQ,OAAO;AAAA,IAC3D,2BAA2B,UAAU,OAAO,QAAQ,OAAO;AAAA,IAC3D,YAAY,OAAO,QAAQ,WAAW;AAAA,MACpC,KAAK,OAAO,QAAQ,QAAQ;AAAA,MAC5B,KAAK,OAAO,QAAQ,QAAQ;AAAA,MAC5B,YAAY,OAAO,QAAQ,QAAQ;AAAA,MACnC,gBAAgB,OAAO,QAAQ,QAAQ;AAAA,MACvC,WAAW,OAAO,QAAQ,QAAQ;AAAA,MAClC,WAAW,OAAO,QAAQ,QAAQ;AAAA,IACpC;AAAA,EACF;AACA,QAAM,oBAAoB,OAAO;AACjC,QAAM,yBAAyB,OAAO,WAAW;AAAA,IAC/C,YAAY,OAAO,QAAQ;AAAA,IAC3B,UAAU,UAAU,OAAO,QAAQ,OAAO;AAAA,IAC1C,WAAW,OAAO,QAAQ;AAAA,EAC5B;AACA,QAAM,sBAAsB,OAAO,WAAW;AAAA,IAC5C,YAAY,OAAO,QAAQ;AAAA,IAC3B,WAAW,OAAO,QAAQ;AAAA,IAC1B,UAAU,OAAO,QAAQ;AAAA,IACzB,UAAU,UAAU,OAAO,QAAQ,OAAO;AAAA,IAC1C,+BAA+B,OAAO,QAAQ;AAAA,IAC9C,+BAA+B,OAAO,QAAQ;AAAA,IAC9C,eAAe,OAAO,QAAQ,YAAY,SAAY,OAAO,OAAO,QAAQ;AAAA;AAAA,IAC5E,YAAY,OAAO,QAAQ;AAAA,EAC7B;AAEA,SAAO;AAAA,IACL;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACF;AACF;;;ACtLO,SAAS,aAAa,SAAS,EAAE,SAAS,GAAG,WAAW;AAC7D,QAAM,EAAE,MAAM,aAAa,KAAK,IAAI;AAGpC,QAAM,eAAe,CAAC;AAEtB,QAAM,UAAU,EAAE,GAAG,iBAAiB,GAAG,QAAQ,QAAQ;AAGzD,aAAW,aAAa,UAAU,QAAQ;AACxC,UAAM,EAAE,eAAe,IAAI;AAC3B,UAAM,aAAa,cAAc,SAAS,QAAQ,eAAe,cAAc;AAC/E,UAAM,gBAAgB;AAAA,MACpB,cAAc,eAAe;AAAA,MAC7B,MAAM,eAAe;AAAA,MACrB,SAAS,WAAW,WAAW,SAAS,CAAC,EAAE;AAAA,MAC3C;AAAA,MACA,OAAO,eAAe;AAAA,MACtB;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAGA,QAAI,EAAE,iBAAiB,YAAY;AACjC,mBAAa,KAAK;AAAA,QAChB,cAAc,eAAe;AAAA,QAC7B,MAAM,QAAQ,QAAQ,KAAK,MAAM,UAAU,MAAM,WAAW,UAAU,MAAM,OAAO,CAAC,EACjF,KAAK,YAAU;AACd,gBAAM,SAAS,EAAE,MAAM,IAAI,SAAS,MAAM,GAAG,QAAQ,EAAE;AACvD,iBAAO;AAAA,YACL,UAAU;AAAA,YACV,MAAM,WAAW,QAAQ,WAAW,eAAe,QAAQ,MAAM;AAAA,UACnE;AAAA,QACF,CAAC;AAAA,MACL,CAAC;AACD;AAAA,IACF;AAGA,iBAAa,KAAK;AAAA,MAChB,cAAc,eAAe;AAAA;AAAA,MAE7B,MAAM,QAAQ,QAAQ,KAAK,MAAM,UAAU,YAAY,WAAW,UAAU,YAAY,OAAO,CAAC,EAC7F,KAAK,OAAM,gBAAe;AACzB,cAAM,cAAc,gBAAgB,EAAE,MAAM,IAAI,SAAS,WAAW,GAAG,QAAQ,EAAE,CAAC;AAElF,cAAM,EAAE,aAAa,UAAU,IAAI;AACnC,cAAM,QAAQ,YAAY;AAC1B,YAAI,YAAY;AAChB,YAAI,UAAU;AACd,YAAI,WAAW;AACf,iBAAS,IAAI,GAAG,IAAI,MAAM,QAAQ,KAAK;AACrC,gBAAM,OAAO,MAAM,CAAC;AACpB,gBAAM,YAAY,OAAO,KAAK,eAAe;AAC7C,gBAAM,UAAU,IAAI,IAAI,MAAM,SAC1B,OAAO,MAAM,IAAI,CAAC,EAAE,eAAe,IACnC,UAAU;AAEd,cAAI,YAAY,aAAa,UAAU,aAAa;AAClD,gBAAI,OAAO,MAAM,SAAS,GAAG;AAC3B,0BAAY,OAAO,KAAK,MAAM;AAC9B,yBAAW;AAAA,YACb;AACA,sBAAU,OAAO,KAAK,MAAM,IAAI,KAAK;AAAA,UACvC;AAAA,QACF;AACA,cAAM,SAAS,MAAM,KAAK,MAAM,WAAW,OAAO;AAClD,cAAM,SAAS,EAAE,MAAM,IAAI,SAAS,MAAM,GAAG,QAAQ,EAAE;AAEvD,cAAM,oBAAoB,WAAW;AAAA,UACnC,GAAG;AAAA,UACH,YAAY,UAAU,aAAa;AAAA,UACnC,aAAa,UAAU,cAAc;AAAA,UACrC,WAAW,UAAU,YAAY;AAAA,QACnC,IAAI;AACJ,eAAO;AAAA,UACL,MAAM,WAAW,QAAQ,mBAAmB,eAAe,QAAQ,MAAM;AAAA,UACzE;AAAA,QACF;AAAA,MACF,CAAC;AAAA,IACL,CAAC;AAAA,EACH;AAEA,SAAO,EAAE,YAAY,UAAU,YAAY,WAAW,UAAU,WAAW,aAAa;AAC1F;AA4BA,eAAsB,iBAAiB,EAAE,aAAa,GAAG,aAAa,WAAW,SAAS,WAAW;AAEnG,QAAM,aAAa,MAAM,QAAQ,IAAI,aAAa,IAAI,OAAO,EAAE,KAAK,MAAM;AACxE,UAAM,QAAQ,MAAM;AACpB,WAAO;AAAA,MACL,GAAG;AAAA,MACH,MAAM,QAAQ,MAAM,IAAI;AAAA,IAC1B;AAAA,EACF,CAAC,CAAC;AAGF,QAAM,sBAAsB,aACzB,IAAI,WAAS,MAAM,aAAa,CAAC,CAAC,EAClC,OAAO,UAAQ,CAAC,WAAW,QAAQ,SAAS,IAAI,CAAC;AACpD,QAAM,cAAc,WAAW;AAC/B,QAAM,gBAAgB,YAAY,IAAI,UAAQ,aAAa,UAAU,YAAU,OAAO,aAAa,CAAC,MAAM,IAAI,CAAC;AAG/G,QAAM,cAAc,YAAY;AAChC,MAAI,cAAc,UAAU;AAE1B,UAAMC,aAAY,MAAM,WAAW;AACnC,aAAS,YAAY,GAAG,YAAY,aAAa,aAAa;AAC5D,YAAM,MAAM,cAAc;AAG1B,YAAM,UAAU,CAAC;AACjB,eAAS,IAAI,GAAG,IAAI,aAAa,QAAQ,KAAK;AAC5C,cAAM,EAAE,MAAM,SAAS,IAAI,WAAW,CAAC;AACvC,gBAAQ,aAAa,CAAC,EAAE,aAAa,CAAC,CAAC,IAAI,KAAK,MAAM,QAAQ;AAAA,MAChE;AACA,MAAAA,WAAU,SAAS,IAAI;AAAA,IACzB;AACA,WAAOA;AAAA,EACT;AAGA,QAAM,YAAY,MAAM,WAAW;AACnC,WAAS,YAAY,GAAG,YAAY,aAAa,aAAa;AAC5D,UAAM,MAAM,cAAc;AAE1B,UAAM,UAAU,MAAM,aAAa,MAAM;AACzC,aAAS,IAAI,GAAG,IAAI,YAAY,QAAQ,KAAK;AAC3C,YAAM,SAAS,cAAc,CAAC;AAC9B,UAAI,UAAU,GAAG;AACf,cAAM,EAAE,MAAM,SAAS,IAAI,WAAW,MAAM;AAC5C,gBAAQ,CAAC,IAAI,KAAK,MAAM,QAAQ;AAAA,MAClC;AAAA,IACF;AACA,cAAU,SAAS,IAAI;AAAA,EACzB;AACA,SAAO;AACT;AASO,SAAS,cAAc,eAAeC,aAAY;AACvD,QAAM,EAAE,aAAa,IAAI;AAEzB,QAAM,YAAY,CAAC;AACnB,aAAW,SAASA,YAAW,UAAU;AACvC,QAAI,MAAM,SAAS,QAAQ;AACzB,YAAM,eAAe,aAAa,OAAO,YAAU,OAAO,aAAa,CAAC,MAAM,MAAM,QAAQ,IAAI;AAChG,UAAI,CAAC,aAAa,OAAQ;AAI1B,YAAM,WAAW,oBAAI,IAAI;AACzB,YAAM,OAAO,QAAQ,IAAI,aAAa,IAAI,YAAU;AAClD,eAAO,OAAO,KAAK,KAAK,CAAC,EAAE,MAAAC,MAAK,MAAM;AACpC,mBAAS,IAAI,OAAO,aAAa,KAAK,GAAG,GAAG,QAAQA,KAAI,CAAC;AAAA,QAC3D,CAAC;AAAA,MACH,CAAC,CAAC,EAAE,KAAK,MAAM;AAEb,uBAAe,UAAU,KAAK;AAC9B,cAAM,aAAa,SAAS,IAAI,MAAM,KAAK,KAAK,GAAG,CAAC;AACpD,YAAI,CAAC,WAAY,OAAM,IAAI,MAAM,mCAAmC;AACpE,eAAO,EAAE,MAAM,CAAC,UAAU,GAAG,UAAU,EAAE;AAAA,MAC3C,CAAC;AAED,gBAAU,KAAK,EAAE,cAAc,MAAM,MAAM,KAAK,CAAC;AAAA,IACnD,OAAO;AAEL,YAAM,cAAc,aAAa,KAAK,YAAU,OAAO,aAAa,CAAC,MAAM,MAAM,QAAQ,IAAI;AAC7F,UAAI,aAAa;AACf,kBAAU,KAAK,WAAW;AAAA,MAC5B;AAAA,IACF;AAAA,EACF;AACA,SAAO,EAAE,GAAG,eAAe,cAAc,UAAU;AACrD;;;AC7MA,eAAsB,YAAY,SAAS;AAEzC,UAAQ,aAAa,MAAM,qBAAqB,QAAQ,MAAM,OAAO;AAErE,QAAM,EAAE,WAAW,GAAG,QAAQ,SAAS,SAAS,YAAY,WAAW,QAAQ,eAAe,KAAK,IAAI;AAGvG,MAAI,UAAU,cAAc,UAAU;AACpC,UAAM,IAAI,MAAM,6CAA6C;AAAA,EAC/D;AAGA,QAAM,gBAAgB,uBAAuB,MAAM;AACnD,MAAI,cAAc,QAAQ;AACxB,UAAM,gBAAgB,cAAc,QAAQ,QAAQ,EAAE,SAAS,IAAI,OAAK,EAAE,QAAQ,IAAI;AACtF,UAAM,iBAAiB,cAAc,OAAO,OAAK,CAAC,cAAc,SAAS,CAAC,CAAC;AAC3E,QAAI,eAAe,QAAQ;AACzB,YAAM,IAAI,MAAM,qCAAqC,eAAe,KAAK,IAAI,CAAC,EAAE;AAAA,IAClF;AAAA,EACF;AACA,MAAI,cAAc;AAClB,MAAI,qBAAqB;AACzB,MAAI,WAAW,QAAQ;AACrB,UAAM,uBAAuB,cAAc,OAAO,OAAK,CAAC,QAAQ,SAAS,CAAC,CAAC;AAC3E,QAAI,qBAAqB,QAAQ;AAC/B,oBAAc,CAAC,GAAG,SAAS,GAAG,oBAAoB;AAClD,2BAAqB;AAAA,IACvB;AAAA,EACF;AAGA,QAAM,cAAc,gBAAgB,UAAU,EAAE,GAAG,SAAS,SAAS,YAAY,IAAI;AACrF,QAAM,cAAc,iBAAiB,WAAW;AAGhD,MAAI,CAAC,cAAc,CAAC,SAAS;AAC3B,eAAW,EAAE,aAAa,KAAK,aAAa;AAC1C,iBAAW,EAAE,KAAK,KAAK,aAAc,OAAM;AAAA,IAC7C;AACA;AAAA,EACF;AAGA,QAAMC,cAAa,cAAc,QAAQ,QAAQ;AACjD,QAAM,YAAY,YAAY,IAAI,SAAO,cAAc,KAAKA,WAAU,CAAC;AAGvE,MAAI,SAAS;AACX,eAAW,cAAc,WAAW;AAClC,iBAAW,eAAe,WAAW,cAAc;AACjD,oBAAY,KAAK,KAAK,CAAC,EAAE,MAAM,SAAS,MAAM;AAC5C,cAAIC,YAAW,WAAW,aAAa;AACvC,qBAAW,cAAc,MAAM;AAC7B,oBAAQ;AAAA,cACN,YAAY,YAAY,aAAa,CAAC;AAAA,cACtC;AAAA,cACA,UAAAA;AAAA,cACA,QAAQA,YAAW,WAAW;AAAA,YAChC,CAAC;AACD,YAAAA,aAAY,WAAW;AAAA,UACzB;AAAA,QACF,CAAC;AAAA,MACH;AAAA,IACF;AAAA,EACF;AAGA,MAAI,YAAY;AAGd,UAAM,OAAO,CAAC;AACd,eAAW,cAAc,WAAW;AAElC,YAAM,cAAc,KAAK,IAAI,WAAW,WAAW,YAAY,CAAC;AAChE,YAAM,YAAY,KAAK,KAAK,UAAU,YAAY,WAAW,YAAY,WAAW,SAAS;AAE7F,YAAM,YAAY,cAAc,WAC9B,MAAM,iBAAiB,YAAY,aAAa,WAAW,aAAa,QAAQ,IAChF,MAAM,iBAAiB,YAAY,aAAa,WAAW,SAAS,OAAO;AAG7E,UAAI,QAAQ;AAEV;AAAA,gBAAW;AAAA;AAAA,UAA6C;AAAA,UAAY;AAClE,cAAI,YAAY,KAAK,QAAQ,YAAY,GAAG;AAC1C,gBAAI,sBAAsB,SAAS;AACjC,yBAAW,OAAO,eAAe;AAC/B,oBAAI,CAAC,QAAQ,SAAS,GAAG,EAAG,QAAO,IAAI,GAAG;AAAA,cAC5C;AAAA,YACF;AACA,iBAAK,KAAK,GAAG;AAAA,UACf;AAAA,QACF;AAAA,MACF,OAAO;AACL,eAAO,MAAM,SAAS;AAAA,MACxB;AAAA,IACF;AACA,eAAW,IAAI;AAAA,EACjB,OAAO;AAEL,eAAW,EAAE,aAAa,KAAK,WAAW;AACxC,iBAAW,EAAE,KAAK,KAAK,aAAc,OAAM;AAAA,IAC7C;AAAA,EACF;AACF;AAMO,SAAS,iBAAiB,SAAS;AACxC,MAAI,CAAC,QAAQ,SAAU,OAAM,IAAI,MAAM,2BAA2B;AAIlE,QAAM,OAAO,YAAY,OAAO;AAChC,UAAQ,OAAO,oBAAoB,QAAQ,MAAM,IAAI;AAGrD,SAAO,KAAK,OAAO,IAAI,eAAa,aAAa,SAAS,MAAM,SAAS,CAAC;AAC5E;AAQA,eAAsB,kBAAkB,SAAS;AAC/C,MAAI,QAAQ,SAAS,WAAW,GAAG;AACjC,UAAM,IAAI,MAAM,kDAAkD;AAAA,EACpE;AACA,UAAQ,aAAa,MAAM,qBAAqB,QAAQ,MAAM,OAAO;AACrE,QAAM,cAAc,iBAAiB,OAAO;AAG5C,QAAMD,cAAa,cAAc,QAAQ,QAAQ;AACjD,QAAM,YAAY,YAAY,IAAI,SAAO,cAAc,KAAKA,WAAU,CAAC;AAGvE,QAAM,aAAa,CAAC;AACpB,aAAW,MAAM,WAAW;AAC1B,eAAW,KAAK,SAAS,MAAM,GAAG,aAAa,CAAC,EAAE,MAAM,IAAI,CAAC;AAAA,EAC/D;AACA,SAAO,QAAQ,UAAU;AAC3B;AASO,SAAS,mBAAmB,SAAS;AAC1C,SAAO,IAAI,QAAQ,CAAC,YAAY,WAAW;AACzC,gBAAY;AAAA,MACV,GAAG;AAAA,MACH,WAAW;AAAA;AAAA,MACX;AAAA,IACF,CAAC,EAAE,MAAM,MAAM;AAAA,EACjB,CAAC;AACH;;;ACxKA,eAAsB,aAAa,SAAS;AAC1C,MAAI,CAAC,QAAQ,QAAQ,EAAE,QAAQ,KAAK,cAAc,IAAI;AACpD,UAAM,IAAI,MAAM,8BAA8B;AAAA,EAChD;AACA,UAAQ,aAAa,MAAM,qBAAqB,QAAQ,MAAM,OAAO;AAErE,QAAM,EAAE,UAAU,WAAW,GAAG,SAAS,SAAS,OAAO,IAAI;AAC7D,MAAI,WAAW,EAAG,OAAM,IAAI,MAAM,mCAAmC;AACrE,QAAM,SAAS,QAAQ,UAAU,OAAO,SAAS,QAAQ;AAGzD,MAAI,SAAS;AACX,UAAM,aAAa,cAAc,QAAQ,QAAQ,EAAE,SAAS,IAAI,OAAK,EAAE,QAAQ,IAAI;AACnF,QAAI,CAAC,WAAW,SAAS,OAAO,GAAG;AACjC,YAAM,IAAI,MAAM,qCAAqC,OAAO,EAAE;AAAA,IAChE;AAAA,EACF;AAEA,MAAI,UAAU,CAAC,WAAW,SAAS,SAAS,UAAU;AAGpD,UAAM,eAAe,CAAC;AACtB,QAAI,aAAa;AACjB,eAAW,SAAS,SAAS,YAAY;AACvC,YAAM,WAAW,aAAa,OAAO,MAAM,QAAQ;AAEnD,YAAM,YAAY,MAAM,mBAAmB;AAAA,QACzC,GAAG;AAAA,QAAS,UAAU;AAAA,QAAY,QAAQ;AAAA,MAC5C,CAAC;AACD,mBAAa,KAAK,GAAG,SAAS;AAC9B,UAAI,aAAa,UAAU,OAAQ;AACnC,mBAAa;AAAA,IACf;AACA,WAAO,aAAa,MAAM,UAAU,MAAM;AAAA,EAC5C,WAAW,UAAU,SAAS;AAE5B,UAAM,cAAc,WAAW,CAAC,QAAQ,SAAS,OAAO,IACpD,CAAC,GAAG,SAAS,OAAO,IACpB;AAEJ,UAAM,UAAU,MAAM,mBAAmB;AAAA,MACvC,GAAG;AAAA,MAAS,UAAU;AAAA,MAAW,QAAQ;AAAA,MAAW,SAAS;AAAA,IAC/D,CAAC;AAGD,YAAQ,KAAK,CAAC,GAAG,MAAM,QAAQ,EAAE,OAAO,GAAG,EAAE,OAAO,CAAC,CAAC;AAGtD,QAAI,gBAAgB,SAAS;AAC3B,iBAAW,OAAO,SAAS;AACzB,eAAO,IAAI,OAAO;AAAA,MACpB;AAAA,IACF;AAEA,WAAO,QAAQ,MAAM,UAAU,MAAM;AAAA,EACvC,WAAW,QAAQ;AAEjB,UAAM,UAAU,MAAM,mBAAmB;AAAA,MACvC,GAAG;AAAA,MAAS,UAAU;AAAA,MAAW,QAAQ;AAAA,IAC3C,CAAC;AACD,WAAO,QAAQ,MAAM,UAAU,MAAM;AAAA,EACvC,WAAW,OAAO,YAAY,UAAU;AAEtC,UAAM,cAAc,MAAM,kBAAkB;AAAA,MAC1C,GAAG;AAAA,MAAS,UAAU;AAAA,MAAW,QAAQ;AAAA,MAAW,SAAS,CAAC,OAAO;AAAA,IACvE,CAAC;AAGD,UAAM,gBAAgB,MAAM,KAAK,aAAa,CAAC,GAAG,UAAU,KAAK,EAC9D,KAAK,CAAC,GAAG,MAAM,QAAQ,YAAY,CAAC,GAAG,YAAY,CAAC,CAAC,CAAC,EACtD,MAAM,UAAU,MAAM;AAEzB,UAAM,aAAa,MAAM,gBAAgB,EAAE,GAAG,SAAS,MAAM,cAAc,CAAC;AAG5E,UAAM,OAAO,cAAc,IAAI,WAAS,WAAW,KAAK,CAAC;AACzD,WAAO;AAAA,EACT,OAAO;AACL,WAAO,MAAM,mBAAmB,OAAO;AAAA,EACzC;AACF;AAQA,eAAe,gBAAgB,SAAS;AACtC,QAAM,EAAE,MAAM,KAAK,IAAI;AACvB,UAAQ,aAAa,MAAM,qBAAqB,MAAM,OAAO;AAC7D,QAAM,EAAE,YAAY,UAAU,IAAI,QAAQ;AAE1C,QAAM,gBAAgB,MAAM,UAAU,MAAM,EAAE,KAAK,KAAK;AACxD,MAAI,aAAa;AACjB,QAAM,YAAY,UAAU,IAAI,WAAS,cAAc,OAAO,MAAM,QAAQ,CAAC;AAC7E,aAAW,SAAS,MAAM;AACxB,UAAM,aAAa,UAAU,UAAU,SAAO,QAAQ,GAAG;AACzD,kBAAc,UAAU,IAAI;AAAA,EAC9B;AAGA,QAAM,YAAY,CAAC;AACnB,MAAI;AACJ,eAAa;AACb,WAAS,IAAI,GAAG,IAAI,cAAc,QAAQ,KAAK;AAC7C,UAAM,WAAW,aAAa,OAAO,UAAU,CAAC,EAAE,QAAQ;AAC1D,QAAI,cAAc,CAAC,GAAG;AACpB,UAAI,eAAe,QAAW;AAC5B,qBAAa;AAAA,MACf;AAAA,IACF,OAAO;AACL,UAAI,eAAe,QAAW;AAC5B,kBAAU,KAAK,CAAC,YAAY,QAAQ,CAAC;AACrC,qBAAa;AAAA,MACf;AAAA,IACF;AACA,iBAAa;AAAA,EACf;AACA,MAAI,eAAe,QAAW;AAC5B,cAAU,KAAK,CAAC,YAAY,UAAU,CAAC;AAAA,EACzC;AAIA,QAAM,aAAa,MAAM,OAAO,QAAQ,SAAS,QAAQ,CAAC;AAC1D,aAAW,CAACE,aAAY,QAAQ,KAAK,WAAW;AAE9C,UAAM,YAAY,MAAM,mBAAmB,EAAE,GAAG,SAAS,UAAUA,aAAY,QAAQ,SAAS,CAAC;AACjG,aAAS,IAAIA,aAAY,IAAI,UAAU,KAAK;AAE1C,iBAAW,CAAC,IAAI,EAAE,WAAW,GAAG,GAAG,UAAU,IAAIA,WAAU,EAAE;AAAA,IAC/D;AAAA,EACF;AACA,SAAO;AACT;AAOA,SAAS,QAAQ,GAAG,GAAG;AACrB,MAAI,IAAI,EAAG,QAAO;AAClB,MAAI,IAAI,EAAG,QAAO;AAClB,SAAO;AACT;",
  "names": ["child", "schemaTree", "decoder", "logicalType", "buffer", "bitWidth", "bitWidth", "len", "bitWidth", "reader", "groupData", "schemaTree", "data", "schemaTree", "rowStart", "rangeStart"]
}
